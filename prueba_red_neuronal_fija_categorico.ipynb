{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGAMOS LIBRERIAS\n",
    "\n",
    "# LIBRERÍAS BÁSICAS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "\n",
    "# CARGA DE DATOS EN AWS S3\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "\n",
    "# MODELOS DE REDES NEURONALES\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout, Reshape, Conv1D, Masking, LSTM, Conv2D, Input, Multiply, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import L2, L1L2, L1\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# EVALUACIÓN DE MODELOS\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, cohen_kappa_score, confusion_matrix, precision_score,accuracy_score, recall_score, precision_recall_curve,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A3TV</th>\n",
       "      <th>ABE</th>\n",
       "      <th>ABG.P</th>\n",
       "      <th>ABG.P_1</th>\n",
       "      <th>ABG</th>\n",
       "      <th>ACS</th>\n",
       "      <th>ACX_1</th>\n",
       "      <th>ACX_2</th>\n",
       "      <th>AENA</th>\n",
       "      <th>AGS</th>\n",
       "      <th>...</th>\n",
       "      <th>TRE</th>\n",
       "      <th>TUB</th>\n",
       "      <th>UNF</th>\n",
       "      <th>UNI_1</th>\n",
       "      <th>VIS</th>\n",
       "      <th>VIS_1</th>\n",
       "      <th>VIS_2</th>\n",
       "      <th>VIS_3</th>\n",
       "      <th>ZEL</th>\n",
       "      <th>benchmark</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993-08-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-08-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002079</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-08-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.004088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.021590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-08-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.007048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-08-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A3TV  ABE  ABG.P  ABG.P_1  ABG  ACS     ACX_1  ACX_2  AENA  AGS  \\\n",
       "date                                                                          \n",
       "1993-08-17   NaN  NaN    NaN      NaN  NaN  NaN  0.000831    NaN   NaN  NaN   \n",
       "1993-08-18   NaN  NaN    NaN      NaN  NaN  NaN -0.002079    NaN   NaN  NaN   \n",
       "1993-08-19   NaN  NaN    NaN      NaN  NaN  NaN -0.004088    NaN   NaN  NaN   \n",
       "1993-08-20   NaN  NaN    NaN      NaN  NaN  NaN -0.007048    NaN   NaN  NaN   \n",
       "1993-08-23   NaN  NaN    NaN      NaN  NaN  NaN -0.002867    NaN   NaN  NaN   \n",
       "\n",
       "            ...  TRE  TUB  UNF  UNI_1       VIS  VIS_1  VIS_2  VIS_3  ZEL  \\\n",
       "date        ...                                                             \n",
       "1993-08-17  ...  NaN  NaN  NaN    NaN  0.025183    NaN    NaN    NaN  NaN   \n",
       "1993-08-18  ...  NaN  NaN  NaN    NaN  0.027093    NaN    NaN    NaN  NaN   \n",
       "1993-08-19  ...  NaN  NaN  NaN    NaN -0.021590    NaN    NaN    NaN  NaN   \n",
       "1993-08-20  ...  NaN  NaN  NaN    NaN -0.005504    NaN    NaN    NaN  NaN   \n",
       "1993-08-23  ...  NaN  NaN  NaN    NaN  0.000000    NaN    NaN    NaN  NaN   \n",
       "\n",
       "            benchmark  \n",
       "date                   \n",
       "1993-08-17   0.016909  \n",
       "1993-08-18   0.012458  \n",
       "1993-08-19   0.007611  \n",
       "1993-08-20  -0.000402  \n",
       "1993-08-23  -0.003132  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CARGAMOS LOS RETORNOS DE TODOS LOS ACTIVOS QUE HAN PERTENECIDO AL IBEX35 Y EL BENCHMARK\n",
    "returns = pd.read_csv('ibex_historical_returns.csv',index_col='date')\n",
    "returns.index = pd.to_datetime(returns.index)\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A3TV</th>\n",
       "      <th>ABE</th>\n",
       "      <th>ABG.P</th>\n",
       "      <th>ABG.P_1</th>\n",
       "      <th>ABG</th>\n",
       "      <th>ACS</th>\n",
       "      <th>ACX_1</th>\n",
       "      <th>ACX_2</th>\n",
       "      <th>AENA</th>\n",
       "      <th>AGS</th>\n",
       "      <th>...</th>\n",
       "      <th>TRE</th>\n",
       "      <th>TUB</th>\n",
       "      <th>UNF</th>\n",
       "      <th>UNI_1</th>\n",
       "      <th>VIS</th>\n",
       "      <th>VIS_1</th>\n",
       "      <th>VIS_2</th>\n",
       "      <th>VIS_3</th>\n",
       "      <th>ZEL</th>\n",
       "      <th>benchmark</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993-08-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3113.896729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-08-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3166.996826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-08-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.1276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3206.696777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-08-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3231.196777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-08-20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3229.896729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A3TV  ABE  ABG.P  ABG.P_1  ABG  ACS   ACX_1  ACX_2  AENA  AGS  \\\n",
       "date                                                                        \n",
       "1993-08-16   NaN  NaN    NaN      NaN  NaN  NaN  1.2025    NaN   NaN  NaN   \n",
       "1993-08-17   NaN  NaN    NaN      NaN  NaN  NaN  1.2035    NaN   NaN  NaN   \n",
       "1993-08-18   NaN  NaN    NaN      NaN  NaN  NaN  1.2010    NaN   NaN  NaN   \n",
       "1993-08-19   NaN  NaN    NaN      NaN  NaN  NaN  1.1961    NaN   NaN  NaN   \n",
       "1993-08-20   NaN  NaN    NaN      NaN  NaN  NaN  1.1877    NaN   NaN  NaN   \n",
       "\n",
       "            ...  TRE  TUB  UNF  UNI_1     VIS  VIS_1  VIS_2  VIS_3  ZEL  \\\n",
       "date        ...                                                           \n",
       "1993-08-16  ...  NaN  NaN  NaN    NaN  2.9683    NaN    NaN    NaN  NaN   \n",
       "1993-08-17  ...  NaN  NaN  NaN    NaN  3.0440    NaN    NaN    NaN  NaN   \n",
       "1993-08-18  ...  NaN  NaN  NaN    NaN  3.1276    NaN    NaN    NaN  NaN   \n",
       "1993-08-19  ...  NaN  NaN  NaN    NaN  3.0608    NaN    NaN    NaN  NaN   \n",
       "1993-08-20  ...  NaN  NaN  NaN    NaN  3.0440    NaN    NaN    NaN  NaN   \n",
       "\n",
       "              benchmark  \n",
       "date                     \n",
       "1993-08-16  3113.896729  \n",
       "1993-08-17  3166.996826  \n",
       "1993-08-18  3206.696777  \n",
       "1993-08-19  3231.196777  \n",
       "1993-08-20  3229.896729  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CARGAMOS LOS DATOS DE PRECIOS DE CIERRE AJUSTADO PARA TODOS LOS ACTIVOS QUE HAN PERTENECIDO AL IBEX35 Y EL BENCHMARK\n",
    "prices = pd.read_csv('ibex_historical_data.csv', index_col='date')\n",
    "prices.index = pd.to_datetime(prices.index)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_risk_free_asset_data(file):\n",
    "    # CARGAMOS LOS DATOS DE LA TASA DE RETORNO DE LOS BONOS A 3 MESES DE LOS EEUU\n",
    "    filenames = [filename for filename in os.listdir('./Datos TFM') if filename.startswith(file)]\n",
    "    risk_free_asset = pd.DataFrame()\n",
    "    for f in filenames:\n",
    "        file_path = './Datos TFM/'+f\n",
    "        file = pd.read_csv(file_path)\n",
    "        risk_free_asset = pd.concat([risk_free_asset,file], axis=0)\n",
    "    risk_free_asset.index = pd.to_datetime(risk_free_asset.Date)\n",
    "    risk_free_asset.sort_index(inplace=True)\n",
    "    risk_free_asset.drop('Date', axis=1, inplace=True)\n",
    "    return risk_free_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_asset = load_risk_free_asset_data('Spain 3-Month Bond Yield Historical Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5135, 37), -1.0195842263237869)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def locate_in_df(df, value):\n",
    "    a = df.to_numpy()\n",
    "    row = np.where(a == value)[0][0]\n",
    "    col = np.where(a == value)[1][0]\n",
    "    return row, col\n",
    "locate_in_df(returns, returns.min().min()), returns.min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELIMINAMOS OUTLIERS EN LOS PRECIOS Y RETORNOS\n",
    "prices.iloc[5136:,37] = np.nan\n",
    "returns.iloc[5135:,37] = np.nan\n",
    "prices.iloc[5626:, 3] = np.nan\n",
    "returns.iloc[5625:, 3] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIMOS NUESTRA TASA LIBRE DE RIESGO A TRAVÉS DE LOS VALORES DE LA TASA T-BILLS DE 3 MESES\n",
    "risk_free_rate = risk_free_asset['Price']/100\n",
    "risk_free_rate = risk_free_rate.sort_index()\n",
    "returns['risk_free_rate'] = risk_free_rate.loc[returns.index[0]:]\n",
    "returns['risk_free_rate'] = returns['risk_free_rate'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NECESITAMOS ARMAR UN DATASET CON INFORMACIÓN SOBRE CADA ACCIÓN CADA DIA. NUESTRO PROBLEMA NO SE BASA EN PREDECIR UN SERIE HISTÓRICA, SINO EN CLASIFICAR UN ACTIVO DEPENDIENDO DE SI EN EL ULTIMO PERÍODO ANALIZADO SE ENCUENTRA DENTRO DEL TOP 10 DE ACTIVOS BAJO UN CRITERIO O NO.\n",
    "\n",
    "PARA ESTO ARMAREMOS UN DATASET PERO SIN IMPORTAR CUAL ES LA POSICIÓN DE CADA ACCIÓN EN SÍ DENTRO DEL RANKING. LO QUE NOS INTERESA SON LOS SIGUIENTES DATOS:\n",
    "1) MEDIA DE RETORNOS DEL ULTIMO PERIODO A ANALIZAR                          OK\n",
    "2) SHARPE DEL ULTIMO PERIODO A ANALIZAR                                     OK\n",
    "3) CANTIDAD DE DIAS EN EL INDICE                                            OK\n",
    "4) ALFA DE JENSEN DEL ULTIMO PERIODO A ANALIZAR                             OK\n",
    "5) BETA DEL ULTIMO PERIODO A ANALIZAR                                       OK\n",
    "6) DESVIACIÓN ESTANDARD DEL ULTIMO PERIODO A ANALIZAR                       OK\n",
    "7) CANTIDAD DE DIAS EN EL TOP 10\n",
    "8) MAXIMO DRAWDOWN DEL ULTIMO PERIODO A ANALIZAR                            COMO NO TENEMOS TODO EL HISTORICO DE DATOS DE TODOS LOS ACTIVOS NO ES CONVENIENTE CALCULAR ESTA\n",
    "                                                                            MEDIDA\n",
    "9) RATIO DE TREYNOR DEL ULTIMO PERIODO A ANALIZAR                           OK\n",
    "10) MEDIDA DE MODIGLIANI DEL ULTIMO PERIODO A ANALIZAR                      OK\n",
    "11) RATIO DE SORTINO DEL ULTIMO PERIODO A ANALIZAR                          OK\n",
    "12) DISTANCIA CON RESPECTO AL BENCHMARK EN RATIO DE SHARPE                  OK\n",
    "13) RATIO DE CALMAR DEL ULTIMO PERIODO A ANALIZAR                           COMO NO TENEMOS TODO EL HISTORICO DE DATOS DE TODOS LOS ACTIVOS NO ES CONVENIENTE CALCULAR ESTA\n",
    "                                                                            MEDIDA\n",
    "14) MINIMO RETORNO DEL ULTIMO PERIODO A ANALIZAR                            OK\n",
    "15) MAXIMO RETORNO DEL ULTIMO PERIODO A ANALIZAR                            OK\n",
    "16) RETORNO ACUMULADO DEL ULTIMO PERIODO A ANALIZAR                         OK\n",
    "17) RETORNO TOTAL DEL ULTIMO PERIODO A ANALIZAR                             OK\n",
    "18) RETORNO TOTAL DESDE INGRESO AL INDICE                                   OK\n",
    "19) MAXIMO DESDE INGRESO AL INDICE                                          OK\n",
    "20) MINIMO DESDE INGRESO AL INDICE                                          OK\n",
    "21) DIAS DESDE ULTIMO MAXIMO                                                \n",
    "22) DIAS DESDE ULTIMO MINIMO\n",
    "23)                          \n",
    "\n",
    "NUESTRA ETIQUETA A PREDECIR ES SI EL ACTIVO HA SUPERADO AL BENCHMARK EN ALGUN MOMENTO DEL SIGUIENTE PERÍODO A ANALIZAR. NUESTRO OBJETIVO NO ES PUNTUAR A LOS ACTIVOS DADO QUE LUEGO REALIZAREMOS LA ASIGNACIÓN DE PESOS A TRAVÉS DE MARKOWITZ. NUESTRO OBJETIVO ES SELECCIONAR ACTIVOS DEL TOTAL DE ACTIVOS QUE SE ENCUENTRAN EN EL INDICE:\n",
    "- 0: NO SE ENCUENTRA EN EL TOP 10.\n",
    "- 1: SE ENCUENTRA EN EL TOP 10.\n",
    "\n",
    "DEBEMOS ITERAR PARA TODOS LOS ACTIVOS Y PARA TODOS LOS DIAS COMPLETANDO LA INFORMACIÓN DE NUESTRO DATASET. AL FINAL TENDREMOS UN DATASET CON N + 1 COLUMNAS (SIENDO N LA CANTIDAD DE FEATURES QUE TENEMOS EN NUESTRO DATASET) Y SUMANDO LA ETIQUETA 0 O 1 DEPENDIENDO DE SI ESTUVO EN EL INDICE EN ESE DIA O NO. NUESTRO DATASET FINAL TENDRÁ M FILAS SIENDO M LA CANTIDAD DE DIAS QUE HA COTIZADO CADA ACTIVO RESTANDO LA CANTIDAD DE PERIODOS QUE SE TOMEN PARA HACER EL PRIMER ANALISIS MULTIPLICADO POR LA CANTIDAD DE ACTIVOS. PARA LA ENTRADA A LA RED NO ES NECESARIO CONOCER QUÉ ACCIÓN ESTAMOS SELECCIONANDO DADO QUE LO QUE NOS IMPORTAN SON SUS CARACTERISTICAS EN EL DIA ANALIZADO. LO QUE SE BUSCA ES ENTONCES DADA UNA SERIE DE CARACTERISTICAS, ETIQUETAR UNA ACCIÓN. LUEGO SE TOMARÁN LOS RECAUDOS NECESARIOS PARA ENTENDER QUÉ ACCIÓN ES Y EN QUÉ MOMENTO. NUESTRO ANALISIS BUSCA TOMAR UNA DECISIÓN A DIA DE HOY BASANDONOS EN INFORMACIÓN HISTÓRICA. NO BUSCA PREDECIR LA BOLSA NI ANALIZAR QUE PASARÁ DE AQUI EN EL FUTURO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import inv, pinv\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,10) and (99,99) not aligned: 10 (dim 1) != 99 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 42\u001b[0m\n\u001b[0;32m     37\u001b[0m w0 \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mn]\u001b[39m*\u001b[39mn\n\u001b[0;32m     39\u001b[0m cons \u001b[39m=\u001b[39m ({\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39meq\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m: total_weight_constraint},\n\u001b[0;32m     40\u001b[0m         {\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mineq\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m: Libor_constraint}, \n\u001b[0;32m     41\u001b[0m         {\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mineq\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m: long_only_constraint})\n\u001b[1;32m---> 42\u001b[0m res\u001b[39m=\u001b[39m minimize(risk_budget_objective, w0, args\u001b[39m=\u001b[39;49m[V,x_t], method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSLSQP\u001b[39;49m\u001b[39m'\u001b[39;49m, constraints\u001b[39m=\u001b[39;49mcons, options\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mdisp\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m})\n\u001b[0;32m     43\u001b[0m w_rb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masmatrix(res\u001b[39m.\u001b[39mx)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scipy\\optimize\\_minimize.py:719\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    716\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    717\u001b[0m                            bounds\u001b[39m=\u001b[39mbounds, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    718\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mslsqp\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 719\u001b[0m     res \u001b[39m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m    720\u001b[0m                           constraints, callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    721\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrust-constr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    722\u001b[0m     res \u001b[39m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    723\u001b[0m                                        bounds, constraints,\n\u001b[0;32m    724\u001b[0m                                        callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scipy\\optimize\\_slsqp_py.py:374\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    371\u001b[0m     xu[infbnd[:, \u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[0;32m    373\u001b[0m \u001b[39m# ScalarFunction provides function and gradient evaluation\u001b[39;00m\n\u001b[1;32m--> 374\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(func, x, jac\u001b[39m=\u001b[39;49mjac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    375\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step,\n\u001b[0;32m    376\u001b[0m                               bounds\u001b[39m=\u001b[39;49mnew_bounds)\n\u001b[0;32m    377\u001b[0m \u001b[39m# gh11403 SLSQP sometimes exceeds bounds by 1 or 2 ULP, make sure this\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[39m# doesn't get sent to the func/grad evaluator.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m wrapped_fun \u001b[39m=\u001b[39m _clip_x_for_func(sf\u001b[39m.\u001b[39mfun, new_bounds)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    379\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[0;32m    381\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    384\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[0;32m    386\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[115], line 16\u001b[0m, in \u001b[0;36mrisk_budget_objective\u001b[1;34m(x, pars)\u001b[0m\n\u001b[0;32m     14\u001b[0m V \u001b[39m=\u001b[39m pars[\u001b[39m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m x_t \u001b[39m=\u001b[39m pars[\u001b[39m1\u001b[39m] \n\u001b[1;32m---> 16\u001b[0m sig_p \u001b[39m=\u001b[39m  np\u001b[39m.\u001b[39msqrt(calculate_portfolio_var(x,V)) \n\u001b[0;32m     17\u001b[0m risk_target \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masmatrix(np\u001b[39m.\u001b[39mmultiply(sig_p,x_t))\n\u001b[0;32m     18\u001b[0m asset_RC \u001b[39m=\u001b[39m calculate_risk_contribution(x,V)\n",
      "Cell \u001b[1;32mIn[115], line 4\u001b[0m, in \u001b[0;36mcalculate_portfolio_var\u001b[1;34m(w, V)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_portfolio_var\u001b[39m(w,V):\n\u001b[0;32m      3\u001b[0m     w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatrix(w)\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mreturn\u001b[39;00m (w\u001b[39m*\u001b[39;49mV\u001b[39m*\u001b[39mw\u001b[39m.\u001b[39mT)[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:218\u001b[0m, in \u001b[0;36mmatrix.__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__mul__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, (N\u001b[39m.\u001b[39mndarray, \u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) :\n\u001b[0;32m    217\u001b[0m         \u001b[39m# This promotes 1-D vectors to row vectors\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m         \u001b[39mreturn\u001b[39;00m N\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m, asmatrix(other))\n\u001b[0;32m    219\u001b[0m     \u001b[39mif\u001b[39;00m isscalar(other) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(other, \u001b[39m'\u001b[39m\u001b[39m__rmul__\u001b[39m\u001b[39m'\u001b[39m) :\n\u001b[0;32m    220\u001b[0m         \u001b[39mreturn\u001b[39;00m N\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m, other)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,10) and (99,99) not aligned: 10 (dim 1) != 99 (dim 0)"
     ]
    }
   ],
   "source": [
    "def getIVP(cov,**kargs): \n",
    "    # Compute the inverse-variance portfolio \n",
    "    ivp=1./np.diag(cov)\n",
    "    ivp/=ivp.sum() \n",
    "    return ivp \n",
    "#------------------------------------------------------------------------------ \n",
    "def getClusterVar(cov,cItems): \n",
    "    # Compute variance per cluster \n",
    "    cov_=cov.loc[cItems,cItems]\n",
    "    # matrix slice \n",
    "    w_=getIVP(cov_).reshape(-1,1)\n",
    "    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "    return cVar \n",
    "#------------------------------------------------------------------------------\n",
    "def getQuasiDiag(link): \n",
    "    # Sort clustered items by distance \n",
    "    link=link.astype(int) \n",
    "    sortIx=pd.Series([link[-1,0],link[-1,1]])\n",
    "    numItems=link[-1,3] \n",
    "    # number of original items \n",
    "    while sortIx.max()>=numItems:\n",
    "        sortIx.index=range(0,sortIx.shape[0]*2,2) # make space\n",
    "        df0=sortIx[sortIx>=numItems] # find clusters\n",
    "        i=df0.index;j=df0.values-numItems\n",
    "        sortIx[i]=link[j,0] # item 1 \n",
    "        df0=pd.Series(link[j,1],index=i+1)\n",
    "        sortIx=sortIx.append(df0) # item 2 \n",
    "        sortIx=sortIx.sort_index() # re-sort\n",
    "        sortIx.index=range(sortIx.shape[0])\n",
    "        # re-index \n",
    "        \n",
    "    return sortIx.tolist()\n",
    "    \n",
    "def getRecBipart(cov,sortIx):\n",
    "    # Compute HRP alloc \n",
    "    w=pd.Series(1,index=sortIx)\n",
    "    cItems=[sortIx] # initialize all items in one cluster \n",
    "    while len(cItems)>0: \n",
    "        cItems=[i[j:k] for i in cItems for j,k in ((0,len(i)//2),(len(i)//2,len(i))) if len(i)>1] # bi-section\n",
    "        for i in range(0,len(cItems),2): # parse in pairs\n",
    "            cItems0=cItems[i] # cluster 1 \n",
    "            cItems1=cItems[i+1] # cluster 2\n",
    "            cVar0=getClusterVar(cov,cItems0) \n",
    "            cVar1=getClusterVar(cov,cItems1) \n",
    "            alpha=1-cVar0/(cVar0+cVar1)\n",
    "            w[cItems0]*=alpha # weight 1 \n",
    "            w[cItems1]*=1-alpha # weight 2 \n",
    "    return w\n",
    "\n",
    "def correlDist(corr): # A distance matrix based on correlation, where 0<=d[i,j]<=1 # This is a proper distance metric\n",
    "    dist=((1-corr)/2.)**.5 # distance matrix\n",
    "    return dist \n",
    "\n",
    "def hierarchical_risk_parity(data,last_date,alpha,corr,cov):\n",
    "    activos = data.dropna().index[:-1]\n",
    "    alpha_activos = alpha.loc[last_date,activos].sort_values(ascending=False)\n",
    "    selected_assets = alpha_activos[alpha_activos>0].index\n",
    "    corr = corr.loc[last_date][selected_assets].T[selected_assets]\n",
    "    cov = cov.loc[last_date][selected_assets].T[selected_assets]\n",
    "    dist = correlDist(corr) \n",
    "    link = sch.linkage(dist.dropna(),'single') \n",
    "    sortIx = getQuasiDiag(link)\n",
    "    sortIx = corr.index[sortIx].tolist() # recover labels \n",
    "    df0 = corr.loc[sortIx,sortIx] # reorder \n",
    "    hrp = getRecBipart(cov,sortIx) \n",
    "    return hrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REALIZAMOS LA GENERACIÓN DE CARACTERISTICAS PARA UNA SOLA ACCIÓN PARA UN SOLO DÍA. ASÍ COMO TAMBIEN TRAERSE LOS DATOS DEL BENCHMARK Y LA TASA LIBRE DE RIESGO\n",
    "# PRIMERO TOMAMOS LOS RETORNOS DIARIOS AL CIERRE DEL PRIMER PERIODO A SELECCIONAR. TOMAMOS COMO PERIODO 30 DÍAS.\n",
    "DIAS = 30\n",
    "VAR_LIMIT = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A3TV\n",
      "ABE\n",
      "ABG.P\n",
      "ABG.P_1\n",
      "ABG\n",
      "ACS\n",
      "ACX_1\n",
      "ACX_2\n",
      "AENA\n",
      "AGS\n",
      "ALB_2\n",
      "ALB_3\n",
      "ALM\n",
      "ALT\n",
      "AMP\n",
      "AMS_1\n",
      "ANA_1\n",
      "ANA_2\n",
      "ANE\n",
      "BBVA\n",
      "BKIA\n",
      "BKIA_1\n",
      "BKT\n",
      "BME\n",
      "BTO_1\n",
      "BTO_2\n",
      "CABK\n",
      "CIE\n",
      "CIN\n",
      "CLNX\n",
      "COL\n",
      "COL_1\n",
      "CRI_2\n",
      "DIA\n",
      "DRC\n",
      "EBRO\n",
      "EBRO_1\n",
      "ELE\n",
      "ELE_1\n",
      "ENC\n",
      "ENC_1\n",
      "ENG\n",
      "EVA\n",
      "FAD\n",
      "FCC\n",
      "FDR\n",
      "FER\n",
      "GAM\n",
      "GAM_1\n",
      "GRF\n",
      "IAG\n",
      "IBE\n",
      "IBLA\n",
      "IBR\n",
      "IDR\n",
      "ITX\n",
      "JAZ\n",
      "LOG\n",
      "LOR\n",
      "MAP\n",
      "MAP_1\n",
      "MAP_2\n",
      "MAS\n",
      "MEL\n",
      "MRL\n",
      "MTS\n",
      "NHH\n",
      "NHH_2\n",
      "NTGY\n",
      "OHLA\n",
      "PHM\n",
      "POP\n",
      "PRS\n",
      "PRS_1\n",
      "PUL\n",
      "RED\n",
      "REE\n",
      "REE_1\n",
      "REP\n",
      "ROVI\n",
      "SAB\n",
      "SAN\n",
      "SAN_1\n",
      "SCYR\n",
      "SCYR_1\n",
      "SCYR_2\n",
      "SGC\n",
      "SGRE\n",
      "SLR\n",
      "SOL\n",
      "TEF\n",
      "TEM\n",
      "TL5\n",
      "TRE\n",
      "TUB\n",
      "UNF\n",
      "UNI_1\n",
      "VIS\n",
      "VIS_1\n",
      "VIS_2\n",
      "VIS_3\n",
      "ZEL\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame()\n",
    "\n",
    "for c in range(0, returns.shape[1] - 2):\n",
    "\n",
    "    period_data = returns.iloc[:,[c,-2,-1]].dropna()\n",
    "    stock_prices = prices.iloc[1:,c].dropna()\n",
    "    returns_act = period_data.iloc[:,0]\n",
    "    returns_risk_free_rate = period_data.iloc[:,-1]\n",
    "    returns_benchmark = period_data.iloc[:,-2]\n",
    "\n",
    "    for i in range(period_data.shape[0] - DIAS):\n",
    "        # OBTENEMOS EL NOMBRE DEL ACTIVO\n",
    "        stock_name = returns_act.name\n",
    "        \n",
    "        # OBTENEMOS LOS RETORNOS DEL PERIODO ANTERIOR\n",
    "        previous_period_act = returns_act[i:DIAS+i]\n",
    "\n",
    "        # CALCULAMOS LOS RETORNOS TOTALES, RETORNOS ACUMULADOS, RETORNOS DESDE SU ENTRADA AL INDICE\n",
    "        previous_period_total_returns = returns_act[DIAS+i] - returns_act[i]\n",
    "        previous_period_cumulative_returns = returns_act[i:DIAS+i].sum()\n",
    "        total_act_cumulative_returns = returns_act[:DIAS+i].sum()\n",
    "        total_act_returns = returns_act[DIAS+i] - returns_act[0]\n",
    "        # CALCULAMOS LA MEDIA Y LA DESVIACIÓN ESTANDARD DE ESE PERIODO\n",
    "        previous_period_mean_returns = previous_period_act.mean()\n",
    "        previous_period_std_returns = previous_period_act.std()\n",
    "\n",
    "        # CALCULAMOS LOS RETORNOS MAXIMOS Y MINIMOS DEL PERIODO\n",
    "        previous_period_max_returns = previous_period_act.max()\n",
    "        previous_period_min_returns = previous_period_act.min()\n",
    "\n",
    "        # OBTENEMOS LOS DIAS QUE ESTUVO EN EL INDICE HASTA EL MOMENTO\n",
    "        days_in_index = DIAS + i\n",
    "\n",
    "        # OBTENEMOS EL DIA DE ANALISIS\n",
    "        date = previous_period_act.index[-1]\n",
    "        \n",
    "        # CALCULAMOS EL MAXIMO Y EL MINIMO DESDE LA SALIDA AL INDICE\n",
    "        total_act_max_price = stock_prices[:DIAS+i].max()\n",
    "        total_act_min_price = stock_prices[:DIAS+i].min()\n",
    "\n",
    "        days_since_last_max = (stock_prices.index[DIAS+i] - stock_prices[:DIAS+i].loc[stock_prices==total_act_max_price].index).days.values[0]\n",
    "        days_since_last_min = (stock_prices.index[DIAS+i] - stock_prices[:DIAS+i].loc[stock_prices==total_act_min_price].index).days.values[0]\n",
    "\n",
    "        daily_drawdown = (stock_prices[:DIAS+i] / total_act_max_price) - 1\n",
    "        max_drawdown = daily_drawdown.min()\n",
    "\n",
    "        # PARA CALCULAR EL RATIO DE SHARPE DEBEMOS OBTENER LOS DATOS DE LA TASA LIBRE DE RIESGO PARA ESE DIA Y DIVIDIR EL RETORNO MEDIO AJUSTADO POR LA TASA LIBRE DE RIESGO\n",
    "        # POR LA DESVIACIÓN ESTANDAR DE LOS RETORNOS.\n",
    "        period_risk_free_rate = returns_risk_free_rate.iloc[DIAS+i]\n",
    "\n",
    "        # CALCULAMOS EL RATIO DE SHARPE PARA ESTE PERIODO\n",
    "        period_sharpe_ratio = (previous_period_mean_returns - period_risk_free_rate) / previous_period_std_returns\n",
    "\n",
    "        # CALCULAMOS EL RATIO DE CALMAR\n",
    "        calmar_ratio = (previous_period_mean_returns - period_risk_free_rate) / max_drawdown\n",
    "\n",
    "        # CALCULAMOS EL VALUE AT RISK Y EL EXPECTED SHORTFALL\n",
    "        value_at_risk = previous_period_act.quantile(q=VAR_LIMIT)\n",
    "        expected_shortfall = previous_period_act[previous_period_act<value_at_risk].mean()\n",
    "\n",
    "        # CALCULAMOS LA \n",
    "        downside_dev  = np.minimum(1e-9, previous_period_act - period_risk_free_rate) ** 2\n",
    "        downside_dev = np.sqrt(downside_dev.mean())\n",
    "\n",
    "        # CALCULAMOS EL RATIO DE SORTINO\n",
    "        period_sortino_ratio =  (previous_period_mean_returns - period_risk_free_rate) / downside_dev\n",
    "\n",
    "        # CALCULAMOS LA MEDIA Y DESVIACION ESTANDAR DE LOS RETORNOS DEL BENCHMARK\n",
    "        previous_period_benchmark = returns_benchmark[i:DIAS+i]\n",
    "        total_benchmark_returns = returns_benchmark[DIAS+i] - returns_benchmark[0]\n",
    "        previous_period_total_returns_benchmark = returns_benchmark[DIAS+i] - returns_benchmark[i]\n",
    "        previous_period_mean_returns_benchmark = previous_period_benchmark.mean()\n",
    "        previous_period_std_returns_benchmark = previous_period_benchmark.std()\n",
    "\n",
    "        # CALCULAMOS LA BETA DEL ACTIVO CON RESPECTO AL BENCHMARK\n",
    "        market_stock_cov = np.cov(previous_period_act, previous_period_benchmark)\n",
    "        beta = (market_stock_cov[0, 1]/market_stock_cov[1, 1])\n",
    "\n",
    "        # CALCULAMOS EL RATO DE TREYNOR\n",
    "        treynor_ratio = previous_period_mean_returns / beta\n",
    "\n",
    "        # AJUSTAMOS LOS RETORNOS DEL ACTIVO POR LOS RETORNOS DEL BENCHMARK\n",
    "        benchmark_adjusted_returns = (previous_period_act - previous_period_benchmark)\n",
    "        benchmark_adjusted_returns_mean = benchmark_adjusted_returns.mean()\n",
    "        benchmark_adjusted_returns_std = benchmark_adjusted_returns.std()\n",
    "\n",
    "        # CALCULAMOS LA DISTANCIA MEDIA AL BENCHMARK\n",
    "        benchmark_distance = np.sqrt(np.mean((previous_period_benchmark - previous_period_act)**2))\n",
    "\n",
    "        # CALCULAMOS EL INFORMATION RATIO\n",
    "        information_ratio = benchmark_adjusted_returns_mean / benchmark_adjusted_returns_std\n",
    "\n",
    "        # CALCULAMOS LA MEDIDA DE MODIGLIANI\n",
    "        modigliani_measure = ((period_sharpe_ratio * previous_period_std_returns_benchmark) + period_risk_free_rate)\n",
    "\n",
    "        # CALCULAMOS EL ALFA DE JENSEN\n",
    "        period_alfa_jensen = previous_period_total_returns - (period_risk_free_rate - beta * (previous_period_total_returns_benchmark - period_risk_free_rate))\n",
    "\n",
    "        data_period_stock = {\n",
    "            'ticker': stock_name,\n",
    "            'date': date,\n",
    "            'days_in_index': days_in_index,\n",
    "            'period_total_return': previous_period_total_returns,\n",
    "            'calmar_ratio': calmar_ratio,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'period_cumulative_return': previous_period_cumulative_returns,\n",
    "            'total_cumulative_return': total_act_cumulative_returns,\n",
    "            'days_since_last_max': days_since_last_max,\n",
    "            'days_since_last_min': days_since_last_min,\n",
    "            'value_at_risk': value_at_risk,\n",
    "            'expected_shortfall': expected_shortfall,\n",
    "            'period_mean_return': previous_period_mean_returns,\n",
    "            'period_std_return': previous_period_std_returns,\n",
    "            'period_min_return': previous_period_min_returns,\n",
    "            'period_max_return': previous_period_max_returns,\n",
    "            'sharpe_ratio': period_sharpe_ratio,\n",
    "            'sortino_ratio': period_sortino_ratio,\n",
    "            'beta': beta,\n",
    "            'treynor_ratio': treynor_ratio,\n",
    "            'benchmark_distance': benchmark_distance,\n",
    "            'information_ratio': information_ratio,\n",
    "            'modigliani_measure': modigliani_measure,\n",
    "            'alfa_jensen': period_alfa_jensen\n",
    "        }\n",
    "\n",
    "        data_period_stock = pd.DataFrame([data_period_stock])\n",
    "        dataset = pd.concat([dataset, data_period_stock], axis=0)\n",
    "    print(stock_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset[~dataset['ticker'].isin(['risk_free_rate','benchmark'])]\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A3TV', 'ABE', 'ABG.P', 'ABG.P_1', 'ABG', 'ACS', 'ACX_1', 'ACX_2',\n",
       "       'AENA', 'AGS', 'ALB_2', 'ALB_3', 'ALM', 'ALT', 'AMP', 'AMS_1',\n",
       "       'ANA_1', 'ANA_2', 'ANE', 'BBVA', 'BKIA', 'BKIA_1', 'BKT', 'BME',\n",
       "       'BTO_1', 'BTO_2', 'CABK', 'CIE', 'CIN', 'CLNX', 'COL', 'COL_1',\n",
       "       'CRI_2', 'DIA', 'DRC', 'EBRO', 'EBRO_1', 'ELE', 'ELE_1', 'ENC',\n",
       "       'ENC_1', 'ENG', 'EVA', 'FAD', 'FCC', 'FDR', 'FER', 'GAM', 'GAM_1',\n",
       "       'GRF', 'IAG', 'IBE', 'IBLA', 'IBR', 'IDR', 'ITX', 'JAZ', 'LOG',\n",
       "       'LOR', 'MAP', 'MAP_1', 'MAP_2', 'MAS', 'MEL', 'MRL', 'MTS', 'NHH',\n",
       "       'NHH_2', 'NTGY', 'OHLA', 'PHM', 'POP', 'PRS', 'PRS_1', 'PUL',\n",
       "       'RED', 'REE', 'REE_1', 'REP', 'ROVI', 'SAB', 'SAN', 'SAN_1',\n",
       "       'SCYR', 'SCYR_1', 'SCYR_2', 'SGC', 'SGRE', 'SLR', 'SOL', 'TEF',\n",
       "       'TEM', 'TL5', 'TRE', 'TUB', 'UNF', 'UNI_1', 'VIS', 'VIS_1',\n",
       "       'VIS_2', 'VIS_3', 'ZEL'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_list = dataset['ticker'].unique()\n",
    "act_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in act_list:\n",
    "    index = dataset[(dataset['ticker'] == act)].index\n",
    "    dataset.loc[index, 'sharpe_ratio_next_period'] = dataset.loc[index,'sharpe_ratio'].shift(-30)\n",
    "    dataset.loc[index, 'returns_next_period'] = dataset.loc[index,'period_total_return'].shift(-30)\n",
    "    dataset.loc[index, 'alfa_next_period'] = dataset.loc[index,'alfa_jensen'].shift(-30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = dataset[dataset['alfa_jensen']>0].index\n",
    "dataset.loc[indexes,'alfa_signal'] = 1\n",
    "dataset.loc[~dataset.index.isin(indexes),'alfa_signal'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in dataset['ticker'].unique():\n",
    "    ticker_data = dataset[dataset['ticker']==t]\n",
    "    alfa_ticker_data = ticker_data['alfa_signal'].iloc[::-1].rolling(30).sum()\n",
    "    alfa_ticker_data_index = alfa_ticker_data[alfa_ticker_data > 0].index\n",
    "    dataset.loc[alfa_ticker_data_index,'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['label'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>days_in_index</th>\n",
       "      <th>period_total_return</th>\n",
       "      <th>calmar_ratio</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>period_cumulative_return</th>\n",
       "      <th>total_cumulative_return</th>\n",
       "      <th>days_since_last_max</th>\n",
       "      <th>days_since_last_min</th>\n",
       "      <th>...</th>\n",
       "      <th>treynor_ratio</th>\n",
       "      <th>benchmark_distance</th>\n",
       "      <th>information_ratio</th>\n",
       "      <th>modigliani_measure</th>\n",
       "      <th>alfa_jensen</th>\n",
       "      <th>sharpe_ratio_next_period</th>\n",
       "      <th>returns_next_period</th>\n",
       "      <th>alfa_next_period</th>\n",
       "      <th>alfa_signal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-19</td>\n",
       "      <td>30</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.928893</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.084034</td>\n",
       "      <td>-0.084034</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005267</td>\n",
       "      <td>0.011935</td>\n",
       "      <td>-0.343632</td>\n",
       "      <td>0.049597</td>\n",
       "      <td>-0.120066</td>\n",
       "      <td>-8.598475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.109859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-22</td>\n",
       "      <td>31</td>\n",
       "      <td>0.008768</td>\n",
       "      <td>0.924262</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.070826</td>\n",
       "      <td>-0.084034</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003285</td>\n",
       "      <td>0.011286</td>\n",
       "      <td>-0.305949</td>\n",
       "      <td>0.050582</td>\n",
       "      <td>-0.142976</td>\n",
       "      <td>-8.598475</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>-0.105099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-23</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.002416</td>\n",
       "      <td>0.921187</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.062058</td>\n",
       "      <td>-0.080980</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003553</td>\n",
       "      <td>0.011391</td>\n",
       "      <td>-0.252988</td>\n",
       "      <td>0.049042</td>\n",
       "      <td>-0.142941</td>\n",
       "      <td>-8.556868</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>-0.110448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-24</td>\n",
       "      <td>33</td>\n",
       "      <td>-0.041096</td>\n",
       "      <td>0.922035</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.064474</td>\n",
       "      <td>-0.089144</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>0.011217</td>\n",
       "      <td>-0.236099</td>\n",
       "      <td>0.049823</td>\n",
       "      <td>-0.192086</td>\n",
       "      <td>-8.603079</td>\n",
       "      <td>0.019626</td>\n",
       "      <td>-0.089563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-25</td>\n",
       "      <td>34</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.936445</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.105571</td>\n",
       "      <td>-0.114041</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004284</td>\n",
       "      <td>0.011298</td>\n",
       "      <td>-0.328375</td>\n",
       "      <td>0.047374</td>\n",
       "      <td>-0.143515</td>\n",
       "      <td>-9.518182</td>\n",
       "      <td>-0.038972</td>\n",
       "      <td>-0.130236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date  days_in_index  period_total_return  calmar_ratio  \\\n",
       "0   A3TV 2005-08-19             30             0.013208      0.928893   \n",
       "1   A3TV 2005-08-22             31             0.008768      0.924262   \n",
       "2   A3TV 2005-08-23             32            -0.002416      0.921187   \n",
       "3   A3TV 2005-08-24             33            -0.041096      0.922035   \n",
       "4   A3TV 2005-08-25             34             0.016930      0.936445   \n",
       "\n",
       "   max_drawdown  period_cumulative_return  total_cumulative_return  \\\n",
       "0     -0.095061                 -0.084034                -0.084034   \n",
       "1     -0.095061                 -0.070826                -0.084034   \n",
       "2     -0.095061                 -0.062058                -0.080980   \n",
       "3     -0.095061                 -0.064474                -0.089144   \n",
       "4     -0.095061                 -0.105571                -0.114041   \n",
       "\n",
       "   days_since_last_max  days_since_last_min  ...  treynor_ratio  \\\n",
       "0                   31                    2  ...      -0.005267   \n",
       "1                   34                    5  ...      -0.003285   \n",
       "2                   35                    6  ...      -0.003553   \n",
       "3                   36                    7  ...      -0.003209   \n",
       "4                   37                    8  ...      -0.004284   \n",
       "\n",
       "   benchmark_distance  information_ratio  modigliani_measure  alfa_jensen  \\\n",
       "0            0.011935          -0.343632            0.049597    -0.120066   \n",
       "1            0.011286          -0.305949            0.050582    -0.142976   \n",
       "2            0.011391          -0.252988            0.049042    -0.142941   \n",
       "3            0.011217          -0.236099            0.049823    -0.192086   \n",
       "4            0.011298          -0.328375            0.047374    -0.143515   \n",
       "\n",
       "   sharpe_ratio_next_period  returns_next_period  alfa_next_period  \\\n",
       "0                 -8.598475             0.000000         -0.109859   \n",
       "1                 -8.598475             0.002218         -0.105099   \n",
       "2                 -8.556868             0.008163         -0.110448   \n",
       "3                 -8.603079             0.019626         -0.089563   \n",
       "4                 -9.518182            -0.038972         -0.130236   \n",
       "\n",
       "   alfa_signal  label  \n",
       "0          0.0    0.0  \n",
       "1          0.0    0.0  \n",
       "2          0.0    0.0  \n",
       "3          0.0    0.0  \n",
       "4          0.0    0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2 = dataset.copy().dropna(axis=0)\n",
    "dataset_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ITERAMOS PARA TODAS LAS FECHAS CON EL OBJETIVO DE ETIQUETAR LOS DATOS\n",
    "# # NUESTRO CRITERIO SE BASARÁ EN EL RATIO DE SHARPE. AQUELLOS ACTIVOS QUE ESTÉN EN EL TOP 10 DE RATIO DE SHARPE PARA CADA FECHA SE LOS ETIQUETARÁ CON UN 1, MIENTRAS\n",
    "# # QUE AQUELLOS QUE SE ENCUENTREN FUERA DEL TOP 10 PARA CADA FECHA SE LOS ETIQUETARÁ CON UN 0\n",
    "# for i in range(dataset_2.shape[0]):\n",
    "#     if dataset_2.loc[i,'alfa_next_period'] > 0:\n",
    "#         dataset_2.loc[i,'label'] = 1\n",
    "#     else:\n",
    "#         dataset_2.loc[i,'label'] = 0\n",
    "#     print(dataset_2.loc[i,'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_2['days_in_top'] = 0\n",
    "# for t in dataset_2['ticker'].unique():\n",
    "#     dataset_2[dataset['ticker']==t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INICIALIZAMOS UNA SESION EN AWS\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ['S3_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_SECRET_KEY'],\n",
    "    region_name=os.environ['S3_REGION'])\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bucket_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstock-market-historical-data\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_pandas(dataset_2)\n\u001b[0;32m      3\u001b[0m buf \u001b[39m=\u001b[39m BytesIO()\n\u001b[0;32m      4\u001b[0m pq\u001b[39m.\u001b[39mwrite_table(table, buf)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_2' is not defined"
     ]
    }
   ],
   "source": [
    "bucket_name = 'stock-market-historical-data'\n",
    "table = pa.Table.from_pandas(dataset_2)\n",
    "buf = BytesIO()\n",
    "pq.write_table(table, buf)\n",
    "\n",
    "s3.Object(bucket_name, 'categorical_dataset').put(Body=buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'stock-market-historical-data'\n",
    "processed_folder = 'processed/marketstack/'\n",
    "file_name = 'ibex_historical_data'\n",
    "full_path = processed_folder + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_parquet_file(s3, bucket, key):\n",
    "    buffer = BytesIO()\n",
    "    s3.Object(bucket, key).download_fileobj(buffer)\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>days_in_index</th>\n",
       "      <th>period_total_return</th>\n",
       "      <th>calmar_ratio</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>period_cumulative_return</th>\n",
       "      <th>total_cumulative_return</th>\n",
       "      <th>days_since_last_max</th>\n",
       "      <th>days_since_last_min</th>\n",
       "      <th>...</th>\n",
       "      <th>treynor_ratio</th>\n",
       "      <th>benchmark_distance</th>\n",
       "      <th>information_ratio</th>\n",
       "      <th>modigliani_measure</th>\n",
       "      <th>alfa_jensen</th>\n",
       "      <th>sharpe_ratio_next_period</th>\n",
       "      <th>returns_next_period</th>\n",
       "      <th>alfa_next_period</th>\n",
       "      <th>alfa_signal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-19</td>\n",
       "      <td>30</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.928893</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.084034</td>\n",
       "      <td>-0.084034</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005267</td>\n",
       "      <td>0.011935</td>\n",
       "      <td>-0.343632</td>\n",
       "      <td>0.049597</td>\n",
       "      <td>-0.120066</td>\n",
       "      <td>-8.598475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.109859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-22</td>\n",
       "      <td>31</td>\n",
       "      <td>0.008768</td>\n",
       "      <td>0.924262</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.070826</td>\n",
       "      <td>-0.084034</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003285</td>\n",
       "      <td>0.011286</td>\n",
       "      <td>-0.305949</td>\n",
       "      <td>0.050582</td>\n",
       "      <td>-0.142976</td>\n",
       "      <td>-8.598475</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>-0.105099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-23</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.002416</td>\n",
       "      <td>0.921187</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.062058</td>\n",
       "      <td>-0.080980</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003553</td>\n",
       "      <td>0.011391</td>\n",
       "      <td>-0.252988</td>\n",
       "      <td>0.049042</td>\n",
       "      <td>-0.142941</td>\n",
       "      <td>-8.556868</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>-0.110448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-24</td>\n",
       "      <td>33</td>\n",
       "      <td>-0.041096</td>\n",
       "      <td>0.922035</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.064474</td>\n",
       "      <td>-0.089144</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>0.011217</td>\n",
       "      <td>-0.236099</td>\n",
       "      <td>0.049823</td>\n",
       "      <td>-0.192086</td>\n",
       "      <td>-8.603079</td>\n",
       "      <td>0.019626</td>\n",
       "      <td>-0.089563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3TV</td>\n",
       "      <td>2005-08-25</td>\n",
       "      <td>34</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.936445</td>\n",
       "      <td>-0.095061</td>\n",
       "      <td>-0.105571</td>\n",
       "      <td>-0.114041</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004284</td>\n",
       "      <td>0.011298</td>\n",
       "      <td>-0.328375</td>\n",
       "      <td>0.047374</td>\n",
       "      <td>-0.143515</td>\n",
       "      <td>-9.518182</td>\n",
       "      <td>-0.038972</td>\n",
       "      <td>-0.130236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date  days_in_index  period_total_return  calmar_ratio  \\\n",
       "0   A3TV 2005-08-19             30             0.013208      0.928893   \n",
       "1   A3TV 2005-08-22             31             0.008768      0.924262   \n",
       "2   A3TV 2005-08-23             32            -0.002416      0.921187   \n",
       "3   A3TV 2005-08-24             33            -0.041096      0.922035   \n",
       "4   A3TV 2005-08-25             34             0.016930      0.936445   \n",
       "\n",
       "   max_drawdown  period_cumulative_return  total_cumulative_return  \\\n",
       "0     -0.095061                 -0.084034                -0.084034   \n",
       "1     -0.095061                 -0.070826                -0.084034   \n",
       "2     -0.095061                 -0.062058                -0.080980   \n",
       "3     -0.095061                 -0.064474                -0.089144   \n",
       "4     -0.095061                 -0.105571                -0.114041   \n",
       "\n",
       "   days_since_last_max  days_since_last_min  ...  treynor_ratio  \\\n",
       "0                   31                    2  ...      -0.005267   \n",
       "1                   34                    5  ...      -0.003285   \n",
       "2                   35                    6  ...      -0.003553   \n",
       "3                   36                    7  ...      -0.003209   \n",
       "4                   37                    8  ...      -0.004284   \n",
       "\n",
       "   benchmark_distance  information_ratio  modigliani_measure  alfa_jensen  \\\n",
       "0            0.011935          -0.343632            0.049597    -0.120066   \n",
       "1            0.011286          -0.305949            0.050582    -0.142976   \n",
       "2            0.011391          -0.252988            0.049042    -0.142941   \n",
       "3            0.011217          -0.236099            0.049823    -0.192086   \n",
       "4            0.011298          -0.328375            0.047374    -0.143515   \n",
       "\n",
       "   sharpe_ratio_next_period  returns_next_period  alfa_next_period  \\\n",
       "0                 -8.598475             0.000000         -0.109859   \n",
       "1                 -8.598475             0.002218         -0.105099   \n",
       "2                 -8.556868             0.008163         -0.110448   \n",
       "3                 -8.603079             0.019626         -0.089563   \n",
       "4                 -9.518182            -0.038972         -0.130236   \n",
       "\n",
       "   alfa_signal  label  \n",
       "0          0.0    0.0  \n",
       "1          0.0    0.0  \n",
       "2          0.0    0.0  \n",
       "3          0.0    0.0  \n",
       "4          0.0    0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CARGAMOS EL DATAFRAME CON TODOS LOS DATOS DE CIERRE DE TODOS LOS ACTIVOS DESDE 02/01/1991\n",
    "file_name = 'categorical_dataset'\n",
    "dataset = pd.DataFrame(pq.read_table(download_s3_parquet_file(s3, bucket_name,file_name)).to_pandas())\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis_labels = np.sort(dataset_2['label'].unique().astype(int).astype(str))\n",
    "label_count = dataset_2['label'].value_counts()\n",
    "legend_label = ['Stocks with negative alpha', 'Stocks with positive alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1.0    115081\n",
       "0.0     92201\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x_axis_labels, label_count, width=0.4, label=legend_label, color=['red','green'])\n",
    "plt.title('Label Count')\n",
    "plt.legend(loc='upper right');\n",
    "plt.savefig('label_imbalance.png', dpi=300, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker  label\n",
       "IBE     0.0      4199\n",
       "TEF     0.0      4174\n",
       "REP     0.0      4159\n",
       "FCC     0.0      4138\n",
       "ACX_1   0.0      4125\n",
       "                 ... \n",
       "DIA     0.0        29\n",
       "ENC_1   0.0        29\n",
       "ALM     0.0        29\n",
       "MAS     0.0        29\n",
       "BKIA_1  0.0        29\n",
       "Name: count, Length: 190, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2.value_counts(['ticker','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2.sort_values('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_16436\\3786310647.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  train_val_dataset = dataset_2[dataset['date'] < '2021-01-01'].drop(['ticker','date','sharpe_ratio_next_period', 'returns_next_period', 'alfa_next_period','alfa_signal'],axis=1)\n",
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_16436\\3786310647.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  dataset_test = dataset_2[dataset['date'] >= '2021-01-01'].drop(['ticker','date','sharpe_ratio_next_period', 'returns_next_period', 'alfa_next_period','alfa_signal'],axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((184964, 23), (22318, 23))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_dataset = dataset_2[dataset['date'] < '2021-01-01'].drop(['ticker','date','sharpe_ratio_next_period', 'returns_next_period', 'alfa_next_period','alfa_signal'],axis=1)\n",
    "dataset_test = dataset_2[dataset['date'] >= '2021-01-01'].drop(['ticker','date','sharpe_ratio_next_period', 'returns_next_period', 'alfa_next_period','alfa_signal'],axis=1)\n",
    "train_val_dataset.shape, dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(train_dataset, test_dataset, val_split=0.8, training_mode='single',scaling=True):\n",
    "\n",
    "    if training_mode == 'single':\n",
    "        val_split = math.floor(train_dataset.shape[0] * val_split)\n",
    "        train = train_dataset.iloc[:val_split]\n",
    "        val = train_dataset.iloc[val_split:]\n",
    "        train = shuffle(train)\n",
    "        val = shuffle(val)\n",
    "        test = shuffle(test_dataset)\n",
    "\n",
    "        X_train = train.iloc[:,:-1]\n",
    "        X_val = val.iloc[:,:-1]\n",
    "        X_test = test.iloc[:,:-1]\n",
    "        y_train = train.iloc[:,-1]\n",
    "        y_val = val.iloc[:,-1]\n",
    "        y_test = test.iloc[:,-1]\n",
    "        \n",
    "        if scaling:\n",
    "            try:\n",
    "                scaler = StandardScaler()\n",
    "            except:\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "                scaler = StandardScaler()\n",
    "\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_val = scaler.transform(X_val)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    elif training_mode == 'cross_val':\n",
    "        train = shuffle(train_dataset)\n",
    "        test = shuffle(test_dataset)\n",
    "        \n",
    "        X_train = train.iloc[:,:-1]\n",
    "        X_test = test.iloc[:,:-1]\n",
    "        y_train = train.iloc[:,-1]\n",
    "        y_test = test.iloc[:,-1]\n",
    "\n",
    "        if scaling:\n",
    "            try:\n",
    "                scaler = StandardScaler()\n",
    "            except:\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "                scaler = StandardScaler()\n",
    "\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = dataset_split(train_val_dataset, dataset_test, 0.8,'single',True)\n",
    "# X_train, X_test, y_train, y_test = dataset_split(train_val_dataset, dataset_test, 0.8,'cross_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_pred_logr = clf.predict(X_test)\n",
    "y_pred_logr_probas = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred_logr, normalize='all'), index = [0, 1],\n",
    "                  columns = [0, 1])\n",
    "\n",
    "sns.heatmap(df_cm, annot=True,cbar=False)\n",
    "plt.title('Confusion Matrix Logistic Regression Model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, y_pred_logr_probas[:,1])\n",
    "kappa = cohen_kappa_score(y_test, y_pred_logr)\n",
    "accuracy = accuracy_score(y_test, y_pred_logr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_logr_probas[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision, recall,thres = precision_recall_curve(y_test, y_pred_logr_probas[:,1])\n",
    "prec = precision_score(y_test, y_pred_logr)\n",
    "rec = recall_score(y_test, y_pred_logr)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.plot(precision, recall, 'g', label = 'Precision = %0.2f\\nRecall = %0.2f' % (prec, rec))\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([1, 0], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "perm = permutation_importance(clf, X_val, y_val.astype(int), scoring='accuracy', n_repeats=10, random_state=42, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'importances_mean': array([ 4.96310113e-03,  3.50606872e-03,  0.00000000e+00, -8.10964236e-06,\n",
       "        -2.70321412e-05, -2.43289271e-05, -8.10964236e-06,  2.28962236e-03,\n",
       "        -8.10964236e-06,  1.25969778e-03, -2.70321412e-05,  2.92217447e-03,\n",
       "        -1.21644635e-04, -2.70321412e-05, -1.62192847e-05,  3.75746763e-04,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.30294921e-03, -2.16257130e-05,\n",
       "        -1.62192847e-05,  3.01408375e-03]),\n",
       " 'importances_std': array([2.29439031e-04, 1.29105008e-04, 0.00000000e+00, 1.23876833e-05,\n",
       "        0.00000000e+00, 8.10964236e-06, 1.23876833e-05, 2.57032880e-04,\n",
       "        1.23876833e-05, 7.47183249e-05, 0.00000000e+00, 1.35942378e-04,\n",
       "        6.07471820e-05, 0.00000000e+00, 1.32429905e-05, 8.10964236e-06,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.20770459e-04, 1.08128565e-05,\n",
       "        1.32429905e-05, 1.01325466e-04]),\n",
       " 'importances': array([[ 4.62249615e-03,  4.83875328e-03,  4.64952829e-03,\n",
       "          4.78468900e-03,  5.29829968e-03,  4.94688184e-03,\n",
       "          5.27126754e-03,  5.00094612e-03,  5.02797827e-03,\n",
       "          5.19017111e-03],\n",
       "        [ 3.46011408e-03,  3.18979266e-03,  3.51417836e-03,\n",
       "          3.46011408e-03,  3.51417836e-03,  3.56824264e-03,\n",
       "          3.62230692e-03,  3.56824264e-03,  3.46011408e-03,\n",
       "          3.70340335e-03],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00, -2.70321412e-05, -2.70321412e-05,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         -2.70321412e-05],\n",
       "        [-2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05],\n",
       "        [-2.70321412e-05,  0.00000000e+00, -2.70321412e-05,\n",
       "         -2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05],\n",
       "        [ 0.00000000e+00, -2.70321412e-05,  0.00000000e+00,\n",
       "         -2.70321412e-05,  0.00000000e+00, -2.70321412e-05,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 2.75727840e-03,  2.18960344e-03,  2.05444273e-03,\n",
       "          1.94631417e-03,  2.54102127e-03,  2.29773200e-03,\n",
       "          2.29773200e-03,  2.59508556e-03,  1.97334631e-03,\n",
       "          2.24366772e-03],\n",
       "        [ 0.00000000e+00, -2.70321412e-05, -2.70321412e-05,\n",
       "          0.00000000e+00, -2.70321412e-05,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 1.21644635e-03,  1.37863920e-03,  1.27051064e-03,\n",
       "          1.21644635e-03,  1.18941421e-03,  1.16238207e-03,\n",
       "          1.37863920e-03,  1.18941421e-03,  1.32457492e-03,\n",
       "          1.27051064e-03],\n",
       "        [-2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05],\n",
       "        [ 2.94650339e-03,  2.81134269e-03,  2.83837483e-03,\n",
       "          2.81134269e-03,  2.83837483e-03,  2.86540697e-03,\n",
       "          2.81134269e-03,  3.18979266e-03,  2.94650339e-03,\n",
       "          3.16276052e-03],\n",
       "        [-1.08128565e-04, -2.43289271e-04, -1.08128565e-04,\n",
       "         -1.08128565e-04, -1.89224989e-04, -1.35160706e-04,\n",
       "         -1.62192847e-04, -5.40642824e-05, -2.70321412e-05,\n",
       "         -8.10964236e-05],\n",
       "        [-2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05],\n",
       "        [-2.70321412e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "         -2.70321412e-05,  0.00000000e+00, -2.70321412e-05,\n",
       "         -2.70321412e-05,  0.00000000e+00, -2.70321412e-05,\n",
       "         -2.70321412e-05],\n",
       "        [ 3.78449977e-04,  3.78449977e-04,  3.78449977e-04,\n",
       "          3.78449977e-04,  3.78449977e-04,  3.78449977e-04,\n",
       "          3.78449977e-04,  3.78449977e-04,  3.78449977e-04,\n",
       "          3.51417836e-04],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 1.27051064e-03,  1.18941421e-03,  1.37863920e-03,\n",
       "          1.08128565e-03,  1.40567134e-03,  1.27051064e-03,\n",
       "          1.37863920e-03,  1.18941421e-03,  1.35160706e-03,\n",
       "          1.51379991e-03],\n",
       "        [-2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "         -2.70321412e-05, -2.70321412e-05,  0.00000000e+00,\n",
       "         -2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "          0.00000000e+00],\n",
       "        [-2.70321412e-05, -2.70321412e-05, -2.70321412e-05,\n",
       "          0.00000000e+00, -2.70321412e-05, -2.70321412e-05,\n",
       "          0.00000000e+00, -2.70321412e-05,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 2.94650339e-03,  2.97353553e-03,  2.91947125e-03,\n",
       "          2.86540697e-03,  3.18979266e-03,  2.97353553e-03,\n",
       "          3.10869624e-03,  3.16276052e-03,  3.02759982e-03,\n",
       "          2.97353553e-03]])}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[294], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m feature_importances \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(perm\u001b[39m.\u001b[39;49mimportances_mean\u001b[39m.\u001b[39;49msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), index\u001b[39m=\u001b[39mtrain_val_dataset\u001b[39m.\u001b[39miloc[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m      3\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[0;32m      4\u001b[0m feature_importances\u001b[39m.\u001b[39mplot\u001b[39m.\u001b[39mbar(yerr\u001b[39m=\u001b[39mperm\u001b[39m.\u001b[39mimportances_std, ax\u001b[39m=\u001b[39max)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sort_values'"
     ]
    }
   ],
   "source": [
    "feature_importances = pd.Series(perm.importances_mean.sort_values(ascending=False), index=train_val_dataset.iloc[:,:-1].columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances.plot.bar(yerr=perm.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "days_in_index          0.004963\n",
       "period_total_return    0.003506\n",
       "days_since_last_min    0.002290\n",
       "expected_shortfall     0.001260\n",
       "period_std_return      0.002922\n",
       "sortino_ratio          0.000376\n",
       "benchmark_distance     0.001303\n",
       "alfa_jensen            0.003014\n",
       "dtype: float64"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = feature_importances[feature_importances > 0]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input = Input(shape=(X_train_scaled.shape[1],))\n",
    "    \n",
    "    layer_1 = Dense(12, activation='relu', kernel_regularizer=L2(0.0001))(input)\n",
    "    dropout_1 = Dropout(0.1)(layer_1)\n",
    "    layer_2 = Dense(6, activation='relu', kernel_regularizer=L2(0.0001))(dropout_1)\n",
    "    dropout_2 = Dropout(0.1)(layer_2)\n",
    "    layer_3 = Dense(3, activation='relu', kernel_regularizer=L2(0.0001))(dropout_2)\n",
    "    dropout_3 = Dropout(0.1)(layer_3)\n",
    "    flatten = Flatten()(dropout_3)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    tf.keras.utils.plot_model(\n",
    "        model,\n",
    "        to_file=\"fixed_model.png\",\n",
    "        show_shapes=True,\n",
    "        dpi=300)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    # mode='min',\n",
    "    restore_best_weights=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/100\n",
      "3330/3330 [==============================] - 11s 3ms/step - loss: 0.4921 - accuracy: 0.7933 - val_loss: 0.2185 - val_accuracy: 0.9496\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2594 - accuracy: 0.8832 - val_loss: 0.1489 - val_accuracy: 0.9580\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2356 - accuracy: 0.8970 - val_loss: 0.1426 - val_accuracy: 0.9596\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2239 - accuracy: 0.9190 - val_loss: 0.1393 - val_accuracy: 0.9602\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2182 - accuracy: 0.9199 - val_loss: 0.1374 - val_accuracy: 0.9607\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2053 - accuracy: 0.9205 - val_loss: 0.1356 - val_accuracy: 0.9607\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1923 - accuracy: 0.9215 - val_loss: 0.1346 - val_accuracy: 0.9612\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1851 - accuracy: 0.9209 - val_loss: 0.1333 - val_accuracy: 0.9613\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1809 - accuracy: 0.9489 - val_loss: 0.1323 - val_accuracy: 0.9617\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1794 - accuracy: 0.9515 - val_loss: 0.1316 - val_accuracy: 0.9620\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1761 - accuracy: 0.9512 - val_loss: 0.1306 - val_accuracy: 0.9625\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1755 - accuracy: 0.9521 - val_loss: 0.1303 - val_accuracy: 0.9625\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1728 - accuracy: 0.9530 - val_loss: 0.1296 - val_accuracy: 0.9624\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1711 - accuracy: 0.9527 - val_loss: 0.1288 - val_accuracy: 0.9627\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1707 - accuracy: 0.9530 - val_loss: 0.1283 - val_accuracy: 0.9627\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1706 - accuracy: 0.9525 - val_loss: 0.1280 - val_accuracy: 0.9629\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1686 - accuracy: 0.9532 - val_loss: 0.1275 - val_accuracy: 0.9631\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1675 - accuracy: 0.9533 - val_loss: 0.1269 - val_accuracy: 0.9633\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1657 - accuracy: 0.9535 - val_loss: 0.1268 - val_accuracy: 0.9632\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1678 - accuracy: 0.9540 - val_loss: 0.1265 - val_accuracy: 0.9632\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1657 - accuracy: 0.9537 - val_loss: 0.1259 - val_accuracy: 0.9631\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1652 - accuracy: 0.9535 - val_loss: 0.1258 - val_accuracy: 0.9634\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1646 - accuracy: 0.9533 - val_loss: 0.1257 - val_accuracy: 0.9637\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1637 - accuracy: 0.9534 - val_loss: 0.1252 - val_accuracy: 0.9636\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1635 - accuracy: 0.9536 - val_loss: 0.1252 - val_accuracy: 0.9638\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1634 - accuracy: 0.9541 - val_loss: 0.1250 - val_accuracy: 0.9635\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1637 - accuracy: 0.9535 - val_loss: 0.1247 - val_accuracy: 0.9639\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1634 - accuracy: 0.9540 - val_loss: 0.1247 - val_accuracy: 0.9640\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1632 - accuracy: 0.9536 - val_loss: 0.1245 - val_accuracy: 0.9638\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1626 - accuracy: 0.9542 - val_loss: 0.1240 - val_accuracy: 0.9638\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1619 - accuracy: 0.9535 - val_loss: 0.1238 - val_accuracy: 0.9638\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1635 - accuracy: 0.9539 - val_loss: 0.1238 - val_accuracy: 0.9637\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1622 - accuracy: 0.9536 - val_loss: 0.1240 - val_accuracy: 0.9639\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1608 - accuracy: 0.9547 - val_loss: 0.1237 - val_accuracy: 0.9640\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1611 - accuracy: 0.9544 - val_loss: 0.1234 - val_accuracy: 0.9642\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1608 - accuracy: 0.9542 - val_loss: 0.1230 - val_accuracy: 0.9640\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1597 - accuracy: 0.9546 - val_loss: 0.1231 - val_accuracy: 0.9641\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1608 - accuracy: 0.9541 - val_loss: 0.1232 - val_accuracy: 0.9639\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1605 - accuracy: 0.9545 - val_loss: 0.1229 - val_accuracy: 0.9640\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1607 - accuracy: 0.9545 - val_loss: 0.1231 - val_accuracy: 0.9640\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1602 - accuracy: 0.9539 - val_loss: 0.1229 - val_accuracy: 0.9640\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1588 - accuracy: 0.9551 - val_loss: 0.1224 - val_accuracy: 0.9640\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1597 - accuracy: 0.9546 - val_loss: 0.1229 - val_accuracy: 0.9641\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1595 - accuracy: 0.9543 - val_loss: 0.1226 - val_accuracy: 0.9640\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1587 - accuracy: 0.9547 - val_loss: 0.1225 - val_accuracy: 0.9640\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1574 - accuracy: 0.9552 - val_loss: 0.1224 - val_accuracy: 0.9637\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1598 - accuracy: 0.9541 - val_loss: 0.1220 - val_accuracy: 0.9641\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1593 - accuracy: 0.9543 - val_loss: 0.1219 - val_accuracy: 0.9639\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1577 - accuracy: 0.9542 - val_loss: 0.1215 - val_accuracy: 0.9641\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1580 - accuracy: 0.9539 - val_loss: 0.1220 - val_accuracy: 0.9638\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1586 - accuracy: 0.9541 - val_loss: 0.1216 - val_accuracy: 0.9644\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1576 - accuracy: 0.9548 - val_loss: 0.1216 - val_accuracy: 0.9643\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1574 - accuracy: 0.9543 - val_loss: 0.1212 - val_accuracy: 0.9643\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1579 - accuracy: 0.9549 - val_loss: 0.1209 - val_accuracy: 0.9646\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1582 - accuracy: 0.9549 - val_loss: 0.1210 - val_accuracy: 0.9644\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1572 - accuracy: 0.9549 - val_loss: 0.1208 - val_accuracy: 0.9643\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1581 - accuracy: 0.9551 - val_loss: 0.1207 - val_accuracy: 0.9645\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1572 - accuracy: 0.9548 - val_loss: 0.1207 - val_accuracy: 0.9644\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1563 - accuracy: 0.9549 - val_loss: 0.1210 - val_accuracy: 0.9643\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1576 - accuracy: 0.9541 - val_loss: 0.1207 - val_accuracy: 0.9644\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1558 - accuracy: 0.9544 - val_loss: 0.1204 - val_accuracy: 0.9644\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1551 - accuracy: 0.9555 - val_loss: 0.1206 - val_accuracy: 0.9644\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1568 - accuracy: 0.9546 - val_loss: 0.1205 - val_accuracy: 0.9644\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1570 - accuracy: 0.9545 - val_loss: 0.1201 - val_accuracy: 0.9647\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9545 - val_loss: 0.1204 - val_accuracy: 0.9644\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1550 - accuracy: 0.9552 - val_loss: 0.1201 - val_accuracy: 0.9643\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1554 - accuracy: 0.9551 - val_loss: 0.1199 - val_accuracy: 0.9643\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1560 - accuracy: 0.9554 - val_loss: 0.1202 - val_accuracy: 0.9645\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1547 - accuracy: 0.9556 - val_loss: 0.1201 - val_accuracy: 0.9643\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1548 - accuracy: 0.9556 - val_loss: 0.1200 - val_accuracy: 0.9643\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1559 - accuracy: 0.9552 - val_loss: 0.1198 - val_accuracy: 0.9644\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1558 - accuracy: 0.9549 - val_loss: 0.1198 - val_accuracy: 0.9643\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1557 - accuracy: 0.9549 - val_loss: 0.1196 - val_accuracy: 0.9644\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1552 - accuracy: 0.9545 - val_loss: 0.1196 - val_accuracy: 0.9644\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1558 - accuracy: 0.9544 - val_loss: 0.1194 - val_accuracy: 0.9645\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1545 - accuracy: 0.9553 - val_loss: 0.1193 - val_accuracy: 0.9646\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1557 - accuracy: 0.9548 - val_loss: 0.1195 - val_accuracy: 0.9646\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1548 - accuracy: 0.9555 - val_loss: 0.1193 - val_accuracy: 0.9644\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9548 - val_loss: 0.1194 - val_accuracy: 0.9645\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1540 - accuracy: 0.9555 - val_loss: 0.1190 - val_accuracy: 0.9646\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1546 - accuracy: 0.9546 - val_loss: 0.1189 - val_accuracy: 0.9646\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1545 - accuracy: 0.9547 - val_loss: 0.1192 - val_accuracy: 0.9642\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1552 - accuracy: 0.9555 - val_loss: 0.1188 - val_accuracy: 0.9644\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1536 - accuracy: 0.9554 - val_loss: 0.1190 - val_accuracy: 0.9645\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1546 - accuracy: 0.9552 - val_loss: 0.1188 - val_accuracy: 0.9645\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1533 - accuracy: 0.9552 - val_loss: 0.1191 - val_accuracy: 0.9646\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1552 - accuracy: 0.9552 - val_loss: 0.1189 - val_accuracy: 0.9647\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1540 - accuracy: 0.9553 - val_loss: 0.1187 - val_accuracy: 0.9644\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1538 - accuracy: 0.9552 - val_loss: 0.1189 - val_accuracy: 0.9646\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1530 - accuracy: 0.9551 - val_loss: 0.1186 - val_accuracy: 0.9644\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1526 - accuracy: 0.9549 - val_loss: 0.1183 - val_accuracy: 0.9647\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1542 - accuracy: 0.9545 - val_loss: 0.1184 - val_accuracy: 0.9646\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1532 - accuracy: 0.9550 - val_loss: 0.1183 - val_accuracy: 0.9650\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1541 - accuracy: 0.9544 - val_loss: 0.1184 - val_accuracy: 0.9649\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1530 - accuracy: 0.9552 - val_loss: 0.1183 - val_accuracy: 0.9647\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1536 - accuracy: 0.9552 - val_loss: 0.1183 - val_accuracy: 0.9645\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1544 - accuracy: 0.9550 - val_loss: 0.1182 - val_accuracy: 0.9644\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1525 - accuracy: 0.9553 - val_loss: 0.1182 - val_accuracy: 0.9644\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1531 - accuracy: 0.9550 - val_loss: 0.1182 - val_accuracy: 0.9649\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1520 - accuracy: 0.9551 - val_loss: 0.1182 - val_accuracy: 0.9646\n",
      "463/463 [==============================] - 1s 1ms/step\n",
      "4162/4162 [==============================] - 7s 2ms/step\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3330/3330 [==============================] - 9s 2ms/step - loss: 0.5108 - accuracy: 0.7646 - val_loss: 0.2291 - val_accuracy: 0.9457\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2410 - accuracy: 0.9040 - val_loss: 0.1568 - val_accuracy: 0.9553\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2095 - accuracy: 0.9123 - val_loss: 0.1465 - val_accuracy: 0.9579\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1988 - accuracy: 0.9142 - val_loss: 0.1417 - val_accuracy: 0.9585\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1949 - accuracy: 0.9408 - val_loss: 0.1386 - val_accuracy: 0.9594\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1879 - accuracy: 0.9460 - val_loss: 0.1366 - val_accuracy: 0.9603\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1868 - accuracy: 0.9467 - val_loss: 0.1352 - val_accuracy: 0.9608\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1833 - accuracy: 0.9473 - val_loss: 0.1340 - val_accuracy: 0.9615\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1827 - accuracy: 0.9473 - val_loss: 0.1332 - val_accuracy: 0.9614\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1793 - accuracy: 0.9486 - val_loss: 0.1326 - val_accuracy: 0.9617\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1781 - accuracy: 0.9475 - val_loss: 0.1319 - val_accuracy: 0.9609\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1778 - accuracy: 0.9490 - val_loss: 0.1314 - val_accuracy: 0.9622\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1750 - accuracy: 0.9492 - val_loss: 0.1309 - val_accuracy: 0.9618\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1736 - accuracy: 0.9490 - val_loss: 0.1305 - val_accuracy: 0.9620\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1730 - accuracy: 0.9492 - val_loss: 0.1301 - val_accuracy: 0.9625\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1722 - accuracy: 0.9498 - val_loss: 0.1299 - val_accuracy: 0.9625\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1715 - accuracy: 0.9499 - val_loss: 0.1296 - val_accuracy: 0.9625\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1702 - accuracy: 0.9498 - val_loss: 0.1291 - val_accuracy: 0.9626\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1680 - accuracy: 0.9505 - val_loss: 0.1290 - val_accuracy: 0.9628\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1694 - accuracy: 0.9503 - val_loss: 0.1285 - val_accuracy: 0.9631\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1681 - accuracy: 0.9511 - val_loss: 0.1286 - val_accuracy: 0.9630\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1668 - accuracy: 0.9512 - val_loss: 0.1285 - val_accuracy: 0.9629\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1657 - accuracy: 0.9508 - val_loss: 0.1283 - val_accuracy: 0.9635\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1665 - accuracy: 0.9505 - val_loss: 0.1282 - val_accuracy: 0.9632\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1655 - accuracy: 0.9505 - val_loss: 0.1278 - val_accuracy: 0.9636\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1651 - accuracy: 0.9508 - val_loss: 0.1278 - val_accuracy: 0.9634\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1652 - accuracy: 0.9500 - val_loss: 0.1276 - val_accuracy: 0.9633\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1659 - accuracy: 0.9505 - val_loss: 0.1273 - val_accuracy: 0.9635\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1632 - accuracy: 0.9510 - val_loss: 0.1268 - val_accuracy: 0.9632\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1652 - accuracy: 0.9513 - val_loss: 0.1269 - val_accuracy: 0.9634\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1644 - accuracy: 0.9529 - val_loss: 0.1265 - val_accuracy: 0.9636\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1631 - accuracy: 0.9531 - val_loss: 0.1262 - val_accuracy: 0.9640\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1622 - accuracy: 0.9529 - val_loss: 0.1263 - val_accuracy: 0.9634\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1612 - accuracy: 0.9537 - val_loss: 0.1260 - val_accuracy: 0.9633\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1613 - accuracy: 0.9534 - val_loss: 0.1260 - val_accuracy: 0.9637\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1614 - accuracy: 0.9536 - val_loss: 0.1259 - val_accuracy: 0.9637\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1591 - accuracy: 0.9546 - val_loss: 0.1259 - val_accuracy: 0.9638\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1606 - accuracy: 0.9530 - val_loss: 0.1255 - val_accuracy: 0.9640\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1601 - accuracy: 0.9538 - val_loss: 0.1254 - val_accuracy: 0.9640\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1588 - accuracy: 0.9545 - val_loss: 0.1253 - val_accuracy: 0.9636\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1602 - accuracy: 0.9537 - val_loss: 0.1255 - val_accuracy: 0.9638\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1583 - accuracy: 0.9543 - val_loss: 0.1252 - val_accuracy: 0.9639\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1585 - accuracy: 0.9542 - val_loss: 0.1253 - val_accuracy: 0.9637\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1592 - accuracy: 0.9535 - val_loss: 0.1253 - val_accuracy: 0.9639\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1578 - accuracy: 0.9539 - val_loss: 0.1249 - val_accuracy: 0.9642\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1590 - accuracy: 0.9536 - val_loss: 0.1247 - val_accuracy: 0.9640\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1581 - accuracy: 0.9537 - val_loss: 0.1251 - val_accuracy: 0.9638\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1572 - accuracy: 0.9544 - val_loss: 0.1247 - val_accuracy: 0.9637\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1578 - accuracy: 0.9539 - val_loss: 0.1244 - val_accuracy: 0.9637\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1571 - accuracy: 0.9547 - val_loss: 0.1242 - val_accuracy: 0.9634\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1581 - accuracy: 0.9539 - val_loss: 0.1246 - val_accuracy: 0.9639\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1580 - accuracy: 0.9542 - val_loss: 0.1242 - val_accuracy: 0.9639\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1561 - accuracy: 0.9546 - val_loss: 0.1240 - val_accuracy: 0.9638\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1571 - accuracy: 0.9537 - val_loss: 0.1237 - val_accuracy: 0.9643\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1564 - accuracy: 0.9546 - val_loss: 0.1237 - val_accuracy: 0.9640\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1558 - accuracy: 0.9549 - val_loss: 0.1238 - val_accuracy: 0.9641\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1572 - accuracy: 0.9543 - val_loss: 0.1235 - val_accuracy: 0.9640\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1565 - accuracy: 0.9542 - val_loss: 0.1235 - val_accuracy: 0.9640\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1570 - accuracy: 0.9542 - val_loss: 0.1234 - val_accuracy: 0.9643\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1564 - accuracy: 0.9539 - val_loss: 0.1236 - val_accuracy: 0.9642\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1563 - accuracy: 0.9545 - val_loss: 0.1234 - val_accuracy: 0.9642\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1569 - accuracy: 0.9542 - val_loss: 0.1234 - val_accuracy: 0.9640\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9545 - val_loss: 0.1233 - val_accuracy: 0.9641\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1568 - accuracy: 0.9544 - val_loss: 0.1231 - val_accuracy: 0.9641\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1563 - accuracy: 0.9537 - val_loss: 0.1229 - val_accuracy: 0.9645\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1571 - accuracy: 0.9546 - val_loss: 0.1231 - val_accuracy: 0.9641\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9545 - val_loss: 0.1232 - val_accuracy: 0.9642\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1555 - accuracy: 0.9547 - val_loss: 0.1228 - val_accuracy: 0.9644\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1571 - accuracy: 0.9541 - val_loss: 0.1230 - val_accuracy: 0.9641\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1549 - accuracy: 0.9545 - val_loss: 0.1230 - val_accuracy: 0.9644\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1552 - accuracy: 0.9542 - val_loss: 0.1228 - val_accuracy: 0.9642\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1548 - accuracy: 0.9548 - val_loss: 0.1228 - val_accuracy: 0.9645\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1549 - accuracy: 0.9544 - val_loss: 0.1228 - val_accuracy: 0.9644\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1561 - accuracy: 0.9544 - val_loss: 0.1228 - val_accuracy: 0.9644\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1552 - accuracy: 0.9545 - val_loss: 0.1224 - val_accuracy: 0.9643\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1552 - accuracy: 0.9548 - val_loss: 0.1226 - val_accuracy: 0.9645\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1544 - accuracy: 0.9550 - val_loss: 0.1224 - val_accuracy: 0.9644\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1549 - accuracy: 0.9550 - val_loss: 0.1221 - val_accuracy: 0.9646\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1561 - accuracy: 0.9543 - val_loss: 0.1222 - val_accuracy: 0.9648\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9551 - val_loss: 0.1226 - val_accuracy: 0.9643\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1542 - accuracy: 0.9553 - val_loss: 0.1220 - val_accuracy: 0.9643\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1555 - accuracy: 0.9549 - val_loss: 0.1219 - val_accuracy: 0.9646\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1552 - accuracy: 0.9550 - val_loss: 0.1218 - val_accuracy: 0.9645\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1560 - accuracy: 0.9544 - val_loss: 0.1217 - val_accuracy: 0.9648\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1541 - accuracy: 0.9555 - val_loss: 0.1216 - val_accuracy: 0.9650\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1548 - accuracy: 0.9549 - val_loss: 0.1217 - val_accuracy: 0.9646\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1538 - accuracy: 0.9553 - val_loss: 0.1220 - val_accuracy: 0.9646\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1523 - accuracy: 0.9556 - val_loss: 0.1217 - val_accuracy: 0.9644\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1547 - accuracy: 0.9547 - val_loss: 0.1218 - val_accuracy: 0.9649\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1539 - accuracy: 0.9548 - val_loss: 0.1213 - val_accuracy: 0.9646\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1540 - accuracy: 0.9552 - val_loss: 0.1213 - val_accuracy: 0.9643\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1530 - accuracy: 0.9555 - val_loss: 0.1213 - val_accuracy: 0.9649\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1534 - accuracy: 0.9553 - val_loss: 0.1212 - val_accuracy: 0.9648\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1544 - accuracy: 0.9549 - val_loss: 0.1212 - val_accuracy: 0.9645\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1541 - accuracy: 0.9552 - val_loss: 0.1211 - val_accuracy: 0.9649\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1553 - accuracy: 0.9549 - val_loss: 0.1210 - val_accuracy: 0.9649\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1534 - accuracy: 0.9557 - val_loss: 0.1210 - val_accuracy: 0.9646\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1526 - accuracy: 0.9555 - val_loss: 0.1209 - val_accuracy: 0.9649\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1542 - accuracy: 0.9553 - val_loss: 0.1206 - val_accuracy: 0.9653\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1545 - accuracy: 0.9554 - val_loss: 0.1207 - val_accuracy: 0.9650\n",
      "463/463 [==============================] - 1s 2ms/step\n",
      "4162/4162 [==============================] - 6s 2ms/step\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330/3330 [==============================] - 9s 2ms/step - loss: 0.4977 - accuracy: 0.7856 - val_loss: 0.2641 - val_accuracy: 0.9448\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2422 - accuracy: 0.9312 - val_loss: 0.1627 - val_accuracy: 0.9525\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2051 - accuracy: 0.9408 - val_loss: 0.1500 - val_accuracy: 0.9559\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1951 - accuracy: 0.9442 - val_loss: 0.1444 - val_accuracy: 0.9582\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1883 - accuracy: 0.9461 - val_loss: 0.1411 - val_accuracy: 0.9591\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1843 - accuracy: 0.9470 - val_loss: 0.1392 - val_accuracy: 0.9592\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1816 - accuracy: 0.9482 - val_loss: 0.1375 - val_accuracy: 0.9598\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1797 - accuracy: 0.9483 - val_loss: 0.1364 - val_accuracy: 0.9599\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1785 - accuracy: 0.9477 - val_loss: 0.1355 - val_accuracy: 0.9601\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1772 - accuracy: 0.9492 - val_loss: 0.1350 - val_accuracy: 0.9604\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1765 - accuracy: 0.9486 - val_loss: 0.1340 - val_accuracy: 0.9606\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1740 - accuracy: 0.9490 - val_loss: 0.1336 - val_accuracy: 0.9609\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1728 - accuracy: 0.9492 - val_loss: 0.1329 - val_accuracy: 0.9612\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1732 - accuracy: 0.9493 - val_loss: 0.1324 - val_accuracy: 0.9615\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1730 - accuracy: 0.9493 - val_loss: 0.1320 - val_accuracy: 0.9614\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1726 - accuracy: 0.9496 - val_loss: 0.1314 - val_accuracy: 0.9619\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1719 - accuracy: 0.9498 - val_loss: 0.1312 - val_accuracy: 0.9621\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1717 - accuracy: 0.9498 - val_loss: 0.1306 - val_accuracy: 0.9623\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1709 - accuracy: 0.9499 - val_loss: 0.1303 - val_accuracy: 0.9623\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1701 - accuracy: 0.9501 - val_loss: 0.1300 - val_accuracy: 0.9623\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1681 - accuracy: 0.9504 - val_loss: 0.1296 - val_accuracy: 0.9624\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1679 - accuracy: 0.9507 - val_loss: 0.1293 - val_accuracy: 0.9626\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1674 - accuracy: 0.9504 - val_loss: 0.1293 - val_accuracy: 0.9631\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1681 - accuracy: 0.9507 - val_loss: 0.1292 - val_accuracy: 0.9630\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1664 - accuracy: 0.9506 - val_loss: 0.1288 - val_accuracy: 0.9630\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1669 - accuracy: 0.9505 - val_loss: 0.1288 - val_accuracy: 0.9629\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1664 - accuracy: 0.9507 - val_loss: 0.1285 - val_accuracy: 0.9631\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1654 - accuracy: 0.9514 - val_loss: 0.1285 - val_accuracy: 0.9629\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1647 - accuracy: 0.9519 - val_loss: 0.1281 - val_accuracy: 0.9631\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1648 - accuracy: 0.9538 - val_loss: 0.1283 - val_accuracy: 0.9634\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1649 - accuracy: 0.9533 - val_loss: 0.1280 - val_accuracy: 0.9632\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1649 - accuracy: 0.9532 - val_loss: 0.1275 - val_accuracy: 0.9631\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1640 - accuracy: 0.9541 - val_loss: 0.1276 - val_accuracy: 0.9633\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1635 - accuracy: 0.9538 - val_loss: 0.1274 - val_accuracy: 0.9637\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1637 - accuracy: 0.9541 - val_loss: 0.1273 - val_accuracy: 0.9634\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1626 - accuracy: 0.9543 - val_loss: 0.1272 - val_accuracy: 0.9637\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1624 - accuracy: 0.9545 - val_loss: 0.1269 - val_accuracy: 0.9636\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1630 - accuracy: 0.9548 - val_loss: 0.1269 - val_accuracy: 0.9640\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1625 - accuracy: 0.9550 - val_loss: 0.1271 - val_accuracy: 0.9635\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1620 - accuracy: 0.9541 - val_loss: 0.1269 - val_accuracy: 0.9641\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1625 - accuracy: 0.9542 - val_loss: 0.1266 - val_accuracy: 0.9643\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1625 - accuracy: 0.9546 - val_loss: 0.1265 - val_accuracy: 0.9641\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1622 - accuracy: 0.9539 - val_loss: 0.1263 - val_accuracy: 0.9644\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1615 - accuracy: 0.9548 - val_loss: 0.1261 - val_accuracy: 0.9644\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1618 - accuracy: 0.9541 - val_loss: 0.1263 - val_accuracy: 0.9643\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1621 - accuracy: 0.9545 - val_loss: 0.1262 - val_accuracy: 0.9640\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1617 - accuracy: 0.9544 - val_loss: 0.1262 - val_accuracy: 0.9641\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1607 - accuracy: 0.9539 - val_loss: 0.1260 - val_accuracy: 0.9644\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1608 - accuracy: 0.9547 - val_loss: 0.1258 - val_accuracy: 0.9644\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1604 - accuracy: 0.9546 - val_loss: 0.1256 - val_accuracy: 0.9642\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1607 - accuracy: 0.9544 - val_loss: 0.1253 - val_accuracy: 0.9643\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1617 - accuracy: 0.9546 - val_loss: 0.1252 - val_accuracy: 0.9644\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1597 - accuracy: 0.9551 - val_loss: 0.1257 - val_accuracy: 0.9643\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1594 - accuracy: 0.9548 - val_loss: 0.1254 - val_accuracy: 0.9640\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1599 - accuracy: 0.9553 - val_loss: 0.1255 - val_accuracy: 0.9643\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1601 - accuracy: 0.9552 - val_loss: 0.1250 - val_accuracy: 0.9647\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1596 - accuracy: 0.9551 - val_loss: 0.1252 - val_accuracy: 0.9647\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1595 - accuracy: 0.9552 - val_loss: 0.1250 - val_accuracy: 0.9644\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1586 - accuracy: 0.9550 - val_loss: 0.1246 - val_accuracy: 0.9648\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1603 - accuracy: 0.9552 - val_loss: 0.1251 - val_accuracy: 0.9650\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1589 - accuracy: 0.9555 - val_loss: 0.1244 - val_accuracy: 0.9651\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1597 - accuracy: 0.9548 - val_loss: 0.1247 - val_accuracy: 0.9648\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1589 - accuracy: 0.9551 - val_loss: 0.1243 - val_accuracy: 0.9651\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1600 - accuracy: 0.9553 - val_loss: 0.1242 - val_accuracy: 0.9649\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1596 - accuracy: 0.9552 - val_loss: 0.1247 - val_accuracy: 0.9650\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1576 - accuracy: 0.9555 - val_loss: 0.1244 - val_accuracy: 0.9653\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1577 - accuracy: 0.9553 - val_loss: 0.1246 - val_accuracy: 0.9650\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1592 - accuracy: 0.9551 - val_loss: 0.1242 - val_accuracy: 0.9647\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1594 - accuracy: 0.9550 - val_loss: 0.1242 - val_accuracy: 0.9649\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1590 - accuracy: 0.9550 - val_loss: 0.1238 - val_accuracy: 0.9647\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1584 - accuracy: 0.9555 - val_loss: 0.1238 - val_accuracy: 0.9653\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1566 - accuracy: 0.9559 - val_loss: 0.1238 - val_accuracy: 0.9647\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1580 - accuracy: 0.9553 - val_loss: 0.1237 - val_accuracy: 0.9651\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1582 - accuracy: 0.9554 - val_loss: 0.1239 - val_accuracy: 0.9651\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1567 - accuracy: 0.9559 - val_loss: 0.1236 - val_accuracy: 0.9648\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1578 - accuracy: 0.9553 - val_loss: 0.1239 - val_accuracy: 0.9649\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1553 - accuracy: 0.9561 - val_loss: 0.1231 - val_accuracy: 0.9650\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1566 - accuracy: 0.9558 - val_loss: 0.1231 - val_accuracy: 0.9651\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1555 - accuracy: 0.9557 - val_loss: 0.1233 - val_accuracy: 0.9650\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1574 - accuracy: 0.9557 - val_loss: 0.1230 - val_accuracy: 0.9654\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1563 - accuracy: 0.9559 - val_loss: 0.1229 - val_accuracy: 0.9656\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1570 - accuracy: 0.9555 - val_loss: 0.1225 - val_accuracy: 0.9655\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1561 - accuracy: 0.9561 - val_loss: 0.1230 - val_accuracy: 0.9654\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1563 - accuracy: 0.9560 - val_loss: 0.1226 - val_accuracy: 0.9655\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9560 - val_loss: 0.1225 - val_accuracy: 0.9654\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1564 - accuracy: 0.9556 - val_loss: 0.1225 - val_accuracy: 0.9654\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1569 - accuracy: 0.9554 - val_loss: 0.1223 - val_accuracy: 0.9654\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1566 - accuracy: 0.9561 - val_loss: 0.1222 - val_accuracy: 0.9656\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1568 - accuracy: 0.9560 - val_loss: 0.1223 - val_accuracy: 0.9656\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1552 - accuracy: 0.9563 - val_loss: 0.1222 - val_accuracy: 0.9655\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1555 - accuracy: 0.9559 - val_loss: 0.1224 - val_accuracy: 0.9658\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1561 - accuracy: 0.9560 - val_loss: 0.1222 - val_accuracy: 0.9656\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9560 - val_loss: 0.1220 - val_accuracy: 0.9656\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1553 - accuracy: 0.9562 - val_loss: 0.1215 - val_accuracy: 0.9661\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1554 - accuracy: 0.9559 - val_loss: 0.1223 - val_accuracy: 0.9652\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1553 - accuracy: 0.9560 - val_loss: 0.1217 - val_accuracy: 0.9656\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9561 - val_loss: 0.1217 - val_accuracy: 0.9656\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1558 - accuracy: 0.9565 - val_loss: 0.1212 - val_accuracy: 0.9658\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9562 - val_loss: 0.1215 - val_accuracy: 0.9660\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1541 - accuracy: 0.9567 - val_loss: 0.1216 - val_accuracy: 0.9655\n",
      "463/463 [==============================] - 1s 1ms/step\n",
      "4162/4162 [==============================] - 6s 1ms/step\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3330/3330 [==============================] - 9s 2ms/step - loss: 0.5277 - accuracy: 0.7819 - val_loss: 0.2306 - val_accuracy: 0.9472\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2372 - accuracy: 0.9106 - val_loss: 0.1515 - val_accuracy: 0.9568\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2136 - accuracy: 0.9175 - val_loss: 0.1434 - val_accuracy: 0.9595\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1974 - accuracy: 0.9189 - val_loss: 0.1410 - val_accuracy: 0.9603\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1918 - accuracy: 0.9205 - val_loss: 0.1394 - val_accuracy: 0.9603\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1898 - accuracy: 0.9210 - val_loss: 0.1379 - val_accuracy: 0.9611\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1863 - accuracy: 0.9204 - val_loss: 0.1367 - val_accuracy: 0.9614\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1853 - accuracy: 0.9329 - val_loss: 0.1359 - val_accuracy: 0.9615\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1833 - accuracy: 0.9471 - val_loss: 0.1350 - val_accuracy: 0.9622\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1798 - accuracy: 0.9479 - val_loss: 0.1345 - val_accuracy: 0.9619\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1797 - accuracy: 0.9481 - val_loss: 0.1338 - val_accuracy: 0.9619\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1782 - accuracy: 0.9485 - val_loss: 0.1333 - val_accuracy: 0.9619\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1766 - accuracy: 0.9499 - val_loss: 0.1328 - val_accuracy: 0.9624\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1761 - accuracy: 0.9499 - val_loss: 0.1321 - val_accuracy: 0.9624\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1747 - accuracy: 0.9512 - val_loss: 0.1317 - val_accuracy: 0.9623\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1731 - accuracy: 0.9524 - val_loss: 0.1313 - val_accuracy: 0.9626\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1717 - accuracy: 0.9536 - val_loss: 0.1309 - val_accuracy: 0.9628\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1719 - accuracy: 0.9529 - val_loss: 0.1304 - val_accuracy: 0.9626\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1690 - accuracy: 0.9531 - val_loss: 0.1299 - val_accuracy: 0.9626\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1691 - accuracy: 0.9532 - val_loss: 0.1295 - val_accuracy: 0.9625\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1696 - accuracy: 0.9531 - val_loss: 0.1291 - val_accuracy: 0.9628\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1679 - accuracy: 0.9540 - val_loss: 0.1287 - val_accuracy: 0.9631\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1666 - accuracy: 0.9535 - val_loss: 0.1285 - val_accuracy: 0.9632\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1657 - accuracy: 0.9535 - val_loss: 0.1281 - val_accuracy: 0.9632\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1650 - accuracy: 0.9535 - val_loss: 0.1280 - val_accuracy: 0.9634\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1638 - accuracy: 0.9540 - val_loss: 0.1273 - val_accuracy: 0.9634\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1652 - accuracy: 0.9534 - val_loss: 0.1273 - val_accuracy: 0.9632\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1641 - accuracy: 0.9537 - val_loss: 0.1271 - val_accuracy: 0.9633\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1631 - accuracy: 0.9541 - val_loss: 0.1268 - val_accuracy: 0.9633\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1641 - accuracy: 0.9534 - val_loss: 0.1270 - val_accuracy: 0.9635\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1620 - accuracy: 0.9547 - val_loss: 0.1267 - val_accuracy: 0.9635\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1624 - accuracy: 0.9541 - val_loss: 0.1265 - val_accuracy: 0.9635\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1618 - accuracy: 0.9540 - val_loss: 0.1260 - val_accuracy: 0.9638\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1619 - accuracy: 0.9537 - val_loss: 0.1260 - val_accuracy: 0.9635\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1626 - accuracy: 0.9545 - val_loss: 0.1257 - val_accuracy: 0.9637\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1599 - accuracy: 0.9548 - val_loss: 0.1251 - val_accuracy: 0.9640\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1607 - accuracy: 0.9542 - val_loss: 0.1249 - val_accuracy: 0.9638\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1604 - accuracy: 0.9544 - val_loss: 0.1250 - val_accuracy: 0.9641\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1600 - accuracy: 0.9546 - val_loss: 0.1247 - val_accuracy: 0.9640\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1601 - accuracy: 0.9543 - val_loss: 0.1246 - val_accuracy: 0.9641\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1591 - accuracy: 0.9548 - val_loss: 0.1243 - val_accuracy: 0.9641\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1586 - accuracy: 0.9549 - val_loss: 0.1244 - val_accuracy: 0.9641\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1584 - accuracy: 0.9544 - val_loss: 0.1241 - val_accuracy: 0.9643\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1593 - accuracy: 0.9539 - val_loss: 0.1240 - val_accuracy: 0.9643\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1591 - accuracy: 0.9541 - val_loss: 0.1241 - val_accuracy: 0.9641\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1589 - accuracy: 0.9540 - val_loss: 0.1237 - val_accuracy: 0.9643\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1585 - accuracy: 0.9541 - val_loss: 0.1237 - val_accuracy: 0.9644\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1574 - accuracy: 0.9547 - val_loss: 0.1235 - val_accuracy: 0.9643\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1588 - accuracy: 0.9543 - val_loss: 0.1236 - val_accuracy: 0.9643\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1582 - accuracy: 0.9544 - val_loss: 0.1235 - val_accuracy: 0.9643\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1578 - accuracy: 0.9541 - val_loss: 0.1238 - val_accuracy: 0.9644\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1585 - accuracy: 0.9544 - val_loss: 0.1241 - val_accuracy: 0.9643\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1574 - accuracy: 0.9548 - val_loss: 0.1232 - val_accuracy: 0.9644\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1587 - accuracy: 0.9548 - val_loss: 0.1231 - val_accuracy: 0.9641\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1570 - accuracy: 0.9548 - val_loss: 0.1232 - val_accuracy: 0.9643\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1569 - accuracy: 0.9547 - val_loss: 0.1230 - val_accuracy: 0.9643\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1580 - accuracy: 0.9545 - val_loss: 0.1227 - val_accuracy: 0.9645\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1573 - accuracy: 0.9545 - val_loss: 0.1227 - val_accuracy: 0.9644\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1568 - accuracy: 0.9552 - val_loss: 0.1229 - val_accuracy: 0.9643\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1568 - accuracy: 0.9550 - val_loss: 0.1228 - val_accuracy: 0.9645\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1559 - accuracy: 0.9556 - val_loss: 0.1228 - val_accuracy: 0.9644\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1559 - accuracy: 0.9545 - val_loss: 0.1228 - val_accuracy: 0.9642\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1557 - accuracy: 0.9550 - val_loss: 0.1224 - val_accuracy: 0.9643\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1559 - accuracy: 0.9552 - val_loss: 0.1226 - val_accuracy: 0.9643\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1566 - accuracy: 0.9549 - val_loss: 0.1225 - val_accuracy: 0.9644\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1562 - accuracy: 0.9550 - val_loss: 0.1224 - val_accuracy: 0.9642\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1557 - accuracy: 0.9546 - val_loss: 0.1224 - val_accuracy: 0.9644\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1561 - accuracy: 0.9544 - val_loss: 0.1222 - val_accuracy: 0.9642\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1560 - accuracy: 0.9552 - val_loss: 0.1221 - val_accuracy: 0.9643\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9552 - val_loss: 0.1221 - val_accuracy: 0.9646\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1550 - accuracy: 0.9549 - val_loss: 0.1218 - val_accuracy: 0.9644\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1559 - accuracy: 0.9552 - val_loss: 0.1218 - val_accuracy: 0.9644\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1553 - accuracy: 0.9554 - val_loss: 0.1222 - val_accuracy: 0.9644\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1549 - accuracy: 0.9550 - val_loss: 0.1216 - val_accuracy: 0.9644\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1550 - accuracy: 0.9553 - val_loss: 0.1218 - val_accuracy: 0.9644\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1563 - accuracy: 0.9549 - val_loss: 0.1218 - val_accuracy: 0.9645\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1548 - accuracy: 0.9554 - val_loss: 0.1214 - val_accuracy: 0.9644\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1549 - accuracy: 0.9549 - val_loss: 0.1214 - val_accuracy: 0.9644\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1551 - accuracy: 0.9547 - val_loss: 0.1213 - val_accuracy: 0.9647\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1554 - accuracy: 0.9548 - val_loss: 0.1214 - val_accuracy: 0.9648\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1543 - accuracy: 0.9553 - val_loss: 0.1215 - val_accuracy: 0.9647\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1546 - accuracy: 0.9548 - val_loss: 0.1212 - val_accuracy: 0.9647\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1540 - accuracy: 0.9558 - val_loss: 0.1209 - val_accuracy: 0.9646\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1545 - accuracy: 0.9552 - val_loss: 0.1210 - val_accuracy: 0.9646\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1558 - accuracy: 0.9547 - val_loss: 0.1210 - val_accuracy: 0.9646\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1529 - accuracy: 0.9555 - val_loss: 0.1210 - val_accuracy: 0.9647\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1547 - accuracy: 0.9552 - val_loss: 0.1211 - val_accuracy: 0.9649\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1544 - accuracy: 0.9551 - val_loss: 0.1207 - val_accuracy: 0.9647\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1542 - accuracy: 0.9553 - val_loss: 0.1204 - val_accuracy: 0.9647\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1537 - accuracy: 0.9554 - val_loss: 0.1212 - val_accuracy: 0.9647\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1540 - accuracy: 0.9554 - val_loss: 0.1206 - val_accuracy: 0.9647\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1541 - accuracy: 0.9556 - val_loss: 0.1209 - val_accuracy: 0.9648\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1542 - accuracy: 0.9551 - val_loss: 0.1203 - val_accuracy: 0.9646\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1530 - accuracy: 0.9556 - val_loss: 0.1202 - val_accuracy: 0.9648\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1534 - accuracy: 0.9553 - val_loss: 0.1203 - val_accuracy: 0.9645\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1545 - accuracy: 0.9552 - val_loss: 0.1202 - val_accuracy: 0.9647\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1547 - accuracy: 0.9549 - val_loss: 0.1203 - val_accuracy: 0.9650\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1534 - accuracy: 0.9558 - val_loss: 0.1203 - val_accuracy: 0.9651\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1550 - accuracy: 0.9553 - val_loss: 0.1198 - val_accuracy: 0.9651\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1536 - accuracy: 0.9553 - val_loss: 0.1198 - val_accuracy: 0.9647\n",
      "463/463 [==============================] - 1s 1ms/step\n",
      "4162/4162 [==============================] - 6s 2ms/step\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3330/3330 [==============================] - 9s 2ms/step - loss: 0.4111 - accuracy: 0.8092 - val_loss: 0.1907 - val_accuracy: 0.9467\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2552 - accuracy: 0.8953 - val_loss: 0.1540 - val_accuracy: 0.9539\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2229 - accuracy: 0.9261 - val_loss: 0.1463 - val_accuracy: 0.9580\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.2024 - accuracy: 0.9436 - val_loss: 0.1421 - val_accuracy: 0.9598\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1957 - accuracy: 0.9456 - val_loss: 0.1395 - val_accuracy: 0.9603\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1879 - accuracy: 0.9469 - val_loss: 0.1377 - val_accuracy: 0.9609\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1881 - accuracy: 0.9464 - val_loss: 0.1368 - val_accuracy: 0.9607\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1857 - accuracy: 0.9472 - val_loss: 0.1359 - val_accuracy: 0.9614\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1820 - accuracy: 0.9486 - val_loss: 0.1352 - val_accuracy: 0.9616\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 11s 3ms/step - loss: 0.1806 - accuracy: 0.9482 - val_loss: 0.1351 - val_accuracy: 0.9611\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1788 - accuracy: 0.9485 - val_loss: 0.1343 - val_accuracy: 0.9617\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1772 - accuracy: 0.9486 - val_loss: 0.1338 - val_accuracy: 0.9616\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1765 - accuracy: 0.9487 - val_loss: 0.1333 - val_accuracy: 0.9611\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1748 - accuracy: 0.9484 - val_loss: 0.1328 - val_accuracy: 0.9617\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1727 - accuracy: 0.9490 - val_loss: 0.1325 - val_accuracy: 0.9614\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1712 - accuracy: 0.9489 - val_loss: 0.1318 - val_accuracy: 0.9617\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1684 - accuracy: 0.9497 - val_loss: 0.1312 - val_accuracy: 0.9621\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1682 - accuracy: 0.9511 - val_loss: 0.1309 - val_accuracy: 0.9624\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1655 - accuracy: 0.9512 - val_loss: 0.1306 - val_accuracy: 0.9626\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1656 - accuracy: 0.9516 - val_loss: 0.1303 - val_accuracy: 0.9628\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1636 - accuracy: 0.9520 - val_loss: 0.1301 - val_accuracy: 0.9629\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1646 - accuracy: 0.9524 - val_loss: 0.1295 - val_accuracy: 0.9631\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1628 - accuracy: 0.9527 - val_loss: 0.1289 - val_accuracy: 0.9632\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1628 - accuracy: 0.9544 - val_loss: 0.1285 - val_accuracy: 0.9631\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1626 - accuracy: 0.9543 - val_loss: 0.1282 - val_accuracy: 0.9633\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1606 - accuracy: 0.9554 - val_loss: 0.1281 - val_accuracy: 0.9636\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1591 - accuracy: 0.9554 - val_loss: 0.1280 - val_accuracy: 0.9639\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1590 - accuracy: 0.9552 - val_loss: 0.1274 - val_accuracy: 0.9633\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1579 - accuracy: 0.9551 - val_loss: 0.1273 - val_accuracy: 0.9637\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1588 - accuracy: 0.9558 - val_loss: 0.1270 - val_accuracy: 0.9635\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1569 - accuracy: 0.9557 - val_loss: 0.1271 - val_accuracy: 0.9635\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1576 - accuracy: 0.9554 - val_loss: 0.1265 - val_accuracy: 0.9638\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1567 - accuracy: 0.9558 - val_loss: 0.1263 - val_accuracy: 0.9638\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1560 - accuracy: 0.9562 - val_loss: 0.1262 - val_accuracy: 0.9637\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1555 - accuracy: 0.9560 - val_loss: 0.1258 - val_accuracy: 0.9641\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1559 - accuracy: 0.9554 - val_loss: 0.1258 - val_accuracy: 0.9640\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1550 - accuracy: 0.9560 - val_loss: 0.1256 - val_accuracy: 0.9641\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1559 - accuracy: 0.9558 - val_loss: 0.1256 - val_accuracy: 0.9640\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1544 - accuracy: 0.9562 - val_loss: 0.1252 - val_accuracy: 0.9643\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1532 - accuracy: 0.9567 - val_loss: 0.1254 - val_accuracy: 0.9643\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1531 - accuracy: 0.9563 - val_loss: 0.1250 - val_accuracy: 0.9643\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1544 - accuracy: 0.9562 - val_loss: 0.1251 - val_accuracy: 0.9644\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1544 - accuracy: 0.9555 - val_loss: 0.1249 - val_accuracy: 0.9641\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1528 - accuracy: 0.9567 - val_loss: 0.1247 - val_accuracy: 0.9643\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1532 - accuracy: 0.9566 - val_loss: 0.1246 - val_accuracy: 0.9643\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1528 - accuracy: 0.9565 - val_loss: 0.1246 - val_accuracy: 0.9643\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1536 - accuracy: 0.9561 - val_loss: 0.1244 - val_accuracy: 0.9646\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1523 - accuracy: 0.9564 - val_loss: 0.1241 - val_accuracy: 0.9643\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1531 - accuracy: 0.9562 - val_loss: 0.1243 - val_accuracy: 0.9646\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1513 - accuracy: 0.9572 - val_loss: 0.1241 - val_accuracy: 0.9644\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1515 - accuracy: 0.9575 - val_loss: 0.1242 - val_accuracy: 0.9644\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1520 - accuracy: 0.9573 - val_loss: 0.1241 - val_accuracy: 0.9647\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1520 - accuracy: 0.9566 - val_loss: 0.1236 - val_accuracy: 0.9647\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1516 - accuracy: 0.9568 - val_loss: 0.1240 - val_accuracy: 0.9643\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1514 - accuracy: 0.9569 - val_loss: 0.1238 - val_accuracy: 0.9642\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1516 - accuracy: 0.9569 - val_loss: 0.1237 - val_accuracy: 0.9642\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1519 - accuracy: 0.9566 - val_loss: 0.1235 - val_accuracy: 0.9646\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1506 - accuracy: 0.9574 - val_loss: 0.1235 - val_accuracy: 0.9643\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1499 - accuracy: 0.9576 - val_loss: 0.1234 - val_accuracy: 0.9643\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1515 - accuracy: 0.9570 - val_loss: 0.1230 - val_accuracy: 0.9644\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1508 - accuracy: 0.9570 - val_loss: 0.1228 - val_accuracy: 0.9643\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1505 - accuracy: 0.9574 - val_loss: 0.1226 - val_accuracy: 0.9643\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1495 - accuracy: 0.9582 - val_loss: 0.1233 - val_accuracy: 0.9641\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1496 - accuracy: 0.9577 - val_loss: 0.1234 - val_accuracy: 0.9644\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1501 - accuracy: 0.9579 - val_loss: 0.1230 - val_accuracy: 0.9644\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1492 - accuracy: 0.9575 - val_loss: 0.1227 - val_accuracy: 0.9644\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1500 - accuracy: 0.9575 - val_loss: 0.1229 - val_accuracy: 0.9644\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1503 - accuracy: 0.9569 - val_loss: 0.1229 - val_accuracy: 0.9644\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1498 - accuracy: 0.9572 - val_loss: 0.1227 - val_accuracy: 0.9643\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1490 - accuracy: 0.9575 - val_loss: 0.1228 - val_accuracy: 0.9641\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1491 - accuracy: 0.9574 - val_loss: 0.1224 - val_accuracy: 0.9645\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1485 - accuracy: 0.9576 - val_loss: 0.1221 - val_accuracy: 0.9647\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1484 - accuracy: 0.9577 - val_loss: 0.1222 - val_accuracy: 0.9645\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1489 - accuracy: 0.9579 - val_loss: 0.1221 - val_accuracy: 0.9646\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1492 - accuracy: 0.9576 - val_loss: 0.1224 - val_accuracy: 0.9645\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1496 - accuracy: 0.9575 - val_loss: 0.1221 - val_accuracy: 0.9647\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1495 - accuracy: 0.9576 - val_loss: 0.1222 - val_accuracy: 0.9642\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1484 - accuracy: 0.9578 - val_loss: 0.1218 - val_accuracy: 0.9644\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1484 - accuracy: 0.9582 - val_loss: 0.1220 - val_accuracy: 0.9645\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1481 - accuracy: 0.9582 - val_loss: 0.1220 - val_accuracy: 0.9649\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1477 - accuracy: 0.9583 - val_loss: 0.1219 - val_accuracy: 0.9647\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1478 - accuracy: 0.9578 - val_loss: 0.1216 - val_accuracy: 0.9646\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1490 - accuracy: 0.9576 - val_loss: 0.1213 - val_accuracy: 0.9650\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1480 - accuracy: 0.9581 - val_loss: 0.1216 - val_accuracy: 0.9653\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1490 - accuracy: 0.9575 - val_loss: 0.1220 - val_accuracy: 0.9649\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1477 - accuracy: 0.9579 - val_loss: 0.1220 - val_accuracy: 0.9649\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1480 - accuracy: 0.9578 - val_loss: 0.1216 - val_accuracy: 0.9652\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1481 - accuracy: 0.9580 - val_loss: 0.1218 - val_accuracy: 0.9650\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1479 - accuracy: 0.9580 - val_loss: 0.1217 - val_accuracy: 0.9651\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1476 - accuracy: 0.9580 - val_loss: 0.1217 - val_accuracy: 0.9651\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1488 - accuracy: 0.9580 - val_loss: 0.1213 - val_accuracy: 0.9652\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1469 - accuracy: 0.9577 - val_loss: 0.1215 - val_accuracy: 0.9651\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1478 - accuracy: 0.9578 - val_loss: 0.1218 - val_accuracy: 0.9652\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1479 - accuracy: 0.9583 - val_loss: 0.1214 - val_accuracy: 0.9648\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1477 - accuracy: 0.9583 - val_loss: 0.1214 - val_accuracy: 0.9650\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1472 - accuracy: 0.9586 - val_loss: 0.1212 - val_accuracy: 0.9651\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1474 - accuracy: 0.9589 - val_loss: 0.1212 - val_accuracy: 0.9647\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1478 - accuracy: 0.9581 - val_loss: 0.1213 - val_accuracy: 0.9651\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1476 - accuracy: 0.9585 - val_loss: 0.1209 - val_accuracy: 0.9654\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1469 - accuracy: 0.9583 - val_loss: 0.1212 - val_accuracy: 0.9652\n",
      "463/463 [==============================] - 1s 2ms/step\n",
      "4162/4162 [==============================] - 6s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/100\n",
      "3330/3330 [==============================] - 9s 2ms/step - loss: 0.4777 - accuracy: 0.7436 - val_loss: 0.2073 - val_accuracy: 0.9447\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2386 - accuracy: 0.9352 - val_loss: 0.1482 - val_accuracy: 0.9586\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2085 - accuracy: 0.9448 - val_loss: 0.1424 - val_accuracy: 0.9596\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1970 - accuracy: 0.9471 - val_loss: 0.1399 - val_accuracy: 0.9605\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1892 - accuracy: 0.9485 - val_loss: 0.1383 - val_accuracy: 0.9605\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1831 - accuracy: 0.9513 - val_loss: 0.1369 - val_accuracy: 0.9611\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1783 - accuracy: 0.9522 - val_loss: 0.1362 - val_accuracy: 0.9610\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1739 - accuracy: 0.9525 - val_loss: 0.1353 - val_accuracy: 0.9612\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1720 - accuracy: 0.9526 - val_loss: 0.1349 - val_accuracy: 0.9608\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1695 - accuracy: 0.9527 - val_loss: 0.1341 - val_accuracy: 0.9612\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1680 - accuracy: 0.9530 - val_loss: 0.1335 - val_accuracy: 0.9616\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1668 - accuracy: 0.9529 - val_loss: 0.1334 - val_accuracy: 0.9617\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1655 - accuracy: 0.9532 - val_loss: 0.1333 - val_accuracy: 0.9614\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1642 - accuracy: 0.9533 - val_loss: 0.1331 - val_accuracy: 0.9614\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1639 - accuracy: 0.9531 - val_loss: 0.1324 - val_accuracy: 0.9621\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1630 - accuracy: 0.9537 - val_loss: 0.1319 - val_accuracy: 0.9623\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1631 - accuracy: 0.9537 - val_loss: 0.1321 - val_accuracy: 0.9617\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1602 - accuracy: 0.9534 - val_loss: 0.1321 - val_accuracy: 0.9623\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1616 - accuracy: 0.9534 - val_loss: 0.1315 - val_accuracy: 0.9622\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1610 - accuracy: 0.9533 - val_loss: 0.1311 - val_accuracy: 0.9623\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1596 - accuracy: 0.9545 - val_loss: 0.1307 - val_accuracy: 0.9624\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1576 - accuracy: 0.9543 - val_loss: 0.1312 - val_accuracy: 0.9619\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1596 - accuracy: 0.9542 - val_loss: 0.1305 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1592 - accuracy: 0.9542 - val_loss: 0.1304 - val_accuracy: 0.9627\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1577 - accuracy: 0.9544 - val_loss: 0.1303 - val_accuracy: 0.9629\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1585 - accuracy: 0.9536 - val_loss: 0.1299 - val_accuracy: 0.9626\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1566 - accuracy: 0.9545 - val_loss: 0.1296 - val_accuracy: 0.9629\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1574 - accuracy: 0.9543 - val_loss: 0.1296 - val_accuracy: 0.9628\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1576 - accuracy: 0.9540 - val_loss: 0.1292 - val_accuracy: 0.9627\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1575 - accuracy: 0.9541 - val_loss: 0.1293 - val_accuracy: 0.9628\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1569 - accuracy: 0.9541 - val_loss: 0.1294 - val_accuracy: 0.9628\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1568 - accuracy: 0.9548 - val_loss: 0.1286 - val_accuracy: 0.9628\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1572 - accuracy: 0.9543 - val_loss: 0.1289 - val_accuracy: 0.9626\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1566 - accuracy: 0.9546 - val_loss: 0.1286 - val_accuracy: 0.9631\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1559 - accuracy: 0.9547 - val_loss: 0.1287 - val_accuracy: 0.9629\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1566 - accuracy: 0.9545 - val_loss: 0.1285 - val_accuracy: 0.9629\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1551 - accuracy: 0.9549 - val_loss: 0.1280 - val_accuracy: 0.9628\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1552 - accuracy: 0.9543 - val_loss: 0.1283 - val_accuracy: 0.9629\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1546 - accuracy: 0.9549 - val_loss: 0.1281 - val_accuracy: 0.9631\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1549 - accuracy: 0.9545 - val_loss: 0.1275 - val_accuracy: 0.9632\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1554 - accuracy: 0.9548 - val_loss: 0.1280 - val_accuracy: 0.9629\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1543 - accuracy: 0.9548 - val_loss: 0.1281 - val_accuracy: 0.9632\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1549 - accuracy: 0.9555 - val_loss: 0.1277 - val_accuracy: 0.9632\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1549 - accuracy: 0.9550 - val_loss: 0.1275 - val_accuracy: 0.9633\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1546 - accuracy: 0.9548 - val_loss: 0.1272 - val_accuracy: 0.9630\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1545 - accuracy: 0.9550 - val_loss: 0.1275 - val_accuracy: 0.9631\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1539 - accuracy: 0.9550 - val_loss: 0.1270 - val_accuracy: 0.9633\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1533 - accuracy: 0.9555 - val_loss: 0.1268 - val_accuracy: 0.9634\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1524 - accuracy: 0.9551 - val_loss: 0.1274 - val_accuracy: 0.9630\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1528 - accuracy: 0.9551 - val_loss: 0.1270 - val_accuracy: 0.9632\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1533 - accuracy: 0.9552 - val_loss: 0.1271 - val_accuracy: 0.9632\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1528 - accuracy: 0.9553 - val_loss: 0.1270 - val_accuracy: 0.9634\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1531 - accuracy: 0.9554 - val_loss: 0.1264 - val_accuracy: 0.9635\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1522 - accuracy: 0.9548 - val_loss: 0.1263 - val_accuracy: 0.9635\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1534 - accuracy: 0.9554 - val_loss: 0.1260 - val_accuracy: 0.9635\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1532 - accuracy: 0.9553 - val_loss: 0.1262 - val_accuracy: 0.9635\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1514 - accuracy: 0.9563 - val_loss: 0.1264 - val_accuracy: 0.9636\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1526 - accuracy: 0.9552 - val_loss: 0.1265 - val_accuracy: 0.9634\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1525 - accuracy: 0.9551 - val_loss: 0.1262 - val_accuracy: 0.9635\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1528 - accuracy: 0.9549 - val_loss: 0.1262 - val_accuracy: 0.9635\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1531 - accuracy: 0.9548 - val_loss: 0.1260 - val_accuracy: 0.9636\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1528 - accuracy: 0.9550 - val_loss: 0.1258 - val_accuracy: 0.9636\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1531 - accuracy: 0.9552 - val_loss: 0.1258 - val_accuracy: 0.9638\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1515 - accuracy: 0.9552 - val_loss: 0.1256 - val_accuracy: 0.9635\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1528 - accuracy: 0.9548 - val_loss: 0.1259 - val_accuracy: 0.9635\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1518 - accuracy: 0.9556 - val_loss: 0.1254 - val_accuracy: 0.9638\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1514 - accuracy: 0.9555 - val_loss: 0.1256 - val_accuracy: 0.9637\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 13s 4ms/step - loss: 0.1517 - accuracy: 0.9557 - val_loss: 0.1254 - val_accuracy: 0.9634\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 11s 3ms/step - loss: 0.1512 - accuracy: 0.9551 - val_loss: 0.1257 - val_accuracy: 0.9636\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1522 - accuracy: 0.9557 - val_loss: 0.1255 - val_accuracy: 0.9638\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1517 - accuracy: 0.9551 - val_loss: 0.1254 - val_accuracy: 0.9638\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1512 - accuracy: 0.9554 - val_loss: 0.1251 - val_accuracy: 0.9639\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1514 - accuracy: 0.9554 - val_loss: 0.1253 - val_accuracy: 0.9638\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1527 - accuracy: 0.9552 - val_loss: 0.1248 - val_accuracy: 0.9635\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1504 - accuracy: 0.9558 - val_loss: 0.1249 - val_accuracy: 0.9634\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1510 - accuracy: 0.9558 - val_loss: 0.1247 - val_accuracy: 0.9634\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1509 - accuracy: 0.9557 - val_loss: 0.1246 - val_accuracy: 0.9633\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1510 - accuracy: 0.9557 - val_loss: 0.1244 - val_accuracy: 0.9634\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1499 - accuracy: 0.9555 - val_loss: 0.1244 - val_accuracy: 0.9635\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1512 - accuracy: 0.9553 - val_loss: 0.1246 - val_accuracy: 0.9634\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1516 - accuracy: 0.9552 - val_loss: 0.1243 - val_accuracy: 0.9634\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1512 - accuracy: 0.9553 - val_loss: 0.1246 - val_accuracy: 0.9640\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1493 - accuracy: 0.9561 - val_loss: 0.1244 - val_accuracy: 0.9636\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1498 - accuracy: 0.9553 - val_loss: 0.1244 - val_accuracy: 0.9635\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1503 - accuracy: 0.9561 - val_loss: 0.1245 - val_accuracy: 0.9639\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1521 - accuracy: 0.9556 - val_loss: 0.1241 - val_accuracy: 0.9635\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1499 - accuracy: 0.9557 - val_loss: 0.1241 - val_accuracy: 0.9638\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1510 - accuracy: 0.9556 - val_loss: 0.1239 - val_accuracy: 0.9635\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1503 - accuracy: 0.9553 - val_loss: 0.1240 - val_accuracy: 0.9639\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1494 - accuracy: 0.9559 - val_loss: 0.1241 - val_accuracy: 0.9633\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1503 - accuracy: 0.9558 - val_loss: 0.1235 - val_accuracy: 0.9636\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1507 - accuracy: 0.9554 - val_loss: 0.1236 - val_accuracy: 0.9639\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1496 - accuracy: 0.9560 - val_loss: 0.1240 - val_accuracy: 0.9637\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1503 - accuracy: 0.9557 - val_loss: 0.1232 - val_accuracy: 0.9640\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1490 - accuracy: 0.9562 - val_loss: 0.1234 - val_accuracy: 0.9638\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1501 - accuracy: 0.9556 - val_loss: 0.1234 - val_accuracy: 0.9638\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1500 - accuracy: 0.9559 - val_loss: 0.1235 - val_accuracy: 0.9640\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1506 - accuracy: 0.9552 - val_loss: 0.1234 - val_accuracy: 0.9640\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1502 - accuracy: 0.9559 - val_loss: 0.1233 - val_accuracy: 0.9636\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1484 - accuracy: 0.9561 - val_loss: 0.1234 - val_accuracy: 0.9641\n",
      "463/463 [==============================] - 1s 2ms/step\n",
      "4162/4162 [==============================] - 7s 2ms/step\n",
      "Model: \"model_14\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.6072 - accuracy: 0.6503 - val_loss: 0.4280 - val_accuracy: 0.8963\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.4089 - accuracy: 0.8694 - val_loss: 0.3245 - val_accuracy: 0.9486\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.3438 - accuracy: 0.9217 - val_loss: 0.2819 - val_accuracy: 0.9531\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.3064 - accuracy: 0.9297 - val_loss: 0.2510 - val_accuracy: 0.9541\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2799 - accuracy: 0.9319 - val_loss: 0.2275 - val_accuracy: 0.9560\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2631 - accuracy: 0.9320 - val_loss: 0.2102 - val_accuracy: 0.9570\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2510 - accuracy: 0.9335 - val_loss: 0.1976 - val_accuracy: 0.9578\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2439 - accuracy: 0.9341 - val_loss: 0.1881 - val_accuracy: 0.9581\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.2360 - accuracy: 0.9352 - val_loss: 0.1807 - val_accuracy: 0.9583\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2311 - accuracy: 0.9350 - val_loss: 0.1755 - val_accuracy: 0.9588\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2266 - accuracy: 0.9367 - val_loss: 0.1705 - val_accuracy: 0.9596\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2235 - accuracy: 0.9365 - val_loss: 0.1673 - val_accuracy: 0.9593\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2203 - accuracy: 0.9376 - val_loss: 0.1644 - val_accuracy: 0.9605\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2187 - accuracy: 0.9376 - val_loss: 0.1620 - val_accuracy: 0.9608\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2177 - accuracy: 0.9373 - val_loss: 0.1603 - val_accuracy: 0.9611\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2148 - accuracy: 0.9382 - val_loss: 0.1587 - val_accuracy: 0.9611\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2144 - accuracy: 0.9384 - val_loss: 0.1567 - val_accuracy: 0.9617\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2128 - accuracy: 0.9390 - val_loss: 0.1557 - val_accuracy: 0.9616\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2129 - accuracy: 0.9385 - val_loss: 0.1543 - val_accuracy: 0.9616\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2092 - accuracy: 0.9392 - val_loss: 0.1535 - val_accuracy: 0.9615\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2091 - accuracy: 0.9387 - val_loss: 0.1528 - val_accuracy: 0.9622\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2071 - accuracy: 0.9401 - val_loss: 0.1519 - val_accuracy: 0.9628\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2062 - accuracy: 0.9400 - val_loss: 0.1511 - val_accuracy: 0.9621\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.2069 - accuracy: 0.9396 - val_loss: 0.1490 - val_accuracy: 0.9628\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2046 - accuracy: 0.9405 - val_loss: 0.1485 - val_accuracy: 0.9627\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2018 - accuracy: 0.9413 - val_loss: 0.1475 - val_accuracy: 0.9632\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2019 - accuracy: 0.9405 - val_loss: 0.1478 - val_accuracy: 0.9631\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2022 - accuracy: 0.9417 - val_loss: 0.1466 - val_accuracy: 0.9630\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2001 - accuracy: 0.9416 - val_loss: 0.1463 - val_accuracy: 0.9631\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2006 - accuracy: 0.9413 - val_loss: 0.1458 - val_accuracy: 0.9629\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1973 - accuracy: 0.9424 - val_loss: 0.1442 - val_accuracy: 0.9629\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1940 - accuracy: 0.9423 - val_loss: 0.1444 - val_accuracy: 0.9632\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1944 - accuracy: 0.9423 - val_loss: 0.1436 - val_accuracy: 0.9629\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1919 - accuracy: 0.9427 - val_loss: 0.1429 - val_accuracy: 0.9631\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1914 - accuracy: 0.9424 - val_loss: 0.1423 - val_accuracy: 0.9633\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1899 - accuracy: 0.9431 - val_loss: 0.1419 - val_accuracy: 0.9629\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1883 - accuracy: 0.9437 - val_loss: 0.1417 - val_accuracy: 0.9634\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1885 - accuracy: 0.9435 - val_loss: 0.1413 - val_accuracy: 0.9631\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1884 - accuracy: 0.9444 - val_loss: 0.1407 - val_accuracy: 0.9632\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1883 - accuracy: 0.9442 - val_loss: 0.1410 - val_accuracy: 0.9632\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1875 - accuracy: 0.9437 - val_loss: 0.1407 - val_accuracy: 0.9629\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1860 - accuracy: 0.9442 - val_loss: 0.1404 - val_accuracy: 0.9631\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1895 - accuracy: 0.9411 - val_loss: 0.1400 - val_accuracy: 0.9633\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1867 - accuracy: 0.9424 - val_loss: 0.1394 - val_accuracy: 0.9635\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1868 - accuracy: 0.9419 - val_loss: 0.1394 - val_accuracy: 0.9631\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1841 - accuracy: 0.9417 - val_loss: 0.1402 - val_accuracy: 0.9634\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1843 - accuracy: 0.9432 - val_loss: 0.1385 - val_accuracy: 0.9631\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1835 - accuracy: 0.9422 - val_loss: 0.1386 - val_accuracy: 0.9635\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1869 - accuracy: 0.9420 - val_loss: 0.1383 - val_accuracy: 0.9634\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1837 - accuracy: 0.9430 - val_loss: 0.1378 - val_accuracy: 0.9637\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1838 - accuracy: 0.9426 - val_loss: 0.1373 - val_accuracy: 0.9634\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1847 - accuracy: 0.9423 - val_loss: 0.1381 - val_accuracy: 0.9633\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1858 - accuracy: 0.9412 - val_loss: 0.1382 - val_accuracy: 0.9635\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1837 - accuracy: 0.9423 - val_loss: 0.1374 - val_accuracy: 0.9633\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1842 - accuracy: 0.9417 - val_loss: 0.1388 - val_accuracy: 0.9635\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1829 - accuracy: 0.9428 - val_loss: 0.1381 - val_accuracy: 0.9634\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1833 - accuracy: 0.9432 - val_loss: 0.1371 - val_accuracy: 0.9634\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1814 - accuracy: 0.9431 - val_loss: 0.1370 - val_accuracy: 0.9632\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1827 - accuracy: 0.9432 - val_loss: 0.1374 - val_accuracy: 0.9631\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1819 - accuracy: 0.9435 - val_loss: 0.1368 - val_accuracy: 0.9633\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1827 - accuracy: 0.9432 - val_loss: 0.1369 - val_accuracy: 0.9637\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1833 - accuracy: 0.9428 - val_loss: 0.1368 - val_accuracy: 0.9634\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1818 - accuracy: 0.9435 - val_loss: 0.1370 - val_accuracy: 0.9635\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1819 - accuracy: 0.9436 - val_loss: 0.1362 - val_accuracy: 0.9635\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1841 - accuracy: 0.9425 - val_loss: 0.1363 - val_accuracy: 0.9636\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1812 - accuracy: 0.9440 - val_loss: 0.1365 - val_accuracy: 0.9634\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1818 - accuracy: 0.9435 - val_loss: 0.1360 - val_accuracy: 0.9634\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1819 - accuracy: 0.9433 - val_loss: 0.1360 - val_accuracy: 0.9632\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1813 - accuracy: 0.9438 - val_loss: 0.1363 - val_accuracy: 0.9632\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1810 - accuracy: 0.9440 - val_loss: 0.1358 - val_accuracy: 0.9635\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1792 - accuracy: 0.9435 - val_loss: 0.1365 - val_accuracy: 0.9633\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1801 - accuracy: 0.9442 - val_loss: 0.1360 - val_accuracy: 0.9633\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1805 - accuracy: 0.9452 - val_loss: 0.1351 - val_accuracy: 0.9639\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1807 - accuracy: 0.9452 - val_loss: 0.1353 - val_accuracy: 0.9635\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1801 - accuracy: 0.9453 - val_loss: 0.1359 - val_accuracy: 0.9638\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1782 - accuracy: 0.9464 - val_loss: 0.1355 - val_accuracy: 0.9637\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1787 - accuracy: 0.9467 - val_loss: 0.1356 - val_accuracy: 0.9636\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1797 - accuracy: 0.9459 - val_loss: 0.1357 - val_accuracy: 0.9634\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1773 - accuracy: 0.9477 - val_loss: 0.1350 - val_accuracy: 0.9637\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1796 - accuracy: 0.9459 - val_loss: 0.1349 - val_accuracy: 0.9638\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1785 - accuracy: 0.9471 - val_loss: 0.1355 - val_accuracy: 0.9638\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1789 - accuracy: 0.9465 - val_loss: 0.1352 - val_accuracy: 0.9638\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1772 - accuracy: 0.9470 - val_loss: 0.1350 - val_accuracy: 0.9640\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1777 - accuracy: 0.9467 - val_loss: 0.1354 - val_accuracy: 0.9637\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1784 - accuracy: 0.9465 - val_loss: 0.1351 - val_accuracy: 0.9636\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1767 - accuracy: 0.9470 - val_loss: 0.1348 - val_accuracy: 0.9638\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1793 - accuracy: 0.9464 - val_loss: 0.1342 - val_accuracy: 0.9638\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1771 - accuracy: 0.9470 - val_loss: 0.1350 - val_accuracy: 0.9636\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1774 - accuracy: 0.9466 - val_loss: 0.1345 - val_accuracy: 0.9637\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1784 - accuracy: 0.9462 - val_loss: 0.1347 - val_accuracy: 0.9638\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1774 - accuracy: 0.9470 - val_loss: 0.1342 - val_accuracy: 0.9638\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1765 - accuracy: 0.9469 - val_loss: 0.1347 - val_accuracy: 0.9634\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1800 - accuracy: 0.9456 - val_loss: 0.1349 - val_accuracy: 0.9641\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1759 - accuracy: 0.9478 - val_loss: 0.1347 - val_accuracy: 0.9637\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1773 - accuracy: 0.9471 - val_loss: 0.1341 - val_accuracy: 0.9634\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1789 - accuracy: 0.9460 - val_loss: 0.1345 - val_accuracy: 0.9633\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1760 - accuracy: 0.9468 - val_loss: 0.1347 - val_accuracy: 0.9638\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1779 - accuracy: 0.9472 - val_loss: 0.1343 - val_accuracy: 0.9637\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1771 - accuracy: 0.9466 - val_loss: 0.1350 - val_accuracy: 0.9635\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1756 - accuracy: 0.9477 - val_loss: 0.1341 - val_accuracy: 0.9633\n",
      "463/463 [==============================] - 1s 2ms/step\n",
      "4162/4162 [==============================] - 6s 2ms/step\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3330/3330 [==============================] - 9s 2ms/step - loss: 0.5412 - accuracy: 0.7863 - val_loss: 0.3960 - val_accuracy: 0.9446\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.3144 - accuracy: 0.9270 - val_loss: 0.1715 - val_accuracy: 0.9549\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2090 - accuracy: 0.9396 - val_loss: 0.1490 - val_accuracy: 0.9582\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1935 - accuracy: 0.9449 - val_loss: 0.1448 - val_accuracy: 0.9594\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1847 - accuracy: 0.9477 - val_loss: 0.1416 - val_accuracy: 0.9603\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1790 - accuracy: 0.9491 - val_loss: 0.1391 - val_accuracy: 0.9612\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1755 - accuracy: 0.9505 - val_loss: 0.1383 - val_accuracy: 0.9614\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1734 - accuracy: 0.9508 - val_loss: 0.1371 - val_accuracy: 0.9614\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1712 - accuracy: 0.9509 - val_loss: 0.1360 - val_accuracy: 0.9616\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1693 - accuracy: 0.9521 - val_loss: 0.1355 - val_accuracy: 0.9617\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1680 - accuracy: 0.9511 - val_loss: 0.1349 - val_accuracy: 0.9617\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1658 - accuracy: 0.9515 - val_loss: 0.1345 - val_accuracy: 0.9620\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1665 - accuracy: 0.9534 - val_loss: 0.1339 - val_accuracy: 0.9622\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1653 - accuracy: 0.9539 - val_loss: 0.1337 - val_accuracy: 0.9614\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1635 - accuracy: 0.9550 - val_loss: 0.1330 - val_accuracy: 0.9619\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1629 - accuracy: 0.9549 - val_loss: 0.1326 - val_accuracy: 0.9623\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1623 - accuracy: 0.9551 - val_loss: 0.1319 - val_accuracy: 0.9623\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1615 - accuracy: 0.9550 - val_loss: 0.1312 - val_accuracy: 0.9624\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1609 - accuracy: 0.9553 - val_loss: 0.1310 - val_accuracy: 0.9622\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1603 - accuracy: 0.9557 - val_loss: 0.1306 - val_accuracy: 0.9622\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1580 - accuracy: 0.9557 - val_loss: 0.1300 - val_accuracy: 0.9628\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1589 - accuracy: 0.9560 - val_loss: 0.1300 - val_accuracy: 0.9632\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1584 - accuracy: 0.9558 - val_loss: 0.1294 - val_accuracy: 0.9631\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1588 - accuracy: 0.9560 - val_loss: 0.1290 - val_accuracy: 0.9629\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1570 - accuracy: 0.9557 - val_loss: 0.1282 - val_accuracy: 0.9634\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1562 - accuracy: 0.9559 - val_loss: 0.1282 - val_accuracy: 0.9632\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9558 - val_loss: 0.1275 - val_accuracy: 0.9636\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1547 - accuracy: 0.9566 - val_loss: 0.1275 - val_accuracy: 0.9635\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1553 - accuracy: 0.9564 - val_loss: 0.1270 - val_accuracy: 0.9635\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1543 - accuracy: 0.9569 - val_loss: 0.1267 - val_accuracy: 0.9639\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1546 - accuracy: 0.9561 - val_loss: 0.1265 - val_accuracy: 0.9638\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1536 - accuracy: 0.9562 - val_loss: 0.1263 - val_accuracy: 0.9634\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1529 - accuracy: 0.9566 - val_loss: 0.1260 - val_accuracy: 0.9641\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1525 - accuracy: 0.9566 - val_loss: 0.1256 - val_accuracy: 0.9641\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1546 - accuracy: 0.9562 - val_loss: 0.1252 - val_accuracy: 0.9642\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1526 - accuracy: 0.9567 - val_loss: 0.1246 - val_accuracy: 0.9646\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1528 - accuracy: 0.9571 - val_loss: 0.1248 - val_accuracy: 0.9643\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1512 - accuracy: 0.9578 - val_loss: 0.1247 - val_accuracy: 0.9642\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1510 - accuracy: 0.9580 - val_loss: 0.1242 - val_accuracy: 0.9643\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1518 - accuracy: 0.9576 - val_loss: 0.1238 - val_accuracy: 0.9644\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1510 - accuracy: 0.9578 - val_loss: 0.1236 - val_accuracy: 0.9645\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1508 - accuracy: 0.9576 - val_loss: 0.1237 - val_accuracy: 0.9646\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1493 - accuracy: 0.9578 - val_loss: 0.1238 - val_accuracy: 0.9646\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1503 - accuracy: 0.9577 - val_loss: 0.1232 - val_accuracy: 0.9648\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1494 - accuracy: 0.9579 - val_loss: 0.1230 - val_accuracy: 0.9649\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1502 - accuracy: 0.9574 - val_loss: 0.1227 - val_accuracy: 0.9646\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1492 - accuracy: 0.9575 - val_loss: 0.1225 - val_accuracy: 0.9649\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1486 - accuracy: 0.9576 - val_loss: 0.1221 - val_accuracy: 0.9649\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1490 - accuracy: 0.9574 - val_loss: 0.1217 - val_accuracy: 0.9650\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1481 - accuracy: 0.9580 - val_loss: 0.1221 - val_accuracy: 0.9652\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1484 - accuracy: 0.9581 - val_loss: 0.1215 - val_accuracy: 0.9650\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1492 - accuracy: 0.9583 - val_loss: 0.1214 - val_accuracy: 0.9648\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1480 - accuracy: 0.9579 - val_loss: 0.1208 - val_accuracy: 0.9652\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1484 - accuracy: 0.9584 - val_loss: 0.1212 - val_accuracy: 0.9651\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1480 - accuracy: 0.9582 - val_loss: 0.1211 - val_accuracy: 0.9652\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1463 - accuracy: 0.9585 - val_loss: 0.1211 - val_accuracy: 0.9654\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1476 - accuracy: 0.9587 - val_loss: 0.1211 - val_accuracy: 0.9652\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1483 - accuracy: 0.9582 - val_loss: 0.1208 - val_accuracy: 0.9651\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1476 - accuracy: 0.9579 - val_loss: 0.1208 - val_accuracy: 0.9655\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1467 - accuracy: 0.9589 - val_loss: 0.1204 - val_accuracy: 0.9656\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1473 - accuracy: 0.9583 - val_loss: 0.1202 - val_accuracy: 0.9658\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1455 - accuracy: 0.9589 - val_loss: 0.1201 - val_accuracy: 0.9660\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1466 - accuracy: 0.9586 - val_loss: 0.1203 - val_accuracy: 0.9655\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1473 - accuracy: 0.9585 - val_loss: 0.1198 - val_accuracy: 0.9661\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1461 - accuracy: 0.9593 - val_loss: 0.1198 - val_accuracy: 0.9660\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1478 - accuracy: 0.9587 - val_loss: 0.1200 - val_accuracy: 0.9661\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1480 - accuracy: 0.9586 - val_loss: 0.1203 - val_accuracy: 0.9657\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1460 - accuracy: 0.9589 - val_loss: 0.1201 - val_accuracy: 0.9656\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1468 - accuracy: 0.9587 - val_loss: 0.1205 - val_accuracy: 0.9657\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1453 - accuracy: 0.9599 - val_loss: 0.1200 - val_accuracy: 0.9660\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1461 - accuracy: 0.9591 - val_loss: 0.1197 - val_accuracy: 0.9663\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1463 - accuracy: 0.9592 - val_loss: 0.1195 - val_accuracy: 0.9662\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1448 - accuracy: 0.9592 - val_loss: 0.1193 - val_accuracy: 0.9661\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1455 - accuracy: 0.9591 - val_loss: 0.1190 - val_accuracy: 0.9666\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1449 - accuracy: 0.9598 - val_loss: 0.1191 - val_accuracy: 0.9665\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1452 - accuracy: 0.9593 - val_loss: 0.1194 - val_accuracy: 0.9665\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1447 - accuracy: 0.9590 - val_loss: 0.1191 - val_accuracy: 0.9663\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1450 - accuracy: 0.9595 - val_loss: 0.1194 - val_accuracy: 0.9663\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1464 - accuracy: 0.9593 - val_loss: 0.1194 - val_accuracy: 0.9662\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1472 - accuracy: 0.9591 - val_loss: 0.1188 - val_accuracy: 0.9664\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1450 - accuracy: 0.9590 - val_loss: 0.1192 - val_accuracy: 0.9666\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1450 - accuracy: 0.9599 - val_loss: 0.1190 - val_accuracy: 0.9664\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1449 - accuracy: 0.9594 - val_loss: 0.1191 - val_accuracy: 0.9665\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1466 - accuracy: 0.9595 - val_loss: 0.1189 - val_accuracy: 0.9664\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1459 - accuracy: 0.9589 - val_loss: 0.1188 - val_accuracy: 0.9666\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1446 - accuracy: 0.9594 - val_loss: 0.1189 - val_accuracy: 0.9659\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1449 - accuracy: 0.9596 - val_loss: 0.1186 - val_accuracy: 0.9666\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1459 - accuracy: 0.9592 - val_loss: 0.1186 - val_accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1467 - accuracy: 0.9594 - val_loss: 0.1185 - val_accuracy: 0.9664\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1438 - accuracy: 0.9596 - val_loss: 0.1185 - val_accuracy: 0.9666\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1449 - accuracy: 0.9592 - val_loss: 0.1187 - val_accuracy: 0.9667\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1448 - accuracy: 0.9595 - val_loss: 0.1184 - val_accuracy: 0.9666\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1447 - accuracy: 0.9598 - val_loss: 0.1187 - val_accuracy: 0.9663\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1450 - accuracy: 0.9593 - val_loss: 0.1184 - val_accuracy: 0.9666\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1438 - accuracy: 0.9598 - val_loss: 0.1185 - val_accuracy: 0.9667\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1437 - accuracy: 0.9599 - val_loss: 0.1186 - val_accuracy: 0.9663\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1436 - accuracy: 0.9597 - val_loss: 0.1184 - val_accuracy: 0.9664\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1439 - accuracy: 0.9596 - val_loss: 0.1183 - val_accuracy: 0.9665\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1435 - accuracy: 0.9599 - val_loss: 0.1185 - val_accuracy: 0.9665\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1438 - accuracy: 0.9601 - val_loss: 0.1183 - val_accuracy: 0.9667\n",
      "463/463 [==============================] - 1s 2ms/step\n",
      "4162/4162 [==============================] - 6s 2ms/step\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.4701 - accuracy: 0.8053 - val_loss: 0.2221 - val_accuracy: 0.9468\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2387 - accuracy: 0.9318 - val_loss: 0.1572 - val_accuracy: 0.9544\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2046 - accuracy: 0.9423 - val_loss: 0.1467 - val_accuracy: 0.9572\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1971 - accuracy: 0.9456 - val_loss: 0.1430 - val_accuracy: 0.9575\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1916 - accuracy: 0.9466 - val_loss: 0.1408 - val_accuracy: 0.9578\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1873 - accuracy: 0.9490 - val_loss: 0.1395 - val_accuracy: 0.9581\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1849 - accuracy: 0.9496 - val_loss: 0.1384 - val_accuracy: 0.9586\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1789 - accuracy: 0.9505 - val_loss: 0.1373 - val_accuracy: 0.9590\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1789 - accuracy: 0.9499 - val_loss: 0.1364 - val_accuracy: 0.9595\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1768 - accuracy: 0.9509 - val_loss: 0.1353 - val_accuracy: 0.9594\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1755 - accuracy: 0.9511 - val_loss: 0.1347 - val_accuracy: 0.9596\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1732 - accuracy: 0.9516 - val_loss: 0.1341 - val_accuracy: 0.9599\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1720 - accuracy: 0.9519 - val_loss: 0.1335 - val_accuracy: 0.9604\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1705 - accuracy: 0.9519 - val_loss: 0.1329 - val_accuracy: 0.9605\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1679 - accuracy: 0.9529 - val_loss: 0.1325 - val_accuracy: 0.9602\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1673 - accuracy: 0.9530 - val_loss: 0.1322 - val_accuracy: 0.9604\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1676 - accuracy: 0.9535 - val_loss: 0.1319 - val_accuracy: 0.9607\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1669 - accuracy: 0.9535 - val_loss: 0.1313 - val_accuracy: 0.9607\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1665 - accuracy: 0.9527 - val_loss: 0.1311 - val_accuracy: 0.9611\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1638 - accuracy: 0.9541 - val_loss: 0.1309 - val_accuracy: 0.9610\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1647 - accuracy: 0.9541 - val_loss: 0.1304 - val_accuracy: 0.9611\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1637 - accuracy: 0.9533 - val_loss: 0.1302 - val_accuracy: 0.9614\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1635 - accuracy: 0.9535 - val_loss: 0.1303 - val_accuracy: 0.9611\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1642 - accuracy: 0.9533 - val_loss: 0.1301 - val_accuracy: 0.9613\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1633 - accuracy: 0.9535 - val_loss: 0.1296 - val_accuracy: 0.9618\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1633 - accuracy: 0.9533 - val_loss: 0.1296 - val_accuracy: 0.9617\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1628 - accuracy: 0.9535 - val_loss: 0.1292 - val_accuracy: 0.9616\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1619 - accuracy: 0.9538 - val_loss: 0.1288 - val_accuracy: 0.9616\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1605 - accuracy: 0.9544 - val_loss: 0.1289 - val_accuracy: 0.9619\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1598 - accuracy: 0.9545 - val_loss: 0.1290 - val_accuracy: 0.9617\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1616 - accuracy: 0.9545 - val_loss: 0.1284 - val_accuracy: 0.9620\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1619 - accuracy: 0.9540 - val_loss: 0.1284 - val_accuracy: 0.9619\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1598 - accuracy: 0.9543 - val_loss: 0.1280 - val_accuracy: 0.9621\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1598 - accuracy: 0.9543 - val_loss: 0.1278 - val_accuracy: 0.9624\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1599 - accuracy: 0.9542 - val_loss: 0.1276 - val_accuracy: 0.9626\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1593 - accuracy: 0.9541 - val_loss: 0.1276 - val_accuracy: 0.9624\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1594 - accuracy: 0.9541 - val_loss: 0.1276 - val_accuracy: 0.9626\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1597 - accuracy: 0.9545 - val_loss: 0.1279 - val_accuracy: 0.9624\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1592 - accuracy: 0.9546 - val_loss: 0.1271 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1595 - accuracy: 0.9548 - val_loss: 0.1269 - val_accuracy: 0.9628\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1591 - accuracy: 0.9547 - val_loss: 0.1268 - val_accuracy: 0.9625\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1588 - accuracy: 0.9548 - val_loss: 0.1265 - val_accuracy: 0.9629\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1597 - accuracy: 0.9549 - val_loss: 0.1266 - val_accuracy: 0.9627\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1588 - accuracy: 0.9542 - val_loss: 0.1265 - val_accuracy: 0.9625\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1589 - accuracy: 0.9550 - val_loss: 0.1261 - val_accuracy: 0.9632\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1586 - accuracy: 0.9551 - val_loss: 0.1258 - val_accuracy: 0.9632\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1566 - accuracy: 0.9550 - val_loss: 0.1259 - val_accuracy: 0.9633\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1585 - accuracy: 0.9547 - val_loss: 0.1258 - val_accuracy: 0.9630\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1577 - accuracy: 0.9551 - val_loss: 0.1257 - val_accuracy: 0.9628\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1567 - accuracy: 0.9555 - val_loss: 0.1258 - val_accuracy: 0.9628\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1580 - accuracy: 0.9546 - val_loss: 0.1252 - val_accuracy: 0.9631\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1569 - accuracy: 0.9552 - val_loss: 0.1252 - val_accuracy: 0.9632\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1559 - accuracy: 0.9552 - val_loss: 0.1251 - val_accuracy: 0.9632\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1565 - accuracy: 0.9554 - val_loss: 0.1250 - val_accuracy: 0.9632\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1568 - accuracy: 0.9546 - val_loss: 0.1248 - val_accuracy: 0.9635\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1562 - accuracy: 0.9551 - val_loss: 0.1248 - val_accuracy: 0.9633\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1578 - accuracy: 0.9550 - val_loss: 0.1249 - val_accuracy: 0.9629\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1564 - accuracy: 0.9552 - val_loss: 0.1244 - val_accuracy: 0.9637\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1572 - accuracy: 0.9551 - val_loss: 0.1243 - val_accuracy: 0.9635\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1574 - accuracy: 0.9552 - val_loss: 0.1246 - val_accuracy: 0.9635\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1557 - accuracy: 0.9552 - val_loss: 0.1242 - val_accuracy: 0.9636\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1554 - accuracy: 0.9553 - val_loss: 0.1242 - val_accuracy: 0.9633\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1554 - accuracy: 0.9558 - val_loss: 0.1242 - val_accuracy: 0.9634\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1556 - accuracy: 0.9552 - val_loss: 0.1241 - val_accuracy: 0.9632\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1551 - accuracy: 0.9552 - val_loss: 0.1241 - val_accuracy: 0.9636\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1547 - accuracy: 0.9559 - val_loss: 0.1241 - val_accuracy: 0.9636\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1557 - accuracy: 0.9553 - val_loss: 0.1243 - val_accuracy: 0.9638\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1548 - accuracy: 0.9555 - val_loss: 0.1240 - val_accuracy: 0.9637\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1562 - accuracy: 0.9551 - val_loss: 0.1239 - val_accuracy: 0.9632\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1557 - accuracy: 0.9554 - val_loss: 0.1241 - val_accuracy: 0.9635\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1557 - accuracy: 0.9557 - val_loss: 0.1243 - val_accuracy: 0.9633\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1548 - accuracy: 0.9548 - val_loss: 0.1238 - val_accuracy: 0.9632\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1545 - accuracy: 0.9554 - val_loss: 0.1238 - val_accuracy: 0.9638\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1541 - accuracy: 0.9559 - val_loss: 0.1237 - val_accuracy: 0.9634\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1548 - accuracy: 0.9550 - val_loss: 0.1236 - val_accuracy: 0.9634\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1543 - accuracy: 0.9552 - val_loss: 0.1236 - val_accuracy: 0.9633\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1541 - accuracy: 0.9554 - val_loss: 0.1233 - val_accuracy: 0.9635\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1542 - accuracy: 0.9556 - val_loss: 0.1238 - val_accuracy: 0.9633\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1543 - accuracy: 0.9555 - val_loss: 0.1231 - val_accuracy: 0.9638\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1544 - accuracy: 0.9563 - val_loss: 0.1232 - val_accuracy: 0.9635\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1537 - accuracy: 0.9554 - val_loss: 0.1229 - val_accuracy: 0.9638\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1536 - accuracy: 0.9559 - val_loss: 0.1232 - val_accuracy: 0.9634\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1536 - accuracy: 0.9554 - val_loss: 0.1227 - val_accuracy: 0.9640\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1536 - accuracy: 0.9555 - val_loss: 0.1231 - val_accuracy: 0.9637\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1533 - accuracy: 0.9555 - val_loss: 0.1229 - val_accuracy: 0.9637\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1555 - accuracy: 0.9555 - val_loss: 0.1225 - val_accuracy: 0.9637\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1539 - accuracy: 0.9555 - val_loss: 0.1228 - val_accuracy: 0.9637\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1535 - accuracy: 0.9551 - val_loss: 0.1226 - val_accuracy: 0.9636\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1540 - accuracy: 0.9548 - val_loss: 0.1225 - val_accuracy: 0.9639\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1536 - accuracy: 0.9559 - val_loss: 0.1225 - val_accuracy: 0.9637\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1540 - accuracy: 0.9562 - val_loss: 0.1229 - val_accuracy: 0.9634\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1525 - accuracy: 0.9558 - val_loss: 0.1224 - val_accuracy: 0.9636\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1527 - accuracy: 0.9558 - val_loss: 0.1222 - val_accuracy: 0.9638\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1530 - accuracy: 0.9564 - val_loss: 0.1223 - val_accuracy: 0.9638\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1538 - accuracy: 0.9561 - val_loss: 0.1220 - val_accuracy: 0.9638\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1519 - accuracy: 0.9558 - val_loss: 0.1219 - val_accuracy: 0.9637\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1518 - accuracy: 0.9562 - val_loss: 0.1221 - val_accuracy: 0.9636\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1539 - accuracy: 0.9558 - val_loss: 0.1225 - val_accuracy: 0.9640\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1530 - accuracy: 0.9563 - val_loss: 0.1217 - val_accuracy: 0.9640\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1521 - accuracy: 0.9565 - val_loss: 0.1222 - val_accuracy: 0.9637\n",
      "463/463 [==============================] - 1s 1ms/step\n",
      "4162/4162 [==============================] - 6s 2ms/step\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 22)]              0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 12)                276       \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379 (1.48 KB)\n",
      "Trainable params: 379 (1.48 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/100\n",
      "3330/3330 [==============================] - 9s 2ms/step - loss: 0.5097 - accuracy: 0.7196 - val_loss: 0.2374 - val_accuracy: 0.9380\n",
      "Epoch 2/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2648 - accuracy: 0.8874 - val_loss: 0.1566 - val_accuracy: 0.9518\n",
      "Epoch 3/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2242 - accuracy: 0.9328 - val_loss: 0.1468 - val_accuracy: 0.9549\n",
      "Epoch 4/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.2078 - accuracy: 0.9430 - val_loss: 0.1435 - val_accuracy: 0.9569\n",
      "Epoch 5/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1961 - accuracy: 0.9458 - val_loss: 0.1409 - val_accuracy: 0.9583\n",
      "Epoch 6/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1888 - accuracy: 0.9481 - val_loss: 0.1383 - val_accuracy: 0.9590\n",
      "Epoch 7/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1836 - accuracy: 0.9489 - val_loss: 0.1376 - val_accuracy: 0.9600\n",
      "Epoch 8/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1792 - accuracy: 0.9497 - val_loss: 0.1371 - val_accuracy: 0.9599\n",
      "Epoch 9/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1767 - accuracy: 0.9501 - val_loss: 0.1361 - val_accuracy: 0.9602\n",
      "Epoch 10/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1746 - accuracy: 0.9498 - val_loss: 0.1355 - val_accuracy: 0.9603\n",
      "Epoch 11/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1730 - accuracy: 0.9503 - val_loss: 0.1350 - val_accuracy: 0.9607\n",
      "Epoch 12/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1730 - accuracy: 0.9510 - val_loss: 0.1348 - val_accuracy: 0.9609\n",
      "Epoch 13/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1704 - accuracy: 0.9503 - val_loss: 0.1342 - val_accuracy: 0.9610\n",
      "Epoch 14/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1690 - accuracy: 0.9507 - val_loss: 0.1335 - val_accuracy: 0.9614\n",
      "Epoch 15/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1664 - accuracy: 0.9513 - val_loss: 0.1335 - val_accuracy: 0.9611\n",
      "Epoch 16/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1656 - accuracy: 0.9510 - val_loss: 0.1328 - val_accuracy: 0.9615\n",
      "Epoch 17/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1656 - accuracy: 0.9514 - val_loss: 0.1324 - val_accuracy: 0.9619\n",
      "Epoch 18/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1647 - accuracy: 0.9525 - val_loss: 0.1321 - val_accuracy: 0.9619\n",
      "Epoch 19/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1625 - accuracy: 0.9546 - val_loss: 0.1316 - val_accuracy: 0.9619\n",
      "Epoch 20/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1616 - accuracy: 0.9549 - val_loss: 0.1315 - val_accuracy: 0.9620\n",
      "Epoch 21/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1622 - accuracy: 0.9546 - val_loss: 0.1312 - val_accuracy: 0.9618\n",
      "Epoch 22/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1623 - accuracy: 0.9549 - val_loss: 0.1308 - val_accuracy: 0.9619\n",
      "Epoch 23/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1606 - accuracy: 0.9548 - val_loss: 0.1308 - val_accuracy: 0.9620\n",
      "Epoch 24/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1598 - accuracy: 0.9557 - val_loss: 0.1305 - val_accuracy: 0.9621\n",
      "Epoch 25/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1587 - accuracy: 0.9550 - val_loss: 0.1300 - val_accuracy: 0.9620\n",
      "Epoch 26/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1591 - accuracy: 0.9551 - val_loss: 0.1298 - val_accuracy: 0.9622\n",
      "Epoch 27/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1584 - accuracy: 0.9556 - val_loss: 0.1299 - val_accuracy: 0.9618\n",
      "Epoch 28/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1582 - accuracy: 0.9549 - val_loss: 0.1296 - val_accuracy: 0.9624\n",
      "Epoch 29/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1584 - accuracy: 0.9545 - val_loss: 0.1295 - val_accuracy: 0.9625\n",
      "Epoch 30/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1570 - accuracy: 0.9553 - val_loss: 0.1290 - val_accuracy: 0.9623\n",
      "Epoch 31/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1564 - accuracy: 0.9553 - val_loss: 0.1289 - val_accuracy: 0.9628\n",
      "Epoch 32/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1560 - accuracy: 0.9557 - val_loss: 0.1286 - val_accuracy: 0.9627\n",
      "Epoch 33/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1560 - accuracy: 0.9554 - val_loss: 0.1281 - val_accuracy: 0.9629\n",
      "Epoch 34/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1544 - accuracy: 0.9555 - val_loss: 0.1281 - val_accuracy: 0.9628\n",
      "Epoch 35/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1562 - accuracy: 0.9551 - val_loss: 0.1276 - val_accuracy: 0.9629\n",
      "Epoch 36/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1541 - accuracy: 0.9555 - val_loss: 0.1276 - val_accuracy: 0.9633\n",
      "Epoch 37/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1536 - accuracy: 0.9561 - val_loss: 0.1275 - val_accuracy: 0.9631\n",
      "Epoch 38/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1550 - accuracy: 0.9554 - val_loss: 0.1273 - val_accuracy: 0.9632\n",
      "Epoch 39/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1552 - accuracy: 0.9557 - val_loss: 0.1273 - val_accuracy: 0.9632\n",
      "Epoch 40/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1537 - accuracy: 0.9557 - val_loss: 0.1272 - val_accuracy: 0.9633\n",
      "Epoch 41/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1533 - accuracy: 0.9556 - val_loss: 0.1264 - val_accuracy: 0.9637\n",
      "Epoch 42/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1539 - accuracy: 0.9559 - val_loss: 0.1262 - val_accuracy: 0.9634\n",
      "Epoch 43/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1531 - accuracy: 0.9557 - val_loss: 0.1266 - val_accuracy: 0.9633\n",
      "Epoch 44/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1531 - accuracy: 0.9561 - val_loss: 0.1264 - val_accuracy: 0.9635\n",
      "Epoch 45/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1526 - accuracy: 0.9562 - val_loss: 0.1263 - val_accuracy: 0.9636\n",
      "Epoch 46/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1524 - accuracy: 0.9565 - val_loss: 0.1258 - val_accuracy: 0.9639\n",
      "Epoch 47/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1526 - accuracy: 0.9566 - val_loss: 0.1257 - val_accuracy: 0.9639\n",
      "Epoch 48/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1524 - accuracy: 0.9562 - val_loss: 0.1253 - val_accuracy: 0.9638\n",
      "Epoch 49/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1517 - accuracy: 0.9560 - val_loss: 0.1254 - val_accuracy: 0.9639\n",
      "Epoch 50/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1509 - accuracy: 0.9570 - val_loss: 0.1252 - val_accuracy: 0.9637\n",
      "Epoch 51/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1497 - accuracy: 0.9573 - val_loss: 0.1255 - val_accuracy: 0.9632\n",
      "Epoch 52/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1518 - accuracy: 0.9564 - val_loss: 0.1251 - val_accuracy: 0.9636\n",
      "Epoch 53/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1504 - accuracy: 0.9572 - val_loss: 0.1251 - val_accuracy: 0.9635\n",
      "Epoch 54/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1508 - accuracy: 0.9572 - val_loss: 0.1251 - val_accuracy: 0.9638\n",
      "Epoch 55/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1509 - accuracy: 0.9568 - val_loss: 0.1244 - val_accuracy: 0.9640\n",
      "Epoch 56/100\n",
      "3330/3330 [==============================] - 11s 3ms/step - loss: 0.1495 - accuracy: 0.9573 - val_loss: 0.1245 - val_accuracy: 0.9643\n",
      "Epoch 57/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1498 - accuracy: 0.9567 - val_loss: 0.1238 - val_accuracy: 0.9640\n",
      "Epoch 58/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1487 - accuracy: 0.9573 - val_loss: 0.1243 - val_accuracy: 0.9639\n",
      "Epoch 59/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1496 - accuracy: 0.9572 - val_loss: 0.1238 - val_accuracy: 0.9640\n",
      "Epoch 60/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1497 - accuracy: 0.9573 - val_loss: 0.1237 - val_accuracy: 0.9645\n",
      "Epoch 61/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1505 - accuracy: 0.9569 - val_loss: 0.1234 - val_accuracy: 0.9645\n",
      "Epoch 62/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1483 - accuracy: 0.9579 - val_loss: 0.1233 - val_accuracy: 0.9645\n",
      "Epoch 63/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1495 - accuracy: 0.9570 - val_loss: 0.1233 - val_accuracy: 0.9645\n",
      "Epoch 64/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1485 - accuracy: 0.9574 - val_loss: 0.1235 - val_accuracy: 0.9646\n",
      "Epoch 65/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1498 - accuracy: 0.9574 - val_loss: 0.1232 - val_accuracy: 0.9644\n",
      "Epoch 66/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1492 - accuracy: 0.9578 - val_loss: 0.1229 - val_accuracy: 0.9648\n",
      "Epoch 67/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1487 - accuracy: 0.9577 - val_loss: 0.1230 - val_accuracy: 0.9645\n",
      "Epoch 68/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1485 - accuracy: 0.9576 - val_loss: 0.1231 - val_accuracy: 0.9649\n",
      "Epoch 69/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1496 - accuracy: 0.9572 - val_loss: 0.1227 - val_accuracy: 0.9646\n",
      "Epoch 70/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1482 - accuracy: 0.9577 - val_loss: 0.1227 - val_accuracy: 0.9648\n",
      "Epoch 71/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1484 - accuracy: 0.9575 - val_loss: 0.1225 - val_accuracy: 0.9650\n",
      "Epoch 72/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1479 - accuracy: 0.9577 - val_loss: 0.1223 - val_accuracy: 0.9649\n",
      "Epoch 73/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1487 - accuracy: 0.9575 - val_loss: 0.1220 - val_accuracy: 0.9647\n",
      "Epoch 74/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1478 - accuracy: 0.9581 - val_loss: 0.1225 - val_accuracy: 0.9651\n",
      "Epoch 75/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1470 - accuracy: 0.9579 - val_loss: 0.1223 - val_accuracy: 0.9651\n",
      "Epoch 76/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1479 - accuracy: 0.9577 - val_loss: 0.1222 - val_accuracy: 0.9649\n",
      "Epoch 77/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1469 - accuracy: 0.9579 - val_loss: 0.1220 - val_accuracy: 0.9647\n",
      "Epoch 78/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1480 - accuracy: 0.9575 - val_loss: 0.1223 - val_accuracy: 0.9644\n",
      "Epoch 79/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1473 - accuracy: 0.9578 - val_loss: 0.1221 - val_accuracy: 0.9647\n",
      "Epoch 80/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1478 - accuracy: 0.9577 - val_loss: 0.1216 - val_accuracy: 0.9653\n",
      "Epoch 81/100\n",
      "3330/3330 [==============================] - 10s 3ms/step - loss: 0.1479 - accuracy: 0.9580 - val_loss: 0.1220 - val_accuracy: 0.9649\n",
      "Epoch 82/100\n",
      "3330/3330 [==============================] - 8s 3ms/step - loss: 0.1476 - accuracy: 0.9578 - val_loss: 0.1215 - val_accuracy: 0.9652\n",
      "Epoch 83/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1470 - accuracy: 0.9580 - val_loss: 0.1216 - val_accuracy: 0.9652\n",
      "Epoch 84/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1474 - accuracy: 0.9576 - val_loss: 0.1217 - val_accuracy: 0.9655\n",
      "Epoch 85/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1479 - accuracy: 0.9579 - val_loss: 0.1218 - val_accuracy: 0.9652\n",
      "Epoch 86/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1468 - accuracy: 0.9581 - val_loss: 0.1214 - val_accuracy: 0.9652\n",
      "Epoch 87/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1483 - accuracy: 0.9573 - val_loss: 0.1214 - val_accuracy: 0.9650\n",
      "Epoch 88/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1455 - accuracy: 0.9590 - val_loss: 0.1218 - val_accuracy: 0.9651\n",
      "Epoch 89/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1460 - accuracy: 0.9581 - val_loss: 0.1214 - val_accuracy: 0.9654\n",
      "Epoch 90/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1469 - accuracy: 0.9582 - val_loss: 0.1213 - val_accuracy: 0.9653\n",
      "Epoch 91/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1466 - accuracy: 0.9583 - val_loss: 0.1212 - val_accuracy: 0.9653\n",
      "Epoch 92/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1477 - accuracy: 0.9580 - val_loss: 0.1211 - val_accuracy: 0.9654\n",
      "Epoch 93/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1455 - accuracy: 0.9585 - val_loss: 0.1211 - val_accuracy: 0.9655\n",
      "Epoch 94/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1467 - accuracy: 0.9581 - val_loss: 0.1212 - val_accuracy: 0.9655\n",
      "Epoch 95/100\n",
      "3330/3330 [==============================] - 9s 3ms/step - loss: 0.1464 - accuracy: 0.9586 - val_loss: 0.1210 - val_accuracy: 0.9654\n",
      "Epoch 96/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1466 - accuracy: 0.9580 - val_loss: 0.1209 - val_accuracy: 0.9655\n",
      "Epoch 97/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1457 - accuracy: 0.9584 - val_loss: 0.1209 - val_accuracy: 0.9657\n",
      "Epoch 98/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1463 - accuracy: 0.9583 - val_loss: 0.1208 - val_accuracy: 0.9655\n",
      "Epoch 99/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1459 - accuracy: 0.9585 - val_loss: 0.1213 - val_accuracy: 0.9655\n",
      "Epoch 100/100\n",
      "3330/3330 [==============================] - 8s 2ms/step - loss: 0.1461 - accuracy: 0.9585 - val_loss: 0.1213 - val_accuracy: 0.9658\n",
      "463/463 [==============================] - 1s 2ms/step\n",
      "4162/4162 [==============================] - 6s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=32, verbose=1, callbacks = [earlystopping], validation_split=0.2)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scoring = {'acc': 'accuracy',\n",
    "           'mse': 'neg_mean_squared_error'}\n",
    "results = cross_validate(model, X_train, y_train, scoring=scoring, cv=kfold, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([797.05461359, 813.82245708, 814.11798   , 805.77991176,\n",
       "        851.3776257 , 838.90474105, 812.4328053 , 803.38641882,\n",
       "        824.74007654, 882.63110614]),\n",
       " 'score_time': array([1.19007421, 1.08218455, 1.03200459, 0.98366022, 2.38714552,\n",
       "        1.89262652, 1.40248823, 1.22232795, 1.31162763, 1.70097399]),\n",
       " 'test_acc': array([0.96303555, 0.96296547, 0.96424951, 0.96472258, 0.96546597,\n",
       "        0.96168142, 0.96431709, 0.96438467, 0.96445225, 0.96384402]),\n",
       " 'train_acc': array([0.96383651, 0.96480544, 0.9651884 , 0.9643549 , 0.96490306,\n",
       "        0.96333368, 0.96331867, 0.96646493, 0.96418971, 0.96551129]),\n",
       " 'test_mse': array([-0.03696445, -0.03703453, -0.03575049, -0.03527742, -0.03453403,\n",
       "        -0.03831858, -0.03568291, -0.03561533, -0.03554775, -0.03615598]),\n",
       " 'train_mse': array([-0.03616349, -0.03519456, -0.0348116 , -0.0356451 , -0.03509694,\n",
       "        -0.03666632, -0.03668133, -0.03353507, -0.03581029, -0.03448871])}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 24s 2ms/step - loss: 0.4381 - accuracy: 0.8537 - val_loss: 0.3031 - val_accuracy: 0.9595\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.2865 - accuracy: 0.9539 - val_loss: 0.2372 - val_accuracy: 0.9624\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.2302 - accuracy: 0.9569 - val_loss: 0.1924 - val_accuracy: 0.9637\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1957 - accuracy: 0.9583 - val_loss: 0.1644 - val_accuracy: 0.9657\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1762 - accuracy: 0.9582 - val_loss: 0.1479 - val_accuracy: 0.9660\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1654 - accuracy: 0.9585 - val_loss: 0.1397 - val_accuracy: 0.9658\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1560 - accuracy: 0.9593 - val_loss: 0.1343 - val_accuracy: 0.9662\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1525 - accuracy: 0.9595 - val_loss: 0.1314 - val_accuracy: 0.9664\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1497 - accuracy: 0.9593 - val_loss: 0.1298 - val_accuracy: 0.9665\n",
      "Epoch 10/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1490 - accuracy: 0.9598 - val_loss: 0.1286 - val_accuracy: 0.9664\n",
      "Epoch 11/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1468 - accuracy: 0.9595 - val_loss: 0.1264 - val_accuracy: 0.9659\n",
      "Epoch 12/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1448 - accuracy: 0.9598 - val_loss: 0.1251 - val_accuracy: 0.9666\n",
      "Epoch 13/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1451 - accuracy: 0.9595 - val_loss: 0.1246 - val_accuracy: 0.9663\n",
      "Epoch 14/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1427 - accuracy: 0.9595 - val_loss: 0.1247 - val_accuracy: 0.9668\n",
      "Epoch 15/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1429 - accuracy: 0.9596 - val_loss: 0.1228 - val_accuracy: 0.9671\n",
      "Epoch 16/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1415 - accuracy: 0.9603 - val_loss: 0.1227 - val_accuracy: 0.9677\n",
      "Epoch 17/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1403 - accuracy: 0.9600 - val_loss: 0.1212 - val_accuracy: 0.9682\n",
      "Epoch 18/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1402 - accuracy: 0.9602 - val_loss: 0.1214 - val_accuracy: 0.9682\n",
      "Epoch 19/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1383 - accuracy: 0.9609 - val_loss: 0.1203 - val_accuracy: 0.9679\n",
      "Epoch 20/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1385 - accuracy: 0.9607 - val_loss: 0.1196 - val_accuracy: 0.9684\n",
      "Epoch 21/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1374 - accuracy: 0.9610 - val_loss: 0.1184 - val_accuracy: 0.9684\n",
      "Epoch 22/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1371 - accuracy: 0.9611 - val_loss: 0.1183 - val_accuracy: 0.9685\n",
      "Epoch 23/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1383 - accuracy: 0.9612 - val_loss: 0.1172 - val_accuracy: 0.9687\n",
      "Epoch 24/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1365 - accuracy: 0.9614 - val_loss: 0.1169 - val_accuracy: 0.9692\n",
      "Epoch 25/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1354 - accuracy: 0.9618 - val_loss: 0.1167 - val_accuracy: 0.9690\n",
      "Epoch 26/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1355 - accuracy: 0.9620 - val_loss: 0.1165 - val_accuracy: 0.9689\n",
      "Epoch 27/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1350 - accuracy: 0.9618 - val_loss: 0.1166 - val_accuracy: 0.9691\n",
      "Epoch 28/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1362 - accuracy: 0.9618 - val_loss: 0.1156 - val_accuracy: 0.9695\n",
      "Epoch 29/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1337 - accuracy: 0.9624 - val_loss: 0.1159 - val_accuracy: 0.9696\n",
      "Epoch 30/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1341 - accuracy: 0.9623 - val_loss: 0.1151 - val_accuracy: 0.9695\n",
      "Epoch 31/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1340 - accuracy: 0.9620 - val_loss: 0.1151 - val_accuracy: 0.9693\n",
      "Epoch 32/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1335 - accuracy: 0.9624 - val_loss: 0.1147 - val_accuracy: 0.9689\n",
      "Epoch 33/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1334 - accuracy: 0.9627 - val_loss: 0.1140 - val_accuracy: 0.9695\n",
      "Epoch 34/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1338 - accuracy: 0.9629 - val_loss: 0.1141 - val_accuracy: 0.9692\n",
      "Epoch 35/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1332 - accuracy: 0.9626 - val_loss: 0.1140 - val_accuracy: 0.9698\n",
      "Epoch 36/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1328 - accuracy: 0.9630 - val_loss: 0.1142 - val_accuracy: 0.9696\n",
      "Epoch 37/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1326 - accuracy: 0.9629 - val_loss: 0.1130 - val_accuracy: 0.9698\n",
      "Epoch 38/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1329 - accuracy: 0.9629 - val_loss: 0.1141 - val_accuracy: 0.9696\n",
      "Epoch 39/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1334 - accuracy: 0.9630 - val_loss: 0.1133 - val_accuracy: 0.9699\n",
      "Epoch 40/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1330 - accuracy: 0.9629 - val_loss: 0.1131 - val_accuracy: 0.9698\n",
      "Epoch 41/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1314 - accuracy: 0.9629 - val_loss: 0.1135 - val_accuracy: 0.9701\n",
      "Epoch 42/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1329 - accuracy: 0.9629 - val_loss: 0.1133 - val_accuracy: 0.9699\n",
      "Epoch 43/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1315 - accuracy: 0.9631 - val_loss: 0.1128 - val_accuracy: 0.9703\n",
      "Epoch 44/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1310 - accuracy: 0.9634 - val_loss: 0.1130 - val_accuracy: 0.9699\n",
      "Epoch 45/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1319 - accuracy: 0.9632 - val_loss: 0.1122 - val_accuracy: 0.9705\n",
      "Epoch 46/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1318 - accuracy: 0.9634 - val_loss: 0.1127 - val_accuracy: 0.9702\n",
      "Epoch 47/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1315 - accuracy: 0.9632 - val_loss: 0.1112 - val_accuracy: 0.9706\n",
      "Epoch 48/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1316 - accuracy: 0.9631 - val_loss: 0.1111 - val_accuracy: 0.9702\n",
      "Epoch 49/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1321 - accuracy: 0.9634 - val_loss: 0.1118 - val_accuracy: 0.9705\n",
      "Epoch 50/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1314 - accuracy: 0.9634 - val_loss: 0.1114 - val_accuracy: 0.9704\n",
      "Epoch 51/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1311 - accuracy: 0.9632 - val_loss: 0.1118 - val_accuracy: 0.9707\n",
      "Epoch 52/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1294 - accuracy: 0.9636 - val_loss: 0.1119 - val_accuracy: 0.9709\n",
      "Epoch 53/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1315 - accuracy: 0.9632 - val_loss: 0.1117 - val_accuracy: 0.9706\n",
      "Epoch 54/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1299 - accuracy: 0.9635 - val_loss: 0.1118 - val_accuracy: 0.9705\n",
      "Epoch 55/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1305 - accuracy: 0.9633 - val_loss: 0.1114 - val_accuracy: 0.9707\n",
      "Epoch 56/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1288 - accuracy: 0.9637 - val_loss: 0.1109 - val_accuracy: 0.9709\n",
      "Epoch 57/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1303 - accuracy: 0.9634 - val_loss: 0.1110 - val_accuracy: 0.9707\n",
      "Epoch 58/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1299 - accuracy: 0.9638 - val_loss: 0.1102 - val_accuracy: 0.9706\n",
      "Epoch 59/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1293 - accuracy: 0.9635 - val_loss: 0.1102 - val_accuracy: 0.9707\n",
      "Epoch 60/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1304 - accuracy: 0.9632 - val_loss: 0.1099 - val_accuracy: 0.9705\n",
      "Epoch 61/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1304 - accuracy: 0.9631 - val_loss: 0.1105 - val_accuracy: 0.9706\n",
      "Epoch 62/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1301 - accuracy: 0.9635 - val_loss: 0.1100 - val_accuracy: 0.9707\n",
      "Epoch 63/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1291 - accuracy: 0.9635 - val_loss: 0.1090 - val_accuracy: 0.9707\n",
      "Epoch 64/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1296 - accuracy: 0.9633 - val_loss: 0.1101 - val_accuracy: 0.9707\n",
      "Epoch 65/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1302 - accuracy: 0.9634 - val_loss: 0.1104 - val_accuracy: 0.9709\n",
      "Epoch 66/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1300 - accuracy: 0.9637 - val_loss: 0.1095 - val_accuracy: 0.9709\n",
      "Epoch 67/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1295 - accuracy: 0.9635 - val_loss: 0.1097 - val_accuracy: 0.9706\n",
      "Epoch 68/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1292 - accuracy: 0.9636 - val_loss: 0.1102 - val_accuracy: 0.9706\n",
      "Epoch 69/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1294 - accuracy: 0.9636 - val_loss: 0.1102 - val_accuracy: 0.9706\n",
      "Epoch 70/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1290 - accuracy: 0.9640 - val_loss: 0.1091 - val_accuracy: 0.9709\n",
      "Epoch 71/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1294 - accuracy: 0.9636 - val_loss: 0.1093 - val_accuracy: 0.9708\n",
      "Epoch 72/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1303 - accuracy: 0.9632 - val_loss: 0.1096 - val_accuracy: 0.9708\n",
      "Epoch 73/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1295 - accuracy: 0.9638 - val_loss: 0.1096 - val_accuracy: 0.9710\n",
      "Epoch 74/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1288 - accuracy: 0.9634 - val_loss: 0.1104 - val_accuracy: 0.9710\n",
      "Epoch 75/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1294 - accuracy: 0.9634 - val_loss: 0.1089 - val_accuracy: 0.9706\n",
      "Epoch 76/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1294 - accuracy: 0.9635 - val_loss: 0.1088 - val_accuracy: 0.9710\n",
      "Epoch 77/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1284 - accuracy: 0.9635 - val_loss: 0.1105 - val_accuracy: 0.9708\n",
      "Epoch 78/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1285 - accuracy: 0.9638 - val_loss: 0.1087 - val_accuracy: 0.9708\n",
      "Epoch 79/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1282 - accuracy: 0.9640 - val_loss: 0.1089 - val_accuracy: 0.9710\n",
      "Epoch 80/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1287 - accuracy: 0.9638 - val_loss: 0.1088 - val_accuracy: 0.9710\n",
      "Epoch 81/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1286 - accuracy: 0.9633 - val_loss: 0.1092 - val_accuracy: 0.9711\n",
      "Epoch 82/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1286 - accuracy: 0.9640 - val_loss: 0.1089 - val_accuracy: 0.9708\n",
      "Epoch 83/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1279 - accuracy: 0.9640 - val_loss: 0.1086 - val_accuracy: 0.9709\n",
      "Epoch 84/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1292 - accuracy: 0.9635 - val_loss: 0.1085 - val_accuracy: 0.9707\n",
      "Epoch 85/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1277 - accuracy: 0.9639 - val_loss: 0.1090 - val_accuracy: 0.9710\n",
      "Epoch 86/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1275 - accuracy: 0.9636 - val_loss: 0.1086 - val_accuracy: 0.9707\n",
      "Epoch 87/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1271 - accuracy: 0.9639 - val_loss: 0.1092 - val_accuracy: 0.9707\n",
      "Epoch 88/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1271 - accuracy: 0.9640 - val_loss: 0.1081 - val_accuracy: 0.9710\n",
      "Epoch 89/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1285 - accuracy: 0.9636 - val_loss: 0.1088 - val_accuracy: 0.9709\n",
      "Epoch 90/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1273 - accuracy: 0.9642 - val_loss: 0.1082 - val_accuracy: 0.9704\n",
      "Epoch 91/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1288 - accuracy: 0.9632 - val_loss: 0.1087 - val_accuracy: 0.9709\n",
      "Epoch 92/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1274 - accuracy: 0.9637 - val_loss: 0.1078 - val_accuracy: 0.9706\n",
      "Epoch 93/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1272 - accuracy: 0.9639 - val_loss: 0.1076 - val_accuracy: 0.9710\n",
      "Epoch 94/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1269 - accuracy: 0.9640 - val_loss: 0.1081 - val_accuracy: 0.9709\n",
      "Epoch 95/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1282 - accuracy: 0.9635 - val_loss: 0.1084 - val_accuracy: 0.9708\n",
      "Epoch 96/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1269 - accuracy: 0.9638 - val_loss: 0.1083 - val_accuracy: 0.9710\n",
      "Epoch 97/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1280 - accuracy: 0.9637 - val_loss: 0.1079 - val_accuracy: 0.9707\n",
      "Epoch 98/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1270 - accuracy: 0.9640 - val_loss: 0.1080 - val_accuracy: 0.9709\n",
      "Epoch 99/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1281 - accuracy: 0.9638 - val_loss: 0.1082 - val_accuracy: 0.9708\n",
      "Epoch 100/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1269 - accuracy: 0.9633 - val_loss: 0.1084 - val_accuracy: 0.9705\n",
      "Score for fold 1: loss of 0.10837120562791824; accuracy of 97.05357551574707%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 14s 2ms/step - loss: 0.4443 - accuracy: 0.8450 - val_loss: 0.2990 - val_accuracy: 0.9589\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.2816 - accuracy: 0.9576 - val_loss: 0.2336 - val_accuracy: 0.9625\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.2257 - accuracy: 0.9604 - val_loss: 0.1905 - val_accuracy: 0.9621\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1920 - accuracy: 0.9605 - val_loss: 0.1653 - val_accuracy: 0.9630\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1722 - accuracy: 0.9598 - val_loss: 0.1518 - val_accuracy: 0.9638\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1622 - accuracy: 0.9603 - val_loss: 0.1440 - val_accuracy: 0.9639\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1568 - accuracy: 0.9598 - val_loss: 0.1399 - val_accuracy: 0.9644\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1531 - accuracy: 0.9599 - val_loss: 0.1374 - val_accuracy: 0.9644\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1518 - accuracy: 0.9593 - val_loss: 0.1358 - val_accuracy: 0.9646\n",
      "Epoch 10/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1501 - accuracy: 0.9599 - val_loss: 0.1336 - val_accuracy: 0.9646\n",
      "Epoch 11/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1496 - accuracy: 0.9595 - val_loss: 0.1332 - val_accuracy: 0.9644\n",
      "Epoch 12/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1463 - accuracy: 0.9605 - val_loss: 0.1319 - val_accuracy: 0.9643\n",
      "Epoch 13/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1461 - accuracy: 0.9600 - val_loss: 0.1308 - val_accuracy: 0.9643\n",
      "Epoch 14/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1443 - accuracy: 0.9601 - val_loss: 0.1301 - val_accuracy: 0.9643\n",
      "Epoch 15/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1440 - accuracy: 0.9602 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 16/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1422 - accuracy: 0.9599 - val_loss: 0.1291 - val_accuracy: 0.9643\n",
      "Epoch 17/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1414 - accuracy: 0.9602 - val_loss: 0.1286 - val_accuracy: 0.9642\n",
      "Epoch 18/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1403 - accuracy: 0.9611 - val_loss: 0.1278 - val_accuracy: 0.9644\n",
      "Epoch 19/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1399 - accuracy: 0.9607 - val_loss: 0.1270 - val_accuracy: 0.9646\n",
      "Epoch 20/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1392 - accuracy: 0.9615 - val_loss: 0.1258 - val_accuracy: 0.9646\n",
      "Epoch 21/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1382 - accuracy: 0.9612 - val_loss: 0.1258 - val_accuracy: 0.9645\n",
      "Epoch 22/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1379 - accuracy: 0.9613 - val_loss: 0.1246 - val_accuracy: 0.9645\n",
      "Epoch 23/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1387 - accuracy: 0.9607 - val_loss: 0.1244 - val_accuracy: 0.9646\n",
      "Epoch 24/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1371 - accuracy: 0.9612 - val_loss: 0.1240 - val_accuracy: 0.9648\n",
      "Epoch 25/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1372 - accuracy: 0.9615 - val_loss: 0.1235 - val_accuracy: 0.9646\n",
      "Epoch 26/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1361 - accuracy: 0.9617 - val_loss: 0.1236 - val_accuracy: 0.9649\n",
      "Epoch 27/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1354 - accuracy: 0.9619 - val_loss: 0.1219 - val_accuracy: 0.9648\n",
      "Epoch 28/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1344 - accuracy: 0.9621 - val_loss: 0.1221 - val_accuracy: 0.9650\n",
      "Epoch 29/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1346 - accuracy: 0.9619 - val_loss: 0.1220 - val_accuracy: 0.9654\n",
      "Epoch 30/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1357 - accuracy: 0.9618 - val_loss: 0.1212 - val_accuracy: 0.9655\n",
      "Epoch 31/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1341 - accuracy: 0.9617 - val_loss: 0.1218 - val_accuracy: 0.9653\n",
      "Epoch 32/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1347 - accuracy: 0.9614 - val_loss: 0.1208 - val_accuracy: 0.9654\n",
      "Epoch 33/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1341 - accuracy: 0.9621 - val_loss: 0.1207 - val_accuracy: 0.9655\n",
      "Epoch 34/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1330 - accuracy: 0.9623 - val_loss: 0.1199 - val_accuracy: 0.9660\n",
      "Epoch 35/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1329 - accuracy: 0.9619 - val_loss: 0.1211 - val_accuracy: 0.9662\n",
      "Epoch 36/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1330 - accuracy: 0.9620 - val_loss: 0.1198 - val_accuracy: 0.9658\n",
      "Epoch 37/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1328 - accuracy: 0.9623 - val_loss: 0.1197 - val_accuracy: 0.9660\n",
      "Epoch 38/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1319 - accuracy: 0.9626 - val_loss: 0.1198 - val_accuracy: 0.9658\n",
      "Epoch 39/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1326 - accuracy: 0.9622 - val_loss: 0.1194 - val_accuracy: 0.9662\n",
      "Epoch 40/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1325 - accuracy: 0.9624 - val_loss: 0.1197 - val_accuracy: 0.9665\n",
      "Epoch 41/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1318 - accuracy: 0.9626 - val_loss: 0.1196 - val_accuracy: 0.9662\n",
      "Epoch 42/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1323 - accuracy: 0.9627 - val_loss: 0.1188 - val_accuracy: 0.9664\n",
      "Epoch 43/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1309 - accuracy: 0.9626 - val_loss: 0.1190 - val_accuracy: 0.9658\n",
      "Epoch 44/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1311 - accuracy: 0.9628 - val_loss: 0.1192 - val_accuracy: 0.9661\n",
      "Epoch 45/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1309 - accuracy: 0.9627 - val_loss: 0.1189 - val_accuracy: 0.9660\n",
      "Epoch 46/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1310 - accuracy: 0.9627 - val_loss: 0.1193 - val_accuracy: 0.9660\n",
      "Epoch 47/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1323 - accuracy: 0.9624 - val_loss: 0.1185 - val_accuracy: 0.9665\n",
      "Epoch 48/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1308 - accuracy: 0.9629 - val_loss: 0.1189 - val_accuracy: 0.9664\n",
      "Epoch 49/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1296 - accuracy: 0.9630 - val_loss: 0.1190 - val_accuracy: 0.9664\n",
      "Epoch 50/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1305 - accuracy: 0.9630 - val_loss: 0.1187 - val_accuracy: 0.9662\n",
      "Epoch 51/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1301 - accuracy: 0.9633 - val_loss: 0.1177 - val_accuracy: 0.9662\n",
      "Epoch 52/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1297 - accuracy: 0.9636 - val_loss: 0.1176 - val_accuracy: 0.9665\n",
      "Epoch 53/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1291 - accuracy: 0.9632 - val_loss: 0.1174 - val_accuracy: 0.9662\n",
      "Epoch 54/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1298 - accuracy: 0.9630 - val_loss: 0.1181 - val_accuracy: 0.9666\n",
      "Epoch 55/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1290 - accuracy: 0.9637 - val_loss: 0.1177 - val_accuracy: 0.9664\n",
      "Epoch 56/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1294 - accuracy: 0.9629 - val_loss: 0.1170 - val_accuracy: 0.9665\n",
      "Epoch 57/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1305 - accuracy: 0.9632 - val_loss: 0.1173 - val_accuracy: 0.9666\n",
      "Epoch 58/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1290 - accuracy: 0.9635 - val_loss: 0.1164 - val_accuracy: 0.9667\n",
      "Epoch 59/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1288 - accuracy: 0.9636 - val_loss: 0.1167 - val_accuracy: 0.9666\n",
      "Epoch 60/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1287 - accuracy: 0.9635 - val_loss: 0.1180 - val_accuracy: 0.9669\n",
      "Epoch 61/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1288 - accuracy: 0.9634 - val_loss: 0.1171 - val_accuracy: 0.9665\n",
      "Epoch 62/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1279 - accuracy: 0.9639 - val_loss: 0.1169 - val_accuracy: 0.9667\n",
      "Epoch 63/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1290 - accuracy: 0.9637 - val_loss: 0.1163 - val_accuracy: 0.9670\n",
      "Epoch 64/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1272 - accuracy: 0.9638 - val_loss: 0.1172 - val_accuracy: 0.9666\n",
      "Epoch 65/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1276 - accuracy: 0.9641 - val_loss: 0.1164 - val_accuracy: 0.9671\n",
      "Epoch 66/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1291 - accuracy: 0.9637 - val_loss: 0.1167 - val_accuracy: 0.9668\n",
      "Epoch 67/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1280 - accuracy: 0.9637 - val_loss: 0.1164 - val_accuracy: 0.9669\n",
      "Epoch 68/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1289 - accuracy: 0.9636 - val_loss: 0.1164 - val_accuracy: 0.9673\n",
      "Epoch 69/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1284 - accuracy: 0.9633 - val_loss: 0.1166 - val_accuracy: 0.9673\n",
      "Epoch 70/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1280 - accuracy: 0.9639 - val_loss: 0.1168 - val_accuracy: 0.9671\n",
      "Epoch 71/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1271 - accuracy: 0.9642 - val_loss: 0.1151 - val_accuracy: 0.9673\n",
      "Epoch 72/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1271 - accuracy: 0.9640 - val_loss: 0.1148 - val_accuracy: 0.9674\n",
      "Epoch 73/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1271 - accuracy: 0.9641 - val_loss: 0.1152 - val_accuracy: 0.9673\n",
      "Epoch 74/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1271 - accuracy: 0.9639 - val_loss: 0.1159 - val_accuracy: 0.9675\n",
      "Epoch 75/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1267 - accuracy: 0.9640 - val_loss: 0.1155 - val_accuracy: 0.9675\n",
      "Epoch 76/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1270 - accuracy: 0.9643 - val_loss: 0.1153 - val_accuracy: 0.9671\n",
      "Epoch 77/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1279 - accuracy: 0.9640 - val_loss: 0.1152 - val_accuracy: 0.9677\n",
      "Epoch 78/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1271 - accuracy: 0.9642 - val_loss: 0.1150 - val_accuracy: 0.9675\n",
      "Epoch 79/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1263 - accuracy: 0.9639 - val_loss: 0.1147 - val_accuracy: 0.9676\n",
      "Epoch 80/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1267 - accuracy: 0.9641 - val_loss: 0.1152 - val_accuracy: 0.9673\n",
      "Epoch 81/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1270 - accuracy: 0.9644 - val_loss: 0.1155 - val_accuracy: 0.9675\n",
      "Epoch 82/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1253 - accuracy: 0.9645 - val_loss: 0.1149 - val_accuracy: 0.9674\n",
      "Epoch 83/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1259 - accuracy: 0.9645 - val_loss: 0.1150 - val_accuracy: 0.9677\n",
      "Epoch 84/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1258 - accuracy: 0.9647 - val_loss: 0.1147 - val_accuracy: 0.9676\n",
      "Epoch 85/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1259 - accuracy: 0.9646 - val_loss: 0.1151 - val_accuracy: 0.9676\n",
      "Epoch 86/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1266 - accuracy: 0.9645 - val_loss: 0.1150 - val_accuracy: 0.9674\n",
      "Epoch 87/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1261 - accuracy: 0.9647 - val_loss: 0.1145 - val_accuracy: 0.9678\n",
      "Epoch 88/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1255 - accuracy: 0.9645 - val_loss: 0.1152 - val_accuracy: 0.9680\n",
      "Epoch 89/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1265 - accuracy: 0.9642 - val_loss: 0.1151 - val_accuracy: 0.9682\n",
      "Epoch 90/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1253 - accuracy: 0.9647 - val_loss: 0.1147 - val_accuracy: 0.9676\n",
      "Epoch 91/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1260 - accuracy: 0.9642 - val_loss: 0.1151 - val_accuracy: 0.9679\n",
      "Epoch 92/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1257 - accuracy: 0.9645 - val_loss: 0.1139 - val_accuracy: 0.9678\n",
      "Epoch 93/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1255 - accuracy: 0.9644 - val_loss: 0.1148 - val_accuracy: 0.9681\n",
      "Epoch 94/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1251 - accuracy: 0.9643 - val_loss: 0.1153 - val_accuracy: 0.9680\n",
      "Epoch 95/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1247 - accuracy: 0.9652 - val_loss: 0.1145 - val_accuracy: 0.9678\n",
      "Epoch 96/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1249 - accuracy: 0.9650 - val_loss: 0.1146 - val_accuracy: 0.9679\n",
      "Epoch 97/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1244 - accuracy: 0.9644 - val_loss: 0.1140 - val_accuracy: 0.9680\n",
      "Epoch 98/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1254 - accuracy: 0.9647 - val_loss: 0.1133 - val_accuracy: 0.9682\n",
      "Epoch 99/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1250 - accuracy: 0.9648 - val_loss: 0.1141 - val_accuracy: 0.9680\n",
      "Epoch 100/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1247 - accuracy: 0.9645 - val_loss: 0.1139 - val_accuracy: 0.9681\n",
      "Score for fold 2: loss of 0.11387186497449875; accuracy of 96.81029319763184%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 14s 2ms/step - loss: 0.5178 - accuracy: 0.7974 - val_loss: 0.3543 - val_accuracy: 0.9589\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.3394 - accuracy: 0.9461 - val_loss: 0.2682 - val_accuracy: 0.9647\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.2731 - accuracy: 0.9495 - val_loss: 0.2136 - val_accuracy: 0.9655\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.2300 - accuracy: 0.9502 - val_loss: 0.1786 - val_accuracy: 0.9654\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.2024 - accuracy: 0.9509 - val_loss: 0.1572 - val_accuracy: 0.9664\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1856 - accuracy: 0.9518 - val_loss: 0.1445 - val_accuracy: 0.9668\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1770 - accuracy: 0.9509 - val_loss: 0.1376 - val_accuracy: 0.9678\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1708 - accuracy: 0.9524 - val_loss: 0.1324 - val_accuracy: 0.9681\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1667 - accuracy: 0.9525 - val_loss: 0.1280 - val_accuracy: 0.9684\n",
      "Epoch 10/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1642 - accuracy: 0.9527 - val_loss: 0.1263 - val_accuracy: 0.9685\n",
      "Epoch 11/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1622 - accuracy: 0.9530 - val_loss: 0.1238 - val_accuracy: 0.9689\n",
      "Epoch 12/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1591 - accuracy: 0.9531 - val_loss: 0.1221 - val_accuracy: 0.9691\n",
      "Epoch 13/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1569 - accuracy: 0.9535 - val_loss: 0.1212 - val_accuracy: 0.9697\n",
      "Epoch 14/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1556 - accuracy: 0.9539 - val_loss: 0.1192 - val_accuracy: 0.9691\n",
      "Epoch 15/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1550 - accuracy: 0.9539 - val_loss: 0.1187 - val_accuracy: 0.9699\n",
      "Epoch 16/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1545 - accuracy: 0.9538 - val_loss: 0.1185 - val_accuracy: 0.9701\n",
      "Epoch 17/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1518 - accuracy: 0.9544 - val_loss: 0.1174 - val_accuracy: 0.9699\n",
      "Epoch 18/100\n",
      "5203/5203 [==============================] - 24s 5ms/step - loss: 0.1522 - accuracy: 0.9547 - val_loss: 0.1170 - val_accuracy: 0.9702\n",
      "Epoch 19/100\n",
      "5203/5203 [==============================] - 18s 3ms/step - loss: 0.1513 - accuracy: 0.9548 - val_loss: 0.1157 - val_accuracy: 0.9705\n",
      "Epoch 20/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1524 - accuracy: 0.9541 - val_loss: 0.1144 - val_accuracy: 0.9701\n",
      "Epoch 21/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1511 - accuracy: 0.9545 - val_loss: 0.1144 - val_accuracy: 0.9704\n",
      "Epoch 22/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1495 - accuracy: 0.9546 - val_loss: 0.1147 - val_accuracy: 0.9705\n",
      "Epoch 23/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1504 - accuracy: 0.9551 - val_loss: 0.1138 - val_accuracy: 0.9705\n",
      "Epoch 24/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1489 - accuracy: 0.9547 - val_loss: 0.1145 - val_accuracy: 0.9707\n",
      "Epoch 25/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1494 - accuracy: 0.9558 - val_loss: 0.1127 - val_accuracy: 0.9706\n",
      "Epoch 26/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1491 - accuracy: 0.9550 - val_loss: 0.1125 - val_accuracy: 0.9705\n",
      "Epoch 27/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1471 - accuracy: 0.9550 - val_loss: 0.1124 - val_accuracy: 0.9705\n",
      "Epoch 28/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1472 - accuracy: 0.9560 - val_loss: 0.1115 - val_accuracy: 0.9706\n",
      "Epoch 29/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1479 - accuracy: 0.9551 - val_loss: 0.1128 - val_accuracy: 0.9708\n",
      "Epoch 30/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1477 - accuracy: 0.9554 - val_loss: 0.1116 - val_accuracy: 0.9706\n",
      "Epoch 31/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1469 - accuracy: 0.9557 - val_loss: 0.1108 - val_accuracy: 0.9710\n",
      "Epoch 32/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1464 - accuracy: 0.9556 - val_loss: 0.1119 - val_accuracy: 0.9706\n",
      "Epoch 33/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1471 - accuracy: 0.9556 - val_loss: 0.1110 - val_accuracy: 0.9705\n",
      "Epoch 34/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1466 - accuracy: 0.9557 - val_loss: 0.1117 - val_accuracy: 0.9711\n",
      "Epoch 35/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1463 - accuracy: 0.9561 - val_loss: 0.1107 - val_accuracy: 0.9713\n",
      "Epoch 36/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1464 - accuracy: 0.9553 - val_loss: 0.1111 - val_accuracy: 0.9708\n",
      "Epoch 37/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1461 - accuracy: 0.9559 - val_loss: 0.1110 - val_accuracy: 0.9710\n",
      "Epoch 38/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1457 - accuracy: 0.9558 - val_loss: 0.1105 - val_accuracy: 0.9708\n",
      "Epoch 39/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1453 - accuracy: 0.9559 - val_loss: 0.1101 - val_accuracy: 0.9705\n",
      "Epoch 40/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1460 - accuracy: 0.9556 - val_loss: 0.1110 - val_accuracy: 0.9712\n",
      "Epoch 41/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1472 - accuracy: 0.9553 - val_loss: 0.1096 - val_accuracy: 0.9711\n",
      "Epoch 42/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1446 - accuracy: 0.9560 - val_loss: 0.1113 - val_accuracy: 0.9715\n",
      "Epoch 43/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1441 - accuracy: 0.9563 - val_loss: 0.1101 - val_accuracy: 0.9708\n",
      "Epoch 44/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1449 - accuracy: 0.9561 - val_loss: 0.1100 - val_accuracy: 0.9716\n",
      "Epoch 45/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1445 - accuracy: 0.9564 - val_loss: 0.1101 - val_accuracy: 0.9714\n",
      "Epoch 46/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1452 - accuracy: 0.9563 - val_loss: 0.1089 - val_accuracy: 0.9717\n",
      "Epoch 47/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1444 - accuracy: 0.9566 - val_loss: 0.1088 - val_accuracy: 0.9712\n",
      "Epoch 48/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1437 - accuracy: 0.9567 - val_loss: 0.1098 - val_accuracy: 0.9714\n",
      "Epoch 49/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1451 - accuracy: 0.9561 - val_loss: 0.1103 - val_accuracy: 0.9713\n",
      "Epoch 50/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1431 - accuracy: 0.9564 - val_loss: 0.1099 - val_accuracy: 0.9714\n",
      "Epoch 51/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1447 - accuracy: 0.9566 - val_loss: 0.1091 - val_accuracy: 0.9712\n",
      "Epoch 52/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1426 - accuracy: 0.9570 - val_loss: 0.1108 - val_accuracy: 0.9718\n",
      "Epoch 53/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1444 - accuracy: 0.9564 - val_loss: 0.1097 - val_accuracy: 0.9716\n",
      "Epoch 54/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1432 - accuracy: 0.9565 - val_loss: 0.1096 - val_accuracy: 0.9716\n",
      "Epoch 55/100\n",
      "5203/5203 [==============================] - 17s 3ms/step - loss: 0.1447 - accuracy: 0.9563 - val_loss: 0.1098 - val_accuracy: 0.9717\n",
      "Epoch 56/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1433 - accuracy: 0.9567 - val_loss: 0.1090 - val_accuracy: 0.9713\n",
      "Epoch 57/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1426 - accuracy: 0.9568 - val_loss: 0.1103 - val_accuracy: 0.9722\n",
      "Epoch 58/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1444 - accuracy: 0.9563 - val_loss: 0.1098 - val_accuracy: 0.9720\n",
      "Epoch 59/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1427 - accuracy: 0.9571 - val_loss: 0.1092 - val_accuracy: 0.9716\n",
      "Epoch 60/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1426 - accuracy: 0.9566 - val_loss: 0.1089 - val_accuracy: 0.9713\n",
      "Epoch 61/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1437 - accuracy: 0.9568 - val_loss: 0.1085 - val_accuracy: 0.9719\n",
      "Epoch 62/100\n",
      "5203/5203 [==============================] - 17s 3ms/step - loss: 0.1428 - accuracy: 0.9571 - val_loss: 0.1085 - val_accuracy: 0.9714\n",
      "Epoch 63/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1424 - accuracy: 0.9575 - val_loss: 0.1095 - val_accuracy: 0.9716\n",
      "Epoch 64/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1435 - accuracy: 0.9570 - val_loss: 0.1083 - val_accuracy: 0.9714\n",
      "Epoch 65/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1424 - accuracy: 0.9566 - val_loss: 0.1094 - val_accuracy: 0.9719\n",
      "Epoch 66/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1441 - accuracy: 0.9571 - val_loss: 0.1082 - val_accuracy: 0.9718\n",
      "Epoch 67/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1421 - accuracy: 0.9569 - val_loss: 0.1094 - val_accuracy: 0.9716\n",
      "Epoch 68/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1430 - accuracy: 0.9570 - val_loss: 0.1097 - val_accuracy: 0.9715\n",
      "Epoch 69/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1424 - accuracy: 0.9568 - val_loss: 0.1083 - val_accuracy: 0.9716\n",
      "Epoch 70/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1427 - accuracy: 0.9569 - val_loss: 0.1088 - val_accuracy: 0.9716\n",
      "Epoch 71/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1416 - accuracy: 0.9574 - val_loss: 0.1090 - val_accuracy: 0.9717\n",
      "Epoch 72/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1414 - accuracy: 0.9579 - val_loss: 0.1095 - val_accuracy: 0.9714\n",
      "Epoch 73/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1423 - accuracy: 0.9578 - val_loss: 0.1094 - val_accuracy: 0.9720\n",
      "Epoch 74/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1417 - accuracy: 0.9573 - val_loss: 0.1088 - val_accuracy: 0.9719\n",
      "Epoch 75/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1421 - accuracy: 0.9576 - val_loss: 0.1085 - val_accuracy: 0.9718\n",
      "Epoch 76/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1427 - accuracy: 0.9577 - val_loss: 0.1087 - val_accuracy: 0.9718\n",
      "Epoch 77/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1415 - accuracy: 0.9572 - val_loss: 0.1078 - val_accuracy: 0.9716\n",
      "Epoch 78/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1428 - accuracy: 0.9567 - val_loss: 0.1080 - val_accuracy: 0.9719\n",
      "Epoch 79/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1419 - accuracy: 0.9573 - val_loss: 0.1086 - val_accuracy: 0.9718\n",
      "Epoch 80/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1428 - accuracy: 0.9574 - val_loss: 0.1087 - val_accuracy: 0.9720\n",
      "Epoch 81/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1410 - accuracy: 0.9576 - val_loss: 0.1082 - val_accuracy: 0.9718\n",
      "Epoch 82/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1426 - accuracy: 0.9571 - val_loss: 0.1076 - val_accuracy: 0.9718\n",
      "Epoch 83/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1413 - accuracy: 0.9575 - val_loss: 0.1081 - val_accuracy: 0.9719\n",
      "Epoch 84/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1417 - accuracy: 0.9577 - val_loss: 0.1072 - val_accuracy: 0.9718\n",
      "Epoch 85/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1420 - accuracy: 0.9570 - val_loss: 0.1081 - val_accuracy: 0.9722\n",
      "Epoch 86/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1419 - accuracy: 0.9575 - val_loss: 0.1081 - val_accuracy: 0.9717\n",
      "Epoch 87/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1421 - accuracy: 0.9572 - val_loss: 0.1078 - val_accuracy: 0.9719\n",
      "Epoch 88/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1425 - accuracy: 0.9573 - val_loss: 0.1086 - val_accuracy: 0.9717\n",
      "Epoch 89/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1426 - accuracy: 0.9573 - val_loss: 0.1081 - val_accuracy: 0.9719\n",
      "Epoch 90/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1409 - accuracy: 0.9579 - val_loss: 0.1090 - val_accuracy: 0.9716\n",
      "Epoch 91/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1416 - accuracy: 0.9576 - val_loss: 0.1081 - val_accuracy: 0.9717\n",
      "Epoch 92/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1403 - accuracy: 0.9578 - val_loss: 0.1081 - val_accuracy: 0.9714\n",
      "Epoch 93/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1403 - accuracy: 0.9575 - val_loss: 0.1073 - val_accuracy: 0.9717\n",
      "Epoch 94/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1406 - accuracy: 0.9579 - val_loss: 0.1073 - val_accuracy: 0.9721\n",
      "Epoch 95/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1405 - accuracy: 0.9580 - val_loss: 0.1076 - val_accuracy: 0.9714\n",
      "Epoch 96/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1412 - accuracy: 0.9578 - val_loss: 0.1072 - val_accuracy: 0.9718\n",
      "Epoch 97/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1409 - accuracy: 0.9575 - val_loss: 0.1070 - val_accuracy: 0.9716\n",
      "Epoch 98/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1410 - accuracy: 0.9574 - val_loss: 0.1067 - val_accuracy: 0.9719\n",
      "Epoch 99/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1423 - accuracy: 0.9574 - val_loss: 0.1072 - val_accuracy: 0.9712\n",
      "Epoch 100/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1394 - accuracy: 0.9582 - val_loss: 0.1069 - val_accuracy: 0.9719\n",
      "Score for fold 3: loss of 0.10690888017416; accuracy of 97.18873500823975%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 14s 2ms/step - loss: 0.5059 - accuracy: 0.8255 - val_loss: 0.3501 - val_accuracy: 0.9566\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.3573 - accuracy: 0.9114 - val_loss: 0.2672 - val_accuracy: 0.9645\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.2728 - accuracy: 0.9461 - val_loss: 0.2123 - val_accuracy: 0.9660\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.2260 - accuracy: 0.9514 - val_loss: 0.1772 - val_accuracy: 0.9657\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1985 - accuracy: 0.9522 - val_loss: 0.1545 - val_accuracy: 0.9673\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1834 - accuracy: 0.9524 - val_loss: 0.1411 - val_accuracy: 0.9680\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1742 - accuracy: 0.9518 - val_loss: 0.1326 - val_accuracy: 0.9690\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1676 - accuracy: 0.9528 - val_loss: 0.1273 - val_accuracy: 0.9688\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1652 - accuracy: 0.9518 - val_loss: 0.1237 - val_accuracy: 0.9704\n",
      "Epoch 10/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1632 - accuracy: 0.9518 - val_loss: 0.1201 - val_accuracy: 0.9700\n",
      "Epoch 11/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1610 - accuracy: 0.9521 - val_loss: 0.1175 - val_accuracy: 0.9700\n",
      "Epoch 12/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1587 - accuracy: 0.9527 - val_loss: 0.1158 - val_accuracy: 0.9706\n",
      "Epoch 13/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1569 - accuracy: 0.9535 - val_loss: 0.1145 - val_accuracy: 0.9708\n",
      "Epoch 14/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1562 - accuracy: 0.9530 - val_loss: 0.1131 - val_accuracy: 0.9705\n",
      "Epoch 15/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1533 - accuracy: 0.9537 - val_loss: 0.1122 - val_accuracy: 0.9710\n",
      "Epoch 16/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1532 - accuracy: 0.9536 - val_loss: 0.1117 - val_accuracy: 0.9713\n",
      "Epoch 17/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1532 - accuracy: 0.9534 - val_loss: 0.1110 - val_accuracy: 0.9715\n",
      "Epoch 18/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1516 - accuracy: 0.9542 - val_loss: 0.1097 - val_accuracy: 0.9715\n",
      "Epoch 19/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1500 - accuracy: 0.9541 - val_loss: 0.1094 - val_accuracy: 0.9716\n",
      "Epoch 20/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1507 - accuracy: 0.9540 - val_loss: 0.1095 - val_accuracy: 0.9717\n",
      "Epoch 21/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1484 - accuracy: 0.9547 - val_loss: 0.1083 - val_accuracy: 0.9717\n",
      "Epoch 22/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1489 - accuracy: 0.9543 - val_loss: 0.1076 - val_accuracy: 0.9719\n",
      "Epoch 23/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1485 - accuracy: 0.9552 - val_loss: 0.1076 - val_accuracy: 0.9717\n",
      "Epoch 24/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1464 - accuracy: 0.9557 - val_loss: 0.1065 - val_accuracy: 0.9718\n",
      "Epoch 25/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1455 - accuracy: 0.9556 - val_loss: 0.1067 - val_accuracy: 0.9718\n",
      "Epoch 26/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1454 - accuracy: 0.9558 - val_loss: 0.1056 - val_accuracy: 0.9717\n",
      "Epoch 27/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1448 - accuracy: 0.9561 - val_loss: 0.1057 - val_accuracy: 0.9717\n",
      "Epoch 28/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1436 - accuracy: 0.9564 - val_loss: 0.1049 - val_accuracy: 0.9718\n",
      "Epoch 29/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1437 - accuracy: 0.9565 - val_loss: 0.1050 - val_accuracy: 0.9718\n",
      "Epoch 30/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1432 - accuracy: 0.9561 - val_loss: 0.1052 - val_accuracy: 0.9722\n",
      "Epoch 31/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1438 - accuracy: 0.9560 - val_loss: 0.1047 - val_accuracy: 0.9722\n",
      "Epoch 32/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1429 - accuracy: 0.9564 - val_loss: 0.1046 - val_accuracy: 0.9723\n",
      "Epoch 33/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1425 - accuracy: 0.9563 - val_loss: 0.1048 - val_accuracy: 0.9724\n",
      "Epoch 34/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1431 - accuracy: 0.9559 - val_loss: 0.1039 - val_accuracy: 0.9725\n",
      "Epoch 35/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1417 - accuracy: 0.9564 - val_loss: 0.1041 - val_accuracy: 0.9726\n",
      "Epoch 36/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1428 - accuracy: 0.9560 - val_loss: 0.1042 - val_accuracy: 0.9726\n",
      "Epoch 37/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1413 - accuracy: 0.9567 - val_loss: 0.1042 - val_accuracy: 0.9723\n",
      "Epoch 38/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1414 - accuracy: 0.9565 - val_loss: 0.1036 - val_accuracy: 0.9722\n",
      "Epoch 39/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1408 - accuracy: 0.9566 - val_loss: 0.1031 - val_accuracy: 0.9722\n",
      "Epoch 40/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1421 - accuracy: 0.9565 - val_loss: 0.1032 - val_accuracy: 0.9723\n",
      "Epoch 41/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1411 - accuracy: 0.9566 - val_loss: 0.1033 - val_accuracy: 0.9723\n",
      "Epoch 42/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1409 - accuracy: 0.9569 - val_loss: 0.1033 - val_accuracy: 0.9724\n",
      "Epoch 43/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1408 - accuracy: 0.9569 - val_loss: 0.1026 - val_accuracy: 0.9725\n",
      "Epoch 44/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1411 - accuracy: 0.9564 - val_loss: 0.1025 - val_accuracy: 0.9723\n",
      "Epoch 45/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1412 - accuracy: 0.9570 - val_loss: 0.1027 - val_accuracy: 0.9724\n",
      "Epoch 46/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1396 - accuracy: 0.9570 - val_loss: 0.1023 - val_accuracy: 0.9726\n",
      "Epoch 47/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1399 - accuracy: 0.9570 - val_loss: 0.1028 - val_accuracy: 0.9724\n",
      "Epoch 48/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1405 - accuracy: 0.9569 - val_loss: 0.1023 - val_accuracy: 0.9724\n",
      "Epoch 49/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1393 - accuracy: 0.9570 - val_loss: 0.1023 - val_accuracy: 0.9728\n",
      "Epoch 50/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1398 - accuracy: 0.9573 - val_loss: 0.1024 - val_accuracy: 0.9723\n",
      "Epoch 51/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1395 - accuracy: 0.9569 - val_loss: 0.1025 - val_accuracy: 0.9724\n",
      "Epoch 52/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1386 - accuracy: 0.9572 - val_loss: 0.1020 - val_accuracy: 0.9726\n",
      "Epoch 53/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1392 - accuracy: 0.9574 - val_loss: 0.1021 - val_accuracy: 0.9725\n",
      "Epoch 54/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1389 - accuracy: 0.9571 - val_loss: 0.1017 - val_accuracy: 0.9724\n",
      "Epoch 55/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1399 - accuracy: 0.9567 - val_loss: 0.1018 - val_accuracy: 0.9725\n",
      "Epoch 56/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1393 - accuracy: 0.9570 - val_loss: 0.1022 - val_accuracy: 0.9726\n",
      "Epoch 57/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1394 - accuracy: 0.9574 - val_loss: 0.1018 - val_accuracy: 0.9726\n",
      "Epoch 58/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1404 - accuracy: 0.9570 - val_loss: 0.1013 - val_accuracy: 0.9725\n",
      "Epoch 59/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1387 - accuracy: 0.9570 - val_loss: 0.1022 - val_accuracy: 0.9726\n",
      "Epoch 60/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1403 - accuracy: 0.9568 - val_loss: 0.1015 - val_accuracy: 0.9724\n",
      "Epoch 61/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1393 - accuracy: 0.9572 - val_loss: 0.1014 - val_accuracy: 0.9725\n",
      "Epoch 62/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1395 - accuracy: 0.9572 - val_loss: 0.1015 - val_accuracy: 0.9728\n",
      "Epoch 63/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1401 - accuracy: 0.9572 - val_loss: 0.1012 - val_accuracy: 0.9728\n",
      "Epoch 64/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1385 - accuracy: 0.9578 - val_loss: 0.1014 - val_accuracy: 0.9730\n",
      "Epoch 65/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1384 - accuracy: 0.9578 - val_loss: 0.1015 - val_accuracy: 0.9725\n",
      "Epoch 66/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1386 - accuracy: 0.9577 - val_loss: 0.1010 - val_accuracy: 0.9730\n",
      "Epoch 67/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1387 - accuracy: 0.9570 - val_loss: 0.1014 - val_accuracy: 0.9728\n",
      "Epoch 68/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1397 - accuracy: 0.9573 - val_loss: 0.1011 - val_accuracy: 0.9730\n",
      "Epoch 69/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1380 - accuracy: 0.9578 - val_loss: 0.1009 - val_accuracy: 0.9730\n",
      "Epoch 70/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1384 - accuracy: 0.9575 - val_loss: 0.1017 - val_accuracy: 0.9728\n",
      "Epoch 71/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1390 - accuracy: 0.9577 - val_loss: 0.1010 - val_accuracy: 0.9729\n",
      "Epoch 72/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1387 - accuracy: 0.9575 - val_loss: 0.1016 - val_accuracy: 0.9726\n",
      "Epoch 73/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1383 - accuracy: 0.9575 - val_loss: 0.1007 - val_accuracy: 0.9729\n",
      "Epoch 74/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1391 - accuracy: 0.9574 - val_loss: 0.1005 - val_accuracy: 0.9730\n",
      "Epoch 75/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1379 - accuracy: 0.9582 - val_loss: 0.1005 - val_accuracy: 0.9733\n",
      "Epoch 76/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1376 - accuracy: 0.9581 - val_loss: 0.1002 - val_accuracy: 0.9732\n",
      "Epoch 77/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1385 - accuracy: 0.9574 - val_loss: 0.1009 - val_accuracy: 0.9731\n",
      "Epoch 78/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1393 - accuracy: 0.9578 - val_loss: 0.1004 - val_accuracy: 0.9732\n",
      "Epoch 79/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1388 - accuracy: 0.9581 - val_loss: 0.1003 - val_accuracy: 0.9732\n",
      "Epoch 80/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1377 - accuracy: 0.9582 - val_loss: 0.1006 - val_accuracy: 0.9733\n",
      "Epoch 81/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1369 - accuracy: 0.9582 - val_loss: 0.1007 - val_accuracy: 0.9732\n",
      "Epoch 82/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1377 - accuracy: 0.9578 - val_loss: 0.0997 - val_accuracy: 0.9731\n",
      "Epoch 83/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1367 - accuracy: 0.9581 - val_loss: 0.1003 - val_accuracy: 0.9733\n",
      "Epoch 84/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1376 - accuracy: 0.9588 - val_loss: 0.0997 - val_accuracy: 0.9734\n",
      "Epoch 85/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1372 - accuracy: 0.9582 - val_loss: 0.1001 - val_accuracy: 0.9733\n",
      "Epoch 86/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1366 - accuracy: 0.9593 - val_loss: 0.1001 - val_accuracy: 0.9731\n",
      "Epoch 87/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1369 - accuracy: 0.9588 - val_loss: 0.0997 - val_accuracy: 0.9731\n",
      "Epoch 88/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1375 - accuracy: 0.9584 - val_loss: 0.0999 - val_accuracy: 0.9735\n",
      "Epoch 89/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1389 - accuracy: 0.9579 - val_loss: 0.0998 - val_accuracy: 0.9733\n",
      "Epoch 90/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1368 - accuracy: 0.9586 - val_loss: 0.1001 - val_accuracy: 0.9733\n",
      "Epoch 91/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1376 - accuracy: 0.9580 - val_loss: 0.1005 - val_accuracy: 0.9728\n",
      "Epoch 92/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1373 - accuracy: 0.9587 - val_loss: 0.0995 - val_accuracy: 0.9735\n",
      "Epoch 93/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1375 - accuracy: 0.9585 - val_loss: 0.0999 - val_accuracy: 0.9730\n",
      "Epoch 94/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1384 - accuracy: 0.9578 - val_loss: 0.0994 - val_accuracy: 0.9735\n",
      "Epoch 95/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1385 - accuracy: 0.9583 - val_loss: 0.0998 - val_accuracy: 0.9733\n",
      "Epoch 96/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1378 - accuracy: 0.9585 - val_loss: 0.0997 - val_accuracy: 0.9733\n",
      "Epoch 97/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1373 - accuracy: 0.9578 - val_loss: 0.1000 - val_accuracy: 0.9731\n",
      "Epoch 98/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1369 - accuracy: 0.9583 - val_loss: 0.1002 - val_accuracy: 0.9733\n",
      "Epoch 99/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1379 - accuracy: 0.9585 - val_loss: 0.0996 - val_accuracy: 0.9733\n",
      "Epoch 100/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1368 - accuracy: 0.9586 - val_loss: 0.0997 - val_accuracy: 0.9734\n",
      "Score for fold 4: loss of 0.09965423494577408; accuracy of 97.34010696411133%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.5613 - accuracy: 0.7161 - val_loss: 0.3024 - val_accuracy: 0.9555\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.2347 - accuracy: 0.9251 - val_loss: 0.1321 - val_accuracy: 0.9605\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1732 - accuracy: 0.9543 - val_loss: 0.1245 - val_accuracy: 0.9627\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1616 - accuracy: 0.9564 - val_loss: 0.1219 - val_accuracy: 0.9631\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1567 - accuracy: 0.9572 - val_loss: 0.1203 - val_accuracy: 0.9640\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1532 - accuracy: 0.9582 - val_loss: 0.1193 - val_accuracy: 0.9644\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1503 - accuracy: 0.9586 - val_loss: 0.1182 - val_accuracy: 0.9652\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1479 - accuracy: 0.9589 - val_loss: 0.1175 - val_accuracy: 0.9652\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1479 - accuracy: 0.9590 - val_loss: 0.1166 - val_accuracy: 0.9660\n",
      "Epoch 10/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1467 - accuracy: 0.9589 - val_loss: 0.1164 - val_accuracy: 0.9665\n",
      "Epoch 11/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1446 - accuracy: 0.9597 - val_loss: 0.1158 - val_accuracy: 0.9667\n",
      "Epoch 12/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1454 - accuracy: 0.9592 - val_loss: 0.1155 - val_accuracy: 0.9668\n",
      "Epoch 13/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1440 - accuracy: 0.9596 - val_loss: 0.1148 - val_accuracy: 0.9673\n",
      "Epoch 14/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1440 - accuracy: 0.9601 - val_loss: 0.1149 - val_accuracy: 0.9672\n",
      "Epoch 15/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1418 - accuracy: 0.9603 - val_loss: 0.1141 - val_accuracy: 0.9677\n",
      "Epoch 16/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1410 - accuracy: 0.9609 - val_loss: 0.1133 - val_accuracy: 0.9682\n",
      "Epoch 17/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1410 - accuracy: 0.9605 - val_loss: 0.1138 - val_accuracy: 0.9679\n",
      "Epoch 18/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1408 - accuracy: 0.9605 - val_loss: 0.1127 - val_accuracy: 0.9682\n",
      "Epoch 19/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1405 - accuracy: 0.9604 - val_loss: 0.1124 - val_accuracy: 0.9683\n",
      "Epoch 20/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1398 - accuracy: 0.9611 - val_loss: 0.1123 - val_accuracy: 0.9683\n",
      "Epoch 21/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1395 - accuracy: 0.9610 - val_loss: 0.1117 - val_accuracy: 0.9688\n",
      "Epoch 22/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1389 - accuracy: 0.9612 - val_loss: 0.1116 - val_accuracy: 0.9684\n",
      "Epoch 23/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1377 - accuracy: 0.9610 - val_loss: 0.1111 - val_accuracy: 0.9688\n",
      "Epoch 24/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1376 - accuracy: 0.9611 - val_loss: 0.1105 - val_accuracy: 0.9684\n",
      "Epoch 25/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1380 - accuracy: 0.9613 - val_loss: 0.1105 - val_accuracy: 0.9691\n",
      "Epoch 26/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1384 - accuracy: 0.9610 - val_loss: 0.1106 - val_accuracy: 0.9688\n",
      "Epoch 27/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1370 - accuracy: 0.9615 - val_loss: 0.1101 - val_accuracy: 0.9689\n",
      "Epoch 28/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1355 - accuracy: 0.9620 - val_loss: 0.1100 - val_accuracy: 0.9689\n",
      "Epoch 29/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1365 - accuracy: 0.9616 - val_loss: 0.1099 - val_accuracy: 0.9692\n",
      "Epoch 30/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1368 - accuracy: 0.9616 - val_loss: 0.1093 - val_accuracy: 0.9688\n",
      "Epoch 31/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1363 - accuracy: 0.9619 - val_loss: 0.1094 - val_accuracy: 0.9692\n",
      "Epoch 32/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1362 - accuracy: 0.9620 - val_loss: 0.1088 - val_accuracy: 0.9694\n",
      "Epoch 33/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1357 - accuracy: 0.9619 - val_loss: 0.1081 - val_accuracy: 0.9695\n",
      "Epoch 34/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1352 - accuracy: 0.9620 - val_loss: 0.1088 - val_accuracy: 0.9693\n",
      "Epoch 35/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1348 - accuracy: 0.9623 - val_loss: 0.1084 - val_accuracy: 0.9697\n",
      "Epoch 36/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1355 - accuracy: 0.9621 - val_loss: 0.1085 - val_accuracy: 0.9693\n",
      "Epoch 37/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1351 - accuracy: 0.9621 - val_loss: 0.1078 - val_accuracy: 0.9691\n",
      "Epoch 38/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1337 - accuracy: 0.9623 - val_loss: 0.1081 - val_accuracy: 0.9696\n",
      "Epoch 39/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1350 - accuracy: 0.9620 - val_loss: 0.1080 - val_accuracy: 0.9697\n",
      "Epoch 40/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1343 - accuracy: 0.9625 - val_loss: 0.1080 - val_accuracy: 0.9697\n",
      "Epoch 41/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1338 - accuracy: 0.9624 - val_loss: 0.1078 - val_accuracy: 0.9696\n",
      "Epoch 42/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1351 - accuracy: 0.9623 - val_loss: 0.1074 - val_accuracy: 0.9697\n",
      "Epoch 43/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1340 - accuracy: 0.9625 - val_loss: 0.1074 - val_accuracy: 0.9700\n",
      "Epoch 44/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1338 - accuracy: 0.9622 - val_loss: 0.1071 - val_accuracy: 0.9695\n",
      "Epoch 45/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1342 - accuracy: 0.9618 - val_loss: 0.1073 - val_accuracy: 0.9697\n",
      "Epoch 46/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1329 - accuracy: 0.9626 - val_loss: 0.1069 - val_accuracy: 0.9695\n",
      "Epoch 47/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1335 - accuracy: 0.9625 - val_loss: 0.1073 - val_accuracy: 0.9698\n",
      "Epoch 48/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1327 - accuracy: 0.9629 - val_loss: 0.1066 - val_accuracy: 0.9698\n",
      "Epoch 49/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1330 - accuracy: 0.9628 - val_loss: 0.1067 - val_accuracy: 0.9701\n",
      "Epoch 50/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1325 - accuracy: 0.9625 - val_loss: 0.1070 - val_accuracy: 0.9698\n",
      "Epoch 51/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1322 - accuracy: 0.9630 - val_loss: 0.1065 - val_accuracy: 0.9698\n",
      "Epoch 52/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1333 - accuracy: 0.9624 - val_loss: 0.1064 - val_accuracy: 0.9700\n",
      "Epoch 53/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1331 - accuracy: 0.9627 - val_loss: 0.1066 - val_accuracy: 0.9695\n",
      "Epoch 54/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1330 - accuracy: 0.9624 - val_loss: 0.1062 - val_accuracy: 0.9703\n",
      "Epoch 55/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1327 - accuracy: 0.9627 - val_loss: 0.1064 - val_accuracy: 0.9702\n",
      "Epoch 56/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1327 - accuracy: 0.9626 - val_loss: 0.1064 - val_accuracy: 0.9699\n",
      "Epoch 57/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1327 - accuracy: 0.9630 - val_loss: 0.1064 - val_accuracy: 0.9698\n",
      "Epoch 58/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1323 - accuracy: 0.9626 - val_loss: 0.1061 - val_accuracy: 0.9702\n",
      "Epoch 59/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1316 - accuracy: 0.9629 - val_loss: 0.1058 - val_accuracy: 0.9704\n",
      "Epoch 60/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1326 - accuracy: 0.9628 - val_loss: 0.1064 - val_accuracy: 0.9699\n",
      "Epoch 61/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1328 - accuracy: 0.9628 - val_loss: 0.1062 - val_accuracy: 0.9697\n",
      "Epoch 62/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1318 - accuracy: 0.9630 - val_loss: 0.1067 - val_accuracy: 0.9700\n",
      "Epoch 63/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1320 - accuracy: 0.9630 - val_loss: 0.1063 - val_accuracy: 0.9697\n",
      "Epoch 64/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1306 - accuracy: 0.9634 - val_loss: 0.1057 - val_accuracy: 0.9703\n",
      "Epoch 65/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1312 - accuracy: 0.9633 - val_loss: 0.1058 - val_accuracy: 0.9703\n",
      "Epoch 66/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1309 - accuracy: 0.9632 - val_loss: 0.1053 - val_accuracy: 0.9706\n",
      "Epoch 67/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1319 - accuracy: 0.9627 - val_loss: 0.1053 - val_accuracy: 0.9704\n",
      "Epoch 68/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1312 - accuracy: 0.9633 - val_loss: 0.1055 - val_accuracy: 0.9703\n",
      "Epoch 69/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1312 - accuracy: 0.9631 - val_loss: 0.1053 - val_accuracy: 0.9704\n",
      "Epoch 70/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1307 - accuracy: 0.9630 - val_loss: 0.1053 - val_accuracy: 0.9700\n",
      "Epoch 71/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1309 - accuracy: 0.9632 - val_loss: 0.1052 - val_accuracy: 0.9705\n",
      "Epoch 72/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1314 - accuracy: 0.9632 - val_loss: 0.1054 - val_accuracy: 0.9700\n",
      "Epoch 73/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1314 - accuracy: 0.9629 - val_loss: 0.1064 - val_accuracy: 0.9695\n",
      "Epoch 74/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1317 - accuracy: 0.9632 - val_loss: 0.1055 - val_accuracy: 0.9701\n",
      "Epoch 75/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1313 - accuracy: 0.9631 - val_loss: 0.1047 - val_accuracy: 0.9705\n",
      "Epoch 76/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1307 - accuracy: 0.9632 - val_loss: 0.1054 - val_accuracy: 0.9698\n",
      "Epoch 77/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1317 - accuracy: 0.9626 - val_loss: 0.1045 - val_accuracy: 0.9708\n",
      "Epoch 78/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1315 - accuracy: 0.9631 - val_loss: 0.1051 - val_accuracy: 0.9704\n",
      "Epoch 79/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1308 - accuracy: 0.9635 - val_loss: 0.1052 - val_accuracy: 0.9704\n",
      "Epoch 80/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1308 - accuracy: 0.9634 - val_loss: 0.1047 - val_accuracy: 0.9706\n",
      "Epoch 81/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1310 - accuracy: 0.9633 - val_loss: 0.1046 - val_accuracy: 0.9705\n",
      "Epoch 82/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1311 - accuracy: 0.9636 - val_loss: 0.1048 - val_accuracy: 0.9707\n",
      "Epoch 83/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1309 - accuracy: 0.9632 - val_loss: 0.1049 - val_accuracy: 0.9700\n",
      "Epoch 84/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1316 - accuracy: 0.9629 - val_loss: 0.1045 - val_accuracy: 0.9706\n",
      "Epoch 85/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1301 - accuracy: 0.9635 - val_loss: 0.1044 - val_accuracy: 0.9703\n",
      "Epoch 86/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1311 - accuracy: 0.9634 - val_loss: 0.1039 - val_accuracy: 0.9708\n",
      "Epoch 87/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1307 - accuracy: 0.9631 - val_loss: 0.1043 - val_accuracy: 0.9706\n",
      "Epoch 88/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1305 - accuracy: 0.9635 - val_loss: 0.1043 - val_accuracy: 0.9706\n",
      "Epoch 89/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1305 - accuracy: 0.9632 - val_loss: 0.1040 - val_accuracy: 0.9708\n",
      "Epoch 90/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1307 - accuracy: 0.9635 - val_loss: 0.1044 - val_accuracy: 0.9709\n",
      "Epoch 91/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1308 - accuracy: 0.9632 - val_loss: 0.1039 - val_accuracy: 0.9706\n",
      "Epoch 92/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1309 - accuracy: 0.9632 - val_loss: 0.1043 - val_accuracy: 0.9708\n",
      "Epoch 93/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1302 - accuracy: 0.9634 - val_loss: 0.1044 - val_accuracy: 0.9702\n",
      "Epoch 94/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1306 - accuracy: 0.9634 - val_loss: 0.1036 - val_accuracy: 0.9708\n",
      "Epoch 95/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1300 - accuracy: 0.9635 - val_loss: 0.1039 - val_accuracy: 0.9709\n",
      "Epoch 96/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1305 - accuracy: 0.9632 - val_loss: 0.1041 - val_accuracy: 0.9708\n",
      "Epoch 97/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1303 - accuracy: 0.9635 - val_loss: 0.1033 - val_accuracy: 0.9710\n",
      "Epoch 98/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1310 - accuracy: 0.9633 - val_loss: 0.1042 - val_accuracy: 0.9709\n",
      "Epoch 99/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1296 - accuracy: 0.9635 - val_loss: 0.1035 - val_accuracy: 0.9706\n",
      "Epoch 100/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1299 - accuracy: 0.9634 - val_loss: 0.1044 - val_accuracy: 0.9703\n",
      "Score for fold 5: loss of 0.10443750768899918; accuracy of 97.02638387680054%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.4721 - accuracy: 0.8599 - val_loss: 0.3370 - val_accuracy: 0.9613\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.3211 - accuracy: 0.9533 - val_loss: 0.2591 - val_accuracy: 0.9649\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.2578 - accuracy: 0.9547 - val_loss: 0.2086 - val_accuracy: 0.9658\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.2196 - accuracy: 0.9540 - val_loss: 0.1756 - val_accuracy: 0.9656\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1943 - accuracy: 0.9538 - val_loss: 0.1546 - val_accuracy: 0.9665\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1828 - accuracy: 0.9530 - val_loss: 0.1434 - val_accuracy: 0.9677\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1719 - accuracy: 0.9528 - val_loss: 0.1354 - val_accuracy: 0.9678\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1672 - accuracy: 0.9529 - val_loss: 0.1308 - val_accuracy: 0.9682\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1641 - accuracy: 0.9531 - val_loss: 0.1269 - val_accuracy: 0.9685\n",
      "Epoch 10/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1607 - accuracy: 0.9537 - val_loss: 0.1250 - val_accuracy: 0.9691\n",
      "Epoch 11/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1583 - accuracy: 0.9544 - val_loss: 0.1221 - val_accuracy: 0.9695\n",
      "Epoch 12/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1557 - accuracy: 0.9541 - val_loss: 0.1206 - val_accuracy: 0.9699\n",
      "Epoch 13/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1549 - accuracy: 0.9543 - val_loss: 0.1195 - val_accuracy: 0.9699\n",
      "Epoch 14/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1521 - accuracy: 0.9553 - val_loss: 0.1178 - val_accuracy: 0.9694\n",
      "Epoch 15/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1509 - accuracy: 0.9552 - val_loss: 0.1177 - val_accuracy: 0.9698\n",
      "Epoch 16/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1503 - accuracy: 0.9559 - val_loss: 0.1161 - val_accuracy: 0.9700\n",
      "Epoch 17/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1479 - accuracy: 0.9558 - val_loss: 0.1156 - val_accuracy: 0.9695\n",
      "Epoch 18/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1492 - accuracy: 0.9559 - val_loss: 0.1148 - val_accuracy: 0.9700\n",
      "Epoch 19/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1479 - accuracy: 0.9564 - val_loss: 0.1148 - val_accuracy: 0.9702\n",
      "Epoch 20/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1472 - accuracy: 0.9564 - val_loss: 0.1151 - val_accuracy: 0.9702\n",
      "Epoch 21/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1458 - accuracy: 0.9567 - val_loss: 0.1140 - val_accuracy: 0.9697\n",
      "Epoch 22/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1457 - accuracy: 0.9570 - val_loss: 0.1145 - val_accuracy: 0.9696\n",
      "Epoch 23/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1452 - accuracy: 0.9575 - val_loss: 0.1130 - val_accuracy: 0.9705\n",
      "Epoch 24/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1452 - accuracy: 0.9572 - val_loss: 0.1125 - val_accuracy: 0.9707\n",
      "Epoch 25/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1437 - accuracy: 0.9575 - val_loss: 0.1127 - val_accuracy: 0.9711\n",
      "Epoch 26/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1448 - accuracy: 0.9575 - val_loss: 0.1117 - val_accuracy: 0.9703\n",
      "Epoch 27/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1435 - accuracy: 0.9579 - val_loss: 0.1116 - val_accuracy: 0.9709\n",
      "Epoch 28/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1438 - accuracy: 0.9573 - val_loss: 0.1116 - val_accuracy: 0.9707\n",
      "Epoch 29/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1435 - accuracy: 0.9575 - val_loss: 0.1122 - val_accuracy: 0.9702\n",
      "Epoch 30/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1436 - accuracy: 0.9580 - val_loss: 0.1120 - val_accuracy: 0.9709\n",
      "Epoch 31/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1425 - accuracy: 0.9581 - val_loss: 0.1110 - val_accuracy: 0.9705\n",
      "Epoch 32/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1412 - accuracy: 0.9583 - val_loss: 0.1109 - val_accuracy: 0.9705\n",
      "Epoch 33/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1436 - accuracy: 0.9576 - val_loss: 0.1107 - val_accuracy: 0.9706\n",
      "Epoch 34/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1429 - accuracy: 0.9577 - val_loss: 0.1107 - val_accuracy: 0.9705\n",
      "Epoch 35/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1421 - accuracy: 0.9579 - val_loss: 0.1109 - val_accuracy: 0.9708\n",
      "Epoch 36/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1434 - accuracy: 0.9577 - val_loss: 0.1103 - val_accuracy: 0.9708\n",
      "Epoch 37/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1424 - accuracy: 0.9579 - val_loss: 0.1106 - val_accuracy: 0.9708\n",
      "Epoch 38/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1419 - accuracy: 0.9581 - val_loss: 0.1108 - val_accuracy: 0.9703\n",
      "Epoch 39/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1426 - accuracy: 0.9579 - val_loss: 0.1107 - val_accuracy: 0.9705\n",
      "Epoch 40/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1413 - accuracy: 0.9585 - val_loss: 0.1114 - val_accuracy: 0.9707\n",
      "Epoch 41/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1434 - accuracy: 0.9579 - val_loss: 0.1102 - val_accuracy: 0.9711\n",
      "Epoch 42/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1417 - accuracy: 0.9584 - val_loss: 0.1112 - val_accuracy: 0.9710\n",
      "Epoch 43/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1411 - accuracy: 0.9581 - val_loss: 0.1105 - val_accuracy: 0.9716\n",
      "Epoch 44/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1416 - accuracy: 0.9584 - val_loss: 0.1104 - val_accuracy: 0.9708\n",
      "Epoch 45/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1435 - accuracy: 0.9581 - val_loss: 0.1103 - val_accuracy: 0.9699\n",
      "Epoch 46/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1409 - accuracy: 0.9586 - val_loss: 0.1100 - val_accuracy: 0.9711\n",
      "Epoch 47/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1417 - accuracy: 0.9582 - val_loss: 0.1091 - val_accuracy: 0.9704\n",
      "Epoch 48/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1409 - accuracy: 0.9583 - val_loss: 0.1104 - val_accuracy: 0.9709\n",
      "Epoch 49/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1417 - accuracy: 0.9583 - val_loss: 0.1106 - val_accuracy: 0.9704\n",
      "Epoch 50/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1411 - accuracy: 0.9582 - val_loss: 0.1101 - val_accuracy: 0.9704\n",
      "Epoch 51/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1413 - accuracy: 0.9580 - val_loss: 0.1102 - val_accuracy: 0.9709\n",
      "Epoch 52/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1405 - accuracy: 0.9583 - val_loss: 0.1105 - val_accuracy: 0.9710\n",
      "Epoch 53/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1401 - accuracy: 0.9589 - val_loss: 0.1106 - val_accuracy: 0.9711\n",
      "Epoch 54/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1409 - accuracy: 0.9585 - val_loss: 0.1101 - val_accuracy: 0.9710\n",
      "Epoch 55/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1405 - accuracy: 0.9586 - val_loss: 0.1098 - val_accuracy: 0.9714\n",
      "Epoch 56/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1402 - accuracy: 0.9586 - val_loss: 0.1102 - val_accuracy: 0.9709\n",
      "Epoch 57/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1410 - accuracy: 0.9584 - val_loss: 0.1095 - val_accuracy: 0.9715\n",
      "Epoch 58/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1412 - accuracy: 0.9581 - val_loss: 0.1101 - val_accuracy: 0.9704\n",
      "Epoch 59/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1412 - accuracy: 0.9583 - val_loss: 0.1105 - val_accuracy: 0.9710\n",
      "Epoch 60/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1397 - accuracy: 0.9588 - val_loss: 0.1099 - val_accuracy: 0.9705\n",
      "Epoch 61/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1392 - accuracy: 0.9592 - val_loss: 0.1095 - val_accuracy: 0.9708\n",
      "Epoch 62/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1397 - accuracy: 0.9585 - val_loss: 0.1091 - val_accuracy: 0.9709\n",
      "Epoch 63/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1406 - accuracy: 0.9584 - val_loss: 0.1096 - val_accuracy: 0.9709\n",
      "Epoch 64/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1407 - accuracy: 0.9590 - val_loss: 0.1096 - val_accuracy: 0.9715\n",
      "Epoch 65/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1392 - accuracy: 0.9590 - val_loss: 0.1099 - val_accuracy: 0.9711\n",
      "Epoch 66/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1399 - accuracy: 0.9586 - val_loss: 0.1098 - val_accuracy: 0.9705\n",
      "Epoch 67/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1404 - accuracy: 0.9586 - val_loss: 0.1099 - val_accuracy: 0.9717\n",
      "Score for fold 6: loss of 0.10914476215839386; accuracy of 97.04260230064392%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.4421 - accuracy: 0.7965 - val_loss: 0.1503 - val_accuracy: 0.9569\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1832 - accuracy: 0.9491 - val_loss: 0.1321 - val_accuracy: 0.9613\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1665 - accuracy: 0.9558 - val_loss: 0.1283 - val_accuracy: 0.9637\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1610 - accuracy: 0.9570 - val_loss: 0.1268 - val_accuracy: 0.9644\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1562 - accuracy: 0.9578 - val_loss: 0.1250 - val_accuracy: 0.9643\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1552 - accuracy: 0.9582 - val_loss: 0.1252 - val_accuracy: 0.9644\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1534 - accuracy: 0.9583 - val_loss: 0.1245 - val_accuracy: 0.9647\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1509 - accuracy: 0.9587 - val_loss: 0.1241 - val_accuracy: 0.9651\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1494 - accuracy: 0.9589 - val_loss: 0.1232 - val_accuracy: 0.9648\n",
      "Epoch 10/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1479 - accuracy: 0.9590 - val_loss: 0.1228 - val_accuracy: 0.9651\n",
      "Epoch 11/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1468 - accuracy: 0.9595 - val_loss: 0.1225 - val_accuracy: 0.9650\n",
      "Epoch 12/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1457 - accuracy: 0.9597 - val_loss: 0.1224 - val_accuracy: 0.9651\n",
      "Epoch 13/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1455 - accuracy: 0.9597 - val_loss: 0.1218 - val_accuracy: 0.9651\n",
      "Epoch 14/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1449 - accuracy: 0.9599 - val_loss: 0.1213 - val_accuracy: 0.9649\n",
      "Epoch 15/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1450 - accuracy: 0.9601 - val_loss: 0.1209 - val_accuracy: 0.9651\n",
      "Epoch 16/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1436 - accuracy: 0.9601 - val_loss: 0.1212 - val_accuracy: 0.9651\n",
      "Epoch 17/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1433 - accuracy: 0.9603 - val_loss: 0.1201 - val_accuracy: 0.9653\n",
      "Epoch 18/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1425 - accuracy: 0.9603 - val_loss: 0.1201 - val_accuracy: 0.9655\n",
      "Epoch 19/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1432 - accuracy: 0.9606 - val_loss: 0.1199 - val_accuracy: 0.9652\n",
      "Epoch 20/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1423 - accuracy: 0.9606 - val_loss: 0.1195 - val_accuracy: 0.9655\n",
      "Epoch 21/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1407 - accuracy: 0.9611 - val_loss: 0.1195 - val_accuracy: 0.9656\n",
      "Epoch 22/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1416 - accuracy: 0.9604 - val_loss: 0.1186 - val_accuracy: 0.9658\n",
      "Epoch 23/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1416 - accuracy: 0.9608 - val_loss: 0.1182 - val_accuracy: 0.9653\n",
      "Epoch 24/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1399 - accuracy: 0.9611 - val_loss: 0.1179 - val_accuracy: 0.9658\n",
      "Epoch 25/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1417 - accuracy: 0.9606 - val_loss: 0.1172 - val_accuracy: 0.9656\n",
      "Epoch 26/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1397 - accuracy: 0.9614 - val_loss: 0.1172 - val_accuracy: 0.9656\n",
      "Epoch 27/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1396 - accuracy: 0.9607 - val_loss: 0.1167 - val_accuracy: 0.9661\n",
      "Epoch 28/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1391 - accuracy: 0.9610 - val_loss: 0.1168 - val_accuracy: 0.9656\n",
      "Epoch 29/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1390 - accuracy: 0.9612 - val_loss: 0.1159 - val_accuracy: 0.9657\n",
      "Epoch 30/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1387 - accuracy: 0.9611 - val_loss: 0.1156 - val_accuracy: 0.9659\n",
      "Epoch 31/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1378 - accuracy: 0.9612 - val_loss: 0.1152 - val_accuracy: 0.9658\n",
      "Epoch 32/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1382 - accuracy: 0.9612 - val_loss: 0.1152 - val_accuracy: 0.9669\n",
      "Epoch 33/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1381 - accuracy: 0.9613 - val_loss: 0.1145 - val_accuracy: 0.9663\n",
      "Epoch 34/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1371 - accuracy: 0.9612 - val_loss: 0.1146 - val_accuracy: 0.9660\n",
      "Epoch 35/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1370 - accuracy: 0.9617 - val_loss: 0.1146 - val_accuracy: 0.9666\n",
      "Epoch 36/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1370 - accuracy: 0.9617 - val_loss: 0.1140 - val_accuracy: 0.9667\n",
      "Epoch 37/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1378 - accuracy: 0.9612 - val_loss: 0.1147 - val_accuracy: 0.9666\n",
      "Epoch 38/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1371 - accuracy: 0.9615 - val_loss: 0.1138 - val_accuracy: 0.9666\n",
      "Epoch 39/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1363 - accuracy: 0.9614 - val_loss: 0.1135 - val_accuracy: 0.9666\n",
      "Epoch 40/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1366 - accuracy: 0.9614 - val_loss: 0.1137 - val_accuracy: 0.9665\n",
      "Epoch 41/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1362 - accuracy: 0.9617 - val_loss: 0.1132 - val_accuracy: 0.9666\n",
      "Epoch 42/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1353 - accuracy: 0.9618 - val_loss: 0.1141 - val_accuracy: 0.9670\n",
      "Epoch 43/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1350 - accuracy: 0.9616 - val_loss: 0.1128 - val_accuracy: 0.9665\n",
      "Epoch 44/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1354 - accuracy: 0.9615 - val_loss: 0.1138 - val_accuracy: 0.9670\n",
      "Epoch 45/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1350 - accuracy: 0.9613 - val_loss: 0.1124 - val_accuracy: 0.9670\n",
      "Epoch 46/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1355 - accuracy: 0.9617 - val_loss: 0.1122 - val_accuracy: 0.9671\n",
      "Epoch 47/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1339 - accuracy: 0.9620 - val_loss: 0.1120 - val_accuracy: 0.9672\n",
      "Epoch 48/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1349 - accuracy: 0.9611 - val_loss: 0.1122 - val_accuracy: 0.9672\n",
      "Epoch 49/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1344 - accuracy: 0.9618 - val_loss: 0.1110 - val_accuracy: 0.9672\n",
      "Epoch 50/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1345 - accuracy: 0.9616 - val_loss: 0.1118 - val_accuracy: 0.9670\n",
      "Epoch 51/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1333 - accuracy: 0.9618 - val_loss: 0.1113 - val_accuracy: 0.9673\n",
      "Epoch 52/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1345 - accuracy: 0.9616 - val_loss: 0.1107 - val_accuracy: 0.9669\n",
      "Epoch 53/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1337 - accuracy: 0.9616 - val_loss: 0.1102 - val_accuracy: 0.9672\n",
      "Epoch 54/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1345 - accuracy: 0.9617 - val_loss: 0.1104 - val_accuracy: 0.9672\n",
      "Epoch 55/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1339 - accuracy: 0.9618 - val_loss: 0.1106 - val_accuracy: 0.9675\n",
      "Epoch 56/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1336 - accuracy: 0.9620 - val_loss: 0.1105 - val_accuracy: 0.9674\n",
      "Epoch 57/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1330 - accuracy: 0.9616 - val_loss: 0.1100 - val_accuracy: 0.9674\n",
      "Epoch 58/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1334 - accuracy: 0.9620 - val_loss: 0.1099 - val_accuracy: 0.9673\n",
      "Epoch 59/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1325 - accuracy: 0.9618 - val_loss: 0.1094 - val_accuracy: 0.9672\n",
      "Epoch 60/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1331 - accuracy: 0.9618 - val_loss: 0.1094 - val_accuracy: 0.9673\n",
      "Epoch 61/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1338 - accuracy: 0.9617 - val_loss: 0.1091 - val_accuracy: 0.9674\n",
      "Epoch 62/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1334 - accuracy: 0.9617 - val_loss: 0.1092 - val_accuracy: 0.9675\n",
      "Epoch 63/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1331 - accuracy: 0.9621 - val_loss: 0.1089 - val_accuracy: 0.9672\n",
      "Epoch 64/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1332 - accuracy: 0.9618 - val_loss: 0.1095 - val_accuracy: 0.9673\n",
      "Epoch 65/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1322 - accuracy: 0.9622 - val_loss: 0.1094 - val_accuracy: 0.9672\n",
      "Epoch 66/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1329 - accuracy: 0.9619 - val_loss: 0.1093 - val_accuracy: 0.9675\n",
      "Epoch 67/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1324 - accuracy: 0.9616 - val_loss: 0.1092 - val_accuracy: 0.9671\n",
      "Epoch 68/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1318 - accuracy: 0.9619 - val_loss: 0.1090 - val_accuracy: 0.9677\n",
      "Epoch 69/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1317 - accuracy: 0.9618 - val_loss: 0.1089 - val_accuracy: 0.9677\n",
      "Epoch 70/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1323 - accuracy: 0.9620 - val_loss: 0.1088 - val_accuracy: 0.9677\n",
      "Epoch 71/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1336 - accuracy: 0.9612 - val_loss: 0.1086 - val_accuracy: 0.9673\n",
      "Epoch 72/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1321 - accuracy: 0.9623 - val_loss: 0.1084 - val_accuracy: 0.9677\n",
      "Epoch 73/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1316 - accuracy: 0.9620 - val_loss: 0.1086 - val_accuracy: 0.9677\n",
      "Epoch 74/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1313 - accuracy: 0.9622 - val_loss: 0.1085 - val_accuracy: 0.9675\n",
      "Epoch 75/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1314 - accuracy: 0.9622 - val_loss: 0.1081 - val_accuracy: 0.9673\n",
      "Epoch 76/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1314 - accuracy: 0.9620 - val_loss: 0.1084 - val_accuracy: 0.9676\n",
      "Epoch 77/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1310 - accuracy: 0.9623 - val_loss: 0.1083 - val_accuracy: 0.9677\n",
      "Epoch 78/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1314 - accuracy: 0.9619 - val_loss: 0.1078 - val_accuracy: 0.9673\n",
      "Epoch 79/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1314 - accuracy: 0.9619 - val_loss: 0.1076 - val_accuracy: 0.9674\n",
      "Epoch 80/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1312 - accuracy: 0.9621 - val_loss: 0.1081 - val_accuracy: 0.9677\n",
      "Epoch 81/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1312 - accuracy: 0.9620 - val_loss: 0.1078 - val_accuracy: 0.9675\n",
      "Epoch 82/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1321 - accuracy: 0.9615 - val_loss: 0.1076 - val_accuracy: 0.9677\n",
      "Epoch 83/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1300 - accuracy: 0.9621 - val_loss: 0.1080 - val_accuracy: 0.9673\n",
      "Epoch 84/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1304 - accuracy: 0.9626 - val_loss: 0.1078 - val_accuracy: 0.9673\n",
      "Epoch 85/100\n",
      "5203/5203 [==============================] - 10s 2ms/step - loss: 0.1319 - accuracy: 0.9619 - val_loss: 0.1079 - val_accuracy: 0.9676\n",
      "Epoch 86/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1305 - accuracy: 0.9624 - val_loss: 0.1075 - val_accuracy: 0.9679\n",
      "Epoch 87/100\n",
      "5203/5203 [==============================] - 10s 2ms/step - loss: 0.1302 - accuracy: 0.9626 - val_loss: 0.1078 - val_accuracy: 0.9675\n",
      "Epoch 88/100\n",
      "5203/5203 [==============================] - 10s 2ms/step - loss: 0.1308 - accuracy: 0.9619 - val_loss: 0.1076 - val_accuracy: 0.9682\n",
      "Epoch 89/100\n",
      "5203/5203 [==============================] - 10s 2ms/step - loss: 0.1312 - accuracy: 0.9623 - val_loss: 0.1073 - val_accuracy: 0.9682\n",
      "Epoch 90/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1306 - accuracy: 0.9624 - val_loss: 0.1068 - val_accuracy: 0.9683\n",
      "Epoch 91/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1312 - accuracy: 0.9621 - val_loss: 0.1076 - val_accuracy: 0.9681\n",
      "Epoch 92/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1312 - accuracy: 0.9621 - val_loss: 0.1074 - val_accuracy: 0.9679\n",
      "Epoch 93/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1306 - accuracy: 0.9619 - val_loss: 0.1067 - val_accuracy: 0.9681\n",
      "Epoch 94/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1317 - accuracy: 0.9617 - val_loss: 0.1072 - val_accuracy: 0.9680\n",
      "Epoch 95/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1302 - accuracy: 0.9620 - val_loss: 0.1074 - val_accuracy: 0.9678\n",
      "Epoch 96/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1312 - accuracy: 0.9620 - val_loss: 0.1072 - val_accuracy: 0.9679\n",
      "Epoch 97/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1306 - accuracy: 0.9624 - val_loss: 0.1070 - val_accuracy: 0.9680\n",
      "Epoch 98/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1312 - accuracy: 0.9620 - val_loss: 0.1066 - val_accuracy: 0.9682\n",
      "Epoch 99/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1306 - accuracy: 0.9623 - val_loss: 0.1065 - val_accuracy: 0.9683\n",
      "Epoch 100/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1296 - accuracy: 0.9620 - val_loss: 0.1071 - val_accuracy: 0.9680\n",
      "Score for fold 7: loss of 0.10706435889005661; accuracy of 96.79930806159973%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 14s 2ms/step - loss: 0.4190 - accuracy: 0.8205 - val_loss: 0.1682 - val_accuracy: 0.9484\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1866 - accuracy: 0.9486 - val_loss: 0.1429 - val_accuracy: 0.9597\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1666 - accuracy: 0.9556 - val_loss: 0.1371 - val_accuracy: 0.9631\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1597 - accuracy: 0.9576 - val_loss: 0.1351 - val_accuracy: 0.9639\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 10s 2ms/step - loss: 0.1553 - accuracy: 0.9585 - val_loss: 0.1329 - val_accuracy: 0.9652\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1514 - accuracy: 0.9590 - val_loss: 0.1316 - val_accuracy: 0.9653\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1502 - accuracy: 0.9592 - val_loss: 0.1302 - val_accuracy: 0.9646\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1482 - accuracy: 0.9597 - val_loss: 0.1297 - val_accuracy: 0.9646\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1463 - accuracy: 0.9598 - val_loss: 0.1291 - val_accuracy: 0.9650\n",
      "Epoch 10/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1444 - accuracy: 0.9599 - val_loss: 0.1289 - val_accuracy: 0.9651\n",
      "Epoch 11/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1453 - accuracy: 0.9599 - val_loss: 0.1287 - val_accuracy: 0.9650\n",
      "Epoch 12/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1434 - accuracy: 0.9602 - val_loss: 0.1275 - val_accuracy: 0.9651\n",
      "Epoch 13/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1437 - accuracy: 0.9604 - val_loss: 0.1285 - val_accuracy: 0.9649\n",
      "Epoch 14/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1421 - accuracy: 0.9606 - val_loss: 0.1273 - val_accuracy: 0.9648\n",
      "Epoch 15/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1418 - accuracy: 0.9604 - val_loss: 0.1263 - val_accuracy: 0.9650\n",
      "Epoch 16/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1418 - accuracy: 0.9605 - val_loss: 0.1260 - val_accuracy: 0.9651\n",
      "Epoch 17/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1415 - accuracy: 0.9607 - val_loss: 0.1265 - val_accuracy: 0.9660\n",
      "Epoch 18/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1403 - accuracy: 0.9613 - val_loss: 0.1259 - val_accuracy: 0.9655\n",
      "Epoch 19/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1399 - accuracy: 0.9607 - val_loss: 0.1251 - val_accuracy: 0.9662\n",
      "Epoch 20/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1396 - accuracy: 0.9610 - val_loss: 0.1250 - val_accuracy: 0.9665\n",
      "Epoch 21/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1390 - accuracy: 0.9610 - val_loss: 0.1245 - val_accuracy: 0.9660\n",
      "Epoch 22/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1385 - accuracy: 0.9611 - val_loss: 0.1235 - val_accuracy: 0.9662\n",
      "Epoch 23/100\n",
      "5203/5203 [==============================] - 11s 2ms/step - loss: 0.1383 - accuracy: 0.9613 - val_loss: 0.1240 - val_accuracy: 0.9667\n",
      "Epoch 24/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1394 - accuracy: 0.9610 - val_loss: 0.1231 - val_accuracy: 0.9664\n",
      "Epoch 25/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1387 - accuracy: 0.9613 - val_loss: 0.1243 - val_accuracy: 0.9659\n",
      "Epoch 26/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1370 - accuracy: 0.9619 - val_loss: 0.1227 - val_accuracy: 0.9667\n",
      "Epoch 27/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1371 - accuracy: 0.9617 - val_loss: 0.1231 - val_accuracy: 0.9663\n",
      "Epoch 28/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1373 - accuracy: 0.9618 - val_loss: 0.1229 - val_accuracy: 0.9667\n",
      "Epoch 29/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1373 - accuracy: 0.9616 - val_loss: 0.1225 - val_accuracy: 0.9666\n",
      "Epoch 30/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1367 - accuracy: 0.9620 - val_loss: 0.1227 - val_accuracy: 0.9664\n",
      "Epoch 31/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1366 - accuracy: 0.9615 - val_loss: 0.1225 - val_accuracy: 0.9667\n",
      "Epoch 32/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1372 - accuracy: 0.9618 - val_loss: 0.1214 - val_accuracy: 0.9671\n",
      "Epoch 33/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1370 - accuracy: 0.9616 - val_loss: 0.1215 - val_accuracy: 0.9670\n",
      "Epoch 34/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1369 - accuracy: 0.9617 - val_loss: 0.1214 - val_accuracy: 0.9671\n",
      "Epoch 35/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1367 - accuracy: 0.9619 - val_loss: 0.1200 - val_accuracy: 0.9668\n",
      "Epoch 36/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1357 - accuracy: 0.9624 - val_loss: 0.1203 - val_accuracy: 0.9669\n",
      "Epoch 37/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1356 - accuracy: 0.9627 - val_loss: 0.1200 - val_accuracy: 0.9672\n",
      "Epoch 38/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1351 - accuracy: 0.9622 - val_loss: 0.1204 - val_accuracy: 0.9673\n",
      "Epoch 39/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1353 - accuracy: 0.9627 - val_loss: 0.1188 - val_accuracy: 0.9668\n",
      "Epoch 40/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1350 - accuracy: 0.9624 - val_loss: 0.1190 - val_accuracy: 0.9669\n",
      "Epoch 41/100\n",
      "5203/5203 [==============================] - 17s 3ms/step - loss: 0.1341 - accuracy: 0.9626 - val_loss: 0.1195 - val_accuracy: 0.9671\n",
      "Epoch 42/100\n",
      "5203/5203 [==============================] - 18s 3ms/step - loss: 0.1338 - accuracy: 0.9622 - val_loss: 0.1196 - val_accuracy: 0.9669\n",
      "Epoch 43/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1341 - accuracy: 0.9625 - val_loss: 0.1199 - val_accuracy: 0.9677\n",
      "Epoch 44/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1332 - accuracy: 0.9627 - val_loss: 0.1188 - val_accuracy: 0.9675\n",
      "Epoch 45/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1344 - accuracy: 0.9626 - val_loss: 0.1187 - val_accuracy: 0.9674\n",
      "Epoch 46/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1346 - accuracy: 0.9626 - val_loss: 0.1171 - val_accuracy: 0.9675\n",
      "Epoch 47/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1349 - accuracy: 0.9624 - val_loss: 0.1184 - val_accuracy: 0.9675\n",
      "Epoch 48/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1331 - accuracy: 0.9626 - val_loss: 0.1176 - val_accuracy: 0.9676\n",
      "Epoch 49/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1337 - accuracy: 0.9629 - val_loss: 0.1172 - val_accuracy: 0.9677\n",
      "Epoch 50/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1329 - accuracy: 0.9629 - val_loss: 0.1167 - val_accuracy: 0.9678\n",
      "Epoch 51/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1322 - accuracy: 0.9631 - val_loss: 0.1168 - val_accuracy: 0.9674\n",
      "Epoch 52/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1332 - accuracy: 0.9629 - val_loss: 0.1172 - val_accuracy: 0.9678\n",
      "Epoch 53/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1324 - accuracy: 0.9629 - val_loss: 0.1164 - val_accuracy: 0.9675\n",
      "Epoch 54/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1333 - accuracy: 0.9624 - val_loss: 0.1167 - val_accuracy: 0.9676\n",
      "Epoch 55/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1328 - accuracy: 0.9622 - val_loss: 0.1175 - val_accuracy: 0.9678\n",
      "Epoch 56/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1324 - accuracy: 0.9625 - val_loss: 0.1160 - val_accuracy: 0.9680\n",
      "Epoch 57/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1319 - accuracy: 0.9632 - val_loss: 0.1158 - val_accuracy: 0.9676\n",
      "Epoch 58/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1317 - accuracy: 0.9636 - val_loss: 0.1159 - val_accuracy: 0.9678\n",
      "Epoch 59/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1329 - accuracy: 0.9629 - val_loss: 0.1156 - val_accuracy: 0.9678\n",
      "Epoch 60/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1318 - accuracy: 0.9629 - val_loss: 0.1158 - val_accuracy: 0.9677\n",
      "Epoch 61/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1319 - accuracy: 0.9630 - val_loss: 0.1153 - val_accuracy: 0.9678\n",
      "Epoch 62/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1326 - accuracy: 0.9627 - val_loss: 0.1162 - val_accuracy: 0.9678\n",
      "Epoch 63/100\n",
      "5203/5203 [==============================] - 12s 2ms/step - loss: 0.1314 - accuracy: 0.9632 - val_loss: 0.1149 - val_accuracy: 0.9678\n",
      "Epoch 64/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1318 - accuracy: 0.9632 - val_loss: 0.1163 - val_accuracy: 0.9679\n",
      "Epoch 65/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1314 - accuracy: 0.9632 - val_loss: 0.1157 - val_accuracy: 0.9680\n",
      "Epoch 66/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1314 - accuracy: 0.9631 - val_loss: 0.1154 - val_accuracy: 0.9680\n",
      "Epoch 67/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1320 - accuracy: 0.9628 - val_loss: 0.1148 - val_accuracy: 0.9677\n",
      "Epoch 68/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1318 - accuracy: 0.9632 - val_loss: 0.1145 - val_accuracy: 0.9680\n",
      "Epoch 69/100\n",
      "5203/5203 [==============================] - 13s 2ms/step - loss: 0.1310 - accuracy: 0.9634 - val_loss: 0.1135 - val_accuracy: 0.9679\n",
      "Epoch 70/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1315 - accuracy: 0.9633 - val_loss: 0.1146 - val_accuracy: 0.9678\n",
      "Epoch 71/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1326 - accuracy: 0.9626 - val_loss: 0.1138 - val_accuracy: 0.9679\n",
      "Epoch 72/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1312 - accuracy: 0.9634 - val_loss: 0.1140 - val_accuracy: 0.9679\n",
      "Epoch 73/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1316 - accuracy: 0.9633 - val_loss: 0.1136 - val_accuracy: 0.9680\n",
      "Epoch 74/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1318 - accuracy: 0.9626 - val_loss: 0.1144 - val_accuracy: 0.9681\n",
      "Epoch 75/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1314 - accuracy: 0.9629 - val_loss: 0.1151 - val_accuracy: 0.9679\n",
      "Epoch 76/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1308 - accuracy: 0.9633 - val_loss: 0.1151 - val_accuracy: 0.9685\n",
      "Epoch 77/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1317 - accuracy: 0.9633 - val_loss: 0.1136 - val_accuracy: 0.9679\n",
      "Epoch 78/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1322 - accuracy: 0.9627 - val_loss: 0.1134 - val_accuracy: 0.9679\n",
      "Epoch 79/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1315 - accuracy: 0.9631 - val_loss: 0.1142 - val_accuracy: 0.9684\n",
      "Epoch 80/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1311 - accuracy: 0.9628 - val_loss: 0.1143 - val_accuracy: 0.9682\n",
      "Epoch 81/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1314 - accuracy: 0.9630 - val_loss: 0.1130 - val_accuracy: 0.9684\n",
      "Epoch 82/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1315 - accuracy: 0.9627 - val_loss: 0.1132 - val_accuracy: 0.9683\n",
      "Epoch 83/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1315 - accuracy: 0.9629 - val_loss: 0.1135 - val_accuracy: 0.9683\n",
      "Epoch 84/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1317 - accuracy: 0.9628 - val_loss: 0.1137 - val_accuracy: 0.9685\n",
      "Epoch 85/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1301 - accuracy: 0.9633 - val_loss: 0.1141 - val_accuracy: 0.9679\n",
      "Epoch 86/100\n",
      "5203/5203 [==============================] - 17s 3ms/step - loss: 0.1309 - accuracy: 0.9633 - val_loss: 0.1143 - val_accuracy: 0.9685\n",
      "Epoch 87/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1310 - accuracy: 0.9633 - val_loss: 0.1132 - val_accuracy: 0.9682\n",
      "Epoch 88/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1315 - accuracy: 0.9629 - val_loss: 0.1132 - val_accuracy: 0.9682\n",
      "Epoch 89/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1310 - accuracy: 0.9631 - val_loss: 0.1139 - val_accuracy: 0.9687\n",
      "Epoch 90/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1307 - accuracy: 0.9629 - val_loss: 0.1134 - val_accuracy: 0.9686\n",
      "Epoch 91/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1301 - accuracy: 0.9637 - val_loss: 0.1134 - val_accuracy: 0.9684\n",
      "Epoch 92/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1310 - accuracy: 0.9635 - val_loss: 0.1132 - val_accuracy: 0.9683\n",
      "Epoch 93/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1308 - accuracy: 0.9634 - val_loss: 0.1119 - val_accuracy: 0.9686\n",
      "Epoch 94/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1306 - accuracy: 0.9636 - val_loss: 0.1125 - val_accuracy: 0.9683\n",
      "Epoch 95/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1308 - accuracy: 0.9632 - val_loss: 0.1134 - val_accuracy: 0.9685\n",
      "Epoch 96/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1306 - accuracy: 0.9631 - val_loss: 0.1129 - val_accuracy: 0.9684\n",
      "Epoch 97/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1306 - accuracy: 0.9634 - val_loss: 0.1127 - val_accuracy: 0.9685\n",
      "Epoch 98/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1310 - accuracy: 0.9631 - val_loss: 0.1128 - val_accuracy: 0.9682\n",
      "Epoch 99/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1313 - accuracy: 0.9627 - val_loss: 0.1127 - val_accuracy: 0.9685\n",
      "Epoch 100/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1303 - accuracy: 0.9630 - val_loss: 0.1128 - val_accuracy: 0.9687\n",
      "Score for fold 8: loss of 0.11279238760471344; accuracy of 96.86959385871887%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 19s 3ms/step - loss: 0.3439 - accuracy: 0.8829 - val_loss: 0.1309 - val_accuracy: 0.9620\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1732 - accuracy: 0.9510 - val_loss: 0.1207 - val_accuracy: 0.9647\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1649 - accuracy: 0.9531 - val_loss: 0.1186 - val_accuracy: 0.9655\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1605 - accuracy: 0.9538 - val_loss: 0.1174 - val_accuracy: 0.9666\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1571 - accuracy: 0.9547 - val_loss: 0.1163 - val_accuracy: 0.9670\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1538 - accuracy: 0.9559 - val_loss: 0.1154 - val_accuracy: 0.9671\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1523 - accuracy: 0.9592 - val_loss: 0.1148 - val_accuracy: 0.9679\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1505 - accuracy: 0.9595 - val_loss: 0.1140 - val_accuracy: 0.9683\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1480 - accuracy: 0.9598 - val_loss: 0.1133 - val_accuracy: 0.9684\n",
      "Epoch 10/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1462 - accuracy: 0.9602 - val_loss: 0.1124 - val_accuracy: 0.9689\n",
      "Epoch 11/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1459 - accuracy: 0.9605 - val_loss: 0.1125 - val_accuracy: 0.9688\n",
      "Epoch 12/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1444 - accuracy: 0.9610 - val_loss: 0.1120 - val_accuracy: 0.9686\n",
      "Epoch 13/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1431 - accuracy: 0.9608 - val_loss: 0.1114 - val_accuracy: 0.9692\n",
      "Epoch 14/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1428 - accuracy: 0.9615 - val_loss: 0.1106 - val_accuracy: 0.9692\n",
      "Epoch 15/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1414 - accuracy: 0.9613 - val_loss: 0.1107 - val_accuracy: 0.9693\n",
      "Epoch 16/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1414 - accuracy: 0.9617 - val_loss: 0.1102 - val_accuracy: 0.9691\n",
      "Epoch 17/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1405 - accuracy: 0.9613 - val_loss: 0.1099 - val_accuracy: 0.9694\n",
      "Epoch 18/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1398 - accuracy: 0.9616 - val_loss: 0.1098 - val_accuracy: 0.9692\n",
      "Epoch 19/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1391 - accuracy: 0.9618 - val_loss: 0.1091 - val_accuracy: 0.9695\n",
      "Epoch 20/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1381 - accuracy: 0.9620 - val_loss: 0.1088 - val_accuracy: 0.9698\n",
      "Epoch 21/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1378 - accuracy: 0.9618 - val_loss: 0.1086 - val_accuracy: 0.9701\n",
      "Epoch 22/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1373 - accuracy: 0.9626 - val_loss: 0.1077 - val_accuracy: 0.9700\n",
      "Epoch 23/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1366 - accuracy: 0.9628 - val_loss: 0.1080 - val_accuracy: 0.9697\n",
      "Epoch 24/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1362 - accuracy: 0.9626 - val_loss: 0.1074 - val_accuracy: 0.9700\n",
      "Epoch 25/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1356 - accuracy: 0.9629 - val_loss: 0.1073 - val_accuracy: 0.9703\n",
      "Epoch 26/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1351 - accuracy: 0.9630 - val_loss: 0.1070 - val_accuracy: 0.9706\n",
      "Epoch 27/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1349 - accuracy: 0.9632 - val_loss: 0.1074 - val_accuracy: 0.9700\n",
      "Epoch 28/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1350 - accuracy: 0.9630 - val_loss: 0.1063 - val_accuracy: 0.9705\n",
      "Epoch 29/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1355 - accuracy: 0.9633 - val_loss: 0.1059 - val_accuracy: 0.9708\n",
      "Epoch 30/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1346 - accuracy: 0.9629 - val_loss: 0.1060 - val_accuracy: 0.9705\n",
      "Epoch 31/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1338 - accuracy: 0.9633 - val_loss: 0.1061 - val_accuracy: 0.9708\n",
      "Epoch 32/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1343 - accuracy: 0.9631 - val_loss: 0.1064 - val_accuracy: 0.9704\n",
      "Epoch 33/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1343 - accuracy: 0.9630 - val_loss: 0.1056 - val_accuracy: 0.9706\n",
      "Epoch 34/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1347 - accuracy: 0.9631 - val_loss: 0.1057 - val_accuracy: 0.9706\n",
      "Epoch 35/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1331 - accuracy: 0.9637 - val_loss: 0.1056 - val_accuracy: 0.9708\n",
      "Epoch 36/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1337 - accuracy: 0.9633 - val_loss: 0.1056 - val_accuracy: 0.9704\n",
      "Epoch 37/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1334 - accuracy: 0.9632 - val_loss: 0.1054 - val_accuracy: 0.9709\n",
      "Epoch 38/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1332 - accuracy: 0.9636 - val_loss: 0.1054 - val_accuracy: 0.9710\n",
      "Epoch 39/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1329 - accuracy: 0.9637 - val_loss: 0.1053 - val_accuracy: 0.9710\n",
      "Epoch 40/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1331 - accuracy: 0.9636 - val_loss: 0.1052 - val_accuracy: 0.9709\n",
      "Epoch 41/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1328 - accuracy: 0.9635 - val_loss: 0.1053 - val_accuracy: 0.9709\n",
      "Epoch 42/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1320 - accuracy: 0.9637 - val_loss: 0.1049 - val_accuracy: 0.9708\n",
      "Epoch 43/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1319 - accuracy: 0.9638 - val_loss: 0.1052 - val_accuracy: 0.9713\n",
      "Epoch 44/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1319 - accuracy: 0.9636 - val_loss: 0.1049 - val_accuracy: 0.9709\n",
      "Epoch 45/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1320 - accuracy: 0.9642 - val_loss: 0.1051 - val_accuracy: 0.9708\n",
      "Epoch 46/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1321 - accuracy: 0.9637 - val_loss: 0.1050 - val_accuracy: 0.9712\n",
      "Epoch 47/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1318 - accuracy: 0.9641 - val_loss: 0.1046 - val_accuracy: 0.9712\n",
      "Epoch 48/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1317 - accuracy: 0.9639 - val_loss: 0.1046 - val_accuracy: 0.9710\n",
      "Epoch 49/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1314 - accuracy: 0.9635 - val_loss: 0.1049 - val_accuracy: 0.9713\n",
      "Epoch 50/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1324 - accuracy: 0.9635 - val_loss: 0.1045 - val_accuracy: 0.9713\n",
      "Epoch 51/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1307 - accuracy: 0.9641 - val_loss: 0.1043 - val_accuracy: 0.9709\n",
      "Epoch 52/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1325 - accuracy: 0.9638 - val_loss: 0.1042 - val_accuracy: 0.9712\n",
      "Epoch 53/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1311 - accuracy: 0.9642 - val_loss: 0.1042 - val_accuracy: 0.9712\n",
      "Epoch 54/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1312 - accuracy: 0.9639 - val_loss: 0.1043 - val_accuracy: 0.9711\n",
      "Epoch 55/100\n",
      "5203/5203 [==============================] - 17s 3ms/step - loss: 0.1316 - accuracy: 0.9638 - val_loss: 0.1045 - val_accuracy: 0.9711\n",
      "Epoch 56/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1315 - accuracy: 0.9640 - val_loss: 0.1048 - val_accuracy: 0.9709\n",
      "Epoch 57/100\n",
      "5203/5203 [==============================] - 18s 3ms/step - loss: 0.1307 - accuracy: 0.9644 - val_loss: 0.1049 - val_accuracy: 0.9708\n",
      "Epoch 58/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1308 - accuracy: 0.9642 - val_loss: 0.1042 - val_accuracy: 0.9718\n",
      "Epoch 59/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1311 - accuracy: 0.9640 - val_loss: 0.1044 - val_accuracy: 0.9713\n",
      "Epoch 60/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1311 - accuracy: 0.9640 - val_loss: 0.1047 - val_accuracy: 0.9712\n",
      "Epoch 61/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1312 - accuracy: 0.9639 - val_loss: 0.1043 - val_accuracy: 0.9712\n",
      "Epoch 62/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1309 - accuracy: 0.9640 - val_loss: 0.1042 - val_accuracy: 0.9714\n",
      "Epoch 63/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1313 - accuracy: 0.9641 - val_loss: 0.1044 - val_accuracy: 0.9709\n",
      "Epoch 64/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1309 - accuracy: 0.9640 - val_loss: 0.1043 - val_accuracy: 0.9711\n",
      "Epoch 65/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1302 - accuracy: 0.9645 - val_loss: 0.1041 - val_accuracy: 0.9715\n",
      "Epoch 66/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1307 - accuracy: 0.9641 - val_loss: 0.1045 - val_accuracy: 0.9713\n",
      "Epoch 67/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1309 - accuracy: 0.9641 - val_loss: 0.1037 - val_accuracy: 0.9716\n",
      "Epoch 68/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1304 - accuracy: 0.9641 - val_loss: 0.1039 - val_accuracy: 0.9715\n",
      "Epoch 69/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1301 - accuracy: 0.9643 - val_loss: 0.1035 - val_accuracy: 0.9711\n",
      "Epoch 70/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1312 - accuracy: 0.9639 - val_loss: 0.1038 - val_accuracy: 0.9713\n",
      "Epoch 71/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1309 - accuracy: 0.9640 - val_loss: 0.1038 - val_accuracy: 0.9716\n",
      "Epoch 72/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1311 - accuracy: 0.9639 - val_loss: 0.1036 - val_accuracy: 0.9715\n",
      "Epoch 73/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1305 - accuracy: 0.9641 - val_loss: 0.1036 - val_accuracy: 0.9715\n",
      "Epoch 74/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1305 - accuracy: 0.9639 - val_loss: 0.1035 - val_accuracy: 0.9712\n",
      "Epoch 75/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1296 - accuracy: 0.9643 - val_loss: 0.1037 - val_accuracy: 0.9707\n",
      "Epoch 76/100\n",
      "5203/5203 [==============================] - 16s 3ms/step - loss: 0.1311 - accuracy: 0.9641 - val_loss: 0.1033 - val_accuracy: 0.9716\n",
      "Epoch 77/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1303 - accuracy: 0.9643 - val_loss: 0.1035 - val_accuracy: 0.9712\n",
      "Epoch 78/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1304 - accuracy: 0.9641 - val_loss: 0.1037 - val_accuracy: 0.9711\n",
      "Epoch 79/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1304 - accuracy: 0.9643 - val_loss: 0.1030 - val_accuracy: 0.9713\n",
      "Epoch 80/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1297 - accuracy: 0.9645 - val_loss: 0.1029 - val_accuracy: 0.9718\n",
      "Epoch 81/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1300 - accuracy: 0.9642 - val_loss: 0.1029 - val_accuracy: 0.9716\n",
      "Epoch 82/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1302 - accuracy: 0.9643 - val_loss: 0.1032 - val_accuracy: 0.9711\n",
      "Epoch 83/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1294 - accuracy: 0.9643 - val_loss: 0.1030 - val_accuracy: 0.9715\n",
      "Epoch 84/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1299 - accuracy: 0.9640 - val_loss: 0.1033 - val_accuracy: 0.9712\n",
      "Epoch 85/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1292 - accuracy: 0.9646 - val_loss: 0.1032 - val_accuracy: 0.9711\n",
      "Epoch 86/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1296 - accuracy: 0.9642 - val_loss: 0.1030 - val_accuracy: 0.9710\n",
      "Epoch 87/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1303 - accuracy: 0.9640 - val_loss: 0.1027 - val_accuracy: 0.9712\n",
      "Epoch 88/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1296 - accuracy: 0.9644 - val_loss: 0.1026 - val_accuracy: 0.9716\n",
      "Epoch 89/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1298 - accuracy: 0.9641 - val_loss: 0.1028 - val_accuracy: 0.9711\n",
      "Epoch 90/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1305 - accuracy: 0.9637 - val_loss: 0.1025 - val_accuracy: 0.9714\n",
      "Epoch 91/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1294 - accuracy: 0.9645 - val_loss: 0.1023 - val_accuracy: 0.9713\n",
      "Epoch 92/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1287 - accuracy: 0.9643 - val_loss: 0.1028 - val_accuracy: 0.9714\n",
      "Epoch 93/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1292 - accuracy: 0.9642 - val_loss: 0.1029 - val_accuracy: 0.9713\n",
      "Epoch 94/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1295 - accuracy: 0.9643 - val_loss: 0.1033 - val_accuracy: 0.9713\n",
      "Epoch 95/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1299 - accuracy: 0.9640 - val_loss: 0.1025 - val_accuracy: 0.9713\n",
      "Epoch 96/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1291 - accuracy: 0.9641 - val_loss: 0.1032 - val_accuracy: 0.9712\n",
      "Epoch 97/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1296 - accuracy: 0.9642 - val_loss: 0.1025 - val_accuracy: 0.9715\n",
      "Epoch 98/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1286 - accuracy: 0.9644 - val_loss: 0.1024 - val_accuracy: 0.9714\n",
      "Epoch 99/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1297 - accuracy: 0.9644 - val_loss: 0.1023 - val_accuracy: 0.9715\n",
      "Epoch 100/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1289 - accuracy: 0.9646 - val_loss: 0.1022 - val_accuracy: 0.9712\n",
      "Score for fold 9: loss of 0.10221244394779205; accuracy of 97.12370038032532%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "5203/5203 [==============================] - 17s 3ms/step - loss: 0.5086 - accuracy: 0.7630 - val_loss: 0.3209 - val_accuracy: 0.9547\n",
      "Epoch 2/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.3492 - accuracy: 0.8921 - val_loss: 0.2442 - val_accuracy: 0.9612\n",
      "Epoch 3/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.2542 - accuracy: 0.9428 - val_loss: 0.2010 - val_accuracy: 0.9619\n",
      "Epoch 4/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.2063 - accuracy: 0.9541 - val_loss: 0.1723 - val_accuracy: 0.9629\n",
      "Epoch 5/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1823 - accuracy: 0.9564 - val_loss: 0.1555 - val_accuracy: 0.9658\n",
      "Epoch 6/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1682 - accuracy: 0.9578 - val_loss: 0.1453 - val_accuracy: 0.9662\n",
      "Epoch 7/100\n",
      "5203/5203 [==============================] - 14s 3ms/step - loss: 0.1594 - accuracy: 0.9589 - val_loss: 0.1420 - val_accuracy: 0.9669\n",
      "Epoch 8/100\n",
      "5203/5203 [==============================] - 13s 3ms/step - loss: 0.1556 - accuracy: 0.9596 - val_loss: 0.1358 - val_accuracy: 0.9663\n",
      "Epoch 9/100\n",
      "5203/5203 [==============================] - 15s 3ms/step - loss: 0.1511 - accuracy: 0.9599 - val_loss: 0.1339 - val_accuracy: 0.9675\n",
      "Epoch 10/100\n",
      "5184/5203 [============================>.] - ETA: 0s - loss: 0.1488 - accuracy: 0.9599"
     ]
    }
   ],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_train, X_val), axis=0)\n",
    "targets = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "for train, val in kfold.split(inputs, targets):\n",
    "\n",
    "  input = Input(shape=(X_train_scaled.shape[1],))\n",
    "\n",
    "  layer_1 = Dense(12, activation='relu', kernel_regularizer=L2(0.0001))(input)\n",
    "  dropout_1 = Dropout(0.1)(layer_1)\n",
    "  layer_2 = Dense(6, activation='relu', kernel_regularizer=L2(0.0001))(dropout_1)\n",
    "  dropout_2 = Dropout(0.1)(layer_2)\n",
    "  layer_3 = Dense(3, activation='relu', kernel_regularizer=L2(0.0001))(dropout_2)\n",
    "  dropout_3 = Dropout(0.1)(layer_3)\n",
    "  flatten = Flatten()(dropout_3)\n",
    "\n",
    "  output = Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "  model = Model(inputs=input, outputs=output)\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  batch_size = 32\n",
    "  h = model.fit(\n",
    "      inputs[train],\n",
    "      targets[train],\n",
    "      epochs=100, \n",
    "      batch_size = batch_size,\n",
    "      validation_data = (inputs[val], targets[val]),\n",
    "      callbacks = [earlystopping],\n",
    "      shuffle=True,\n",
    "      verbose=True\n",
    "      )\n",
    "\n",
    "  # Generate generalization metrics\n",
    "  scores = model.evaluate(inputs[val], targets[val], verbose=0)\n",
    "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "  acc_per_fold.append(scores[1] * 100)\n",
    "  loss_per_fold.append(scores[0])\n",
    "  \n",
    "  plt.plot(h.history['loss'], label='Train Loss')\n",
    "  plt.plot(h.history['val_loss'], label='Validation Loss')\n",
    "  plt.title('Model Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend();\n",
    "\n",
    "  plt.plot(h.history['accuracy'], label='Accuracy')\n",
    "  plt.plot(h.history['val_accuracy'], label='Validation Accuracy')\n",
    "  plt.title('Model Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend();\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "layer_1 = Dense(12, activation='relu', kernel_regularizer=L2(0.0001))(input)\n",
    "dropout_1 = Dropout(0.1)(layer_1)\n",
    "layer_2 = Dense(6, activation='relu', kernel_regularizer=L2(0.0001))(dropout_1)\n",
    "dropout_2 = Dropout(0.1)(layer_2)\n",
    "layer_3 = Dense(3, activation='relu', kernel_regularizer=L2(0.0001))(dropout_2)\n",
    "dropout_3 = Dropout(0.1)(layer_3)\n",
    "flatten = Flatten()(dropout_3)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "optimizer=Adam(learning_rate=0.0001),\n",
    "loss='binary_crossentropy',\n",
    "metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4625/4625 [==============================] - 12s 2ms/step - loss: 0.4043 - accuracy: 0.8040 - val_loss: 0.0599 - val_accuracy: 0.9934\n",
      "Epoch 2/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.2211 - accuracy: 0.9129 - val_loss: 0.0489 - val_accuracy: 0.9937\n",
      "Epoch 3/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.2019 - accuracy: 0.9186 - val_loss: 0.0458 - val_accuracy: 0.9937\n",
      "Epoch 4/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1918 - accuracy: 0.9192 - val_loss: 0.0438 - val_accuracy: 0.9937\n",
      "Epoch 5/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1836 - accuracy: 0.9280 - val_loss: 0.0428 - val_accuracy: 0.9937\n",
      "Epoch 6/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1790 - accuracy: 0.9509 - val_loss: 0.0419 - val_accuracy: 0.9937\n",
      "Epoch 7/100\n",
      "4625/4625 [==============================] - 10s 2ms/step - loss: 0.1767 - accuracy: 0.9520 - val_loss: 0.0419 - val_accuracy: 0.9937\n",
      "Epoch 8/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1742 - accuracy: 0.9526 - val_loss: 0.0414 - val_accuracy: 0.9937\n",
      "Epoch 9/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1723 - accuracy: 0.9529 - val_loss: 0.0409 - val_accuracy: 0.9937\n",
      "Epoch 10/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1693 - accuracy: 0.9532 - val_loss: 0.0400 - val_accuracy: 0.9938\n",
      "Epoch 11/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1686 - accuracy: 0.9527 - val_loss: 0.0401 - val_accuracy: 0.9938\n",
      "Epoch 12/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1672 - accuracy: 0.9533 - val_loss: 0.0399 - val_accuracy: 0.9938\n",
      "Epoch 13/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1657 - accuracy: 0.9535 - val_loss: 0.0401 - val_accuracy: 0.9938\n",
      "Epoch 14/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1647 - accuracy: 0.9540 - val_loss: 0.0405 - val_accuracy: 0.9938\n",
      "Epoch 15/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1647 - accuracy: 0.9537 - val_loss: 0.0397 - val_accuracy: 0.9938\n",
      "Epoch 16/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1619 - accuracy: 0.9541 - val_loss: 0.0398 - val_accuracy: 0.9938\n",
      "Epoch 17/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1621 - accuracy: 0.9538 - val_loss: 0.0403 - val_accuracy: 0.9938\n",
      "Epoch 18/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1620 - accuracy: 0.9544 - val_loss: 0.0404 - val_accuracy: 0.9938\n",
      "Epoch 19/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1616 - accuracy: 0.9535 - val_loss: 0.0401 - val_accuracy: 0.9938\n",
      "Epoch 20/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1604 - accuracy: 0.9541 - val_loss: 0.0401 - val_accuracy: 0.9938\n",
      "Epoch 21/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1593 - accuracy: 0.9546 - val_loss: 0.0400 - val_accuracy: 0.9938\n",
      "Epoch 22/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1590 - accuracy: 0.9547 - val_loss: 0.0401 - val_accuracy: 0.9938\n",
      "Epoch 23/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1596 - accuracy: 0.9545 - val_loss: 0.0399 - val_accuracy: 0.9938\n",
      "Epoch 24/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1582 - accuracy: 0.9548 - val_loss: 0.0400 - val_accuracy: 0.9938\n",
      "Epoch 25/100\n",
      "4625/4625 [==============================] - 12s 3ms/step - loss: 0.1590 - accuracy: 0.9548 - val_loss: 0.0399 - val_accuracy: 0.9938\n",
      "Epoch 26/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1582 - accuracy: 0.9543 - val_loss: 0.0398 - val_accuracy: 0.9938\n",
      "Epoch 27/100\n",
      "4625/4625 [==============================] - 12s 3ms/step - loss: 0.1583 - accuracy: 0.9546 - val_loss: 0.0397 - val_accuracy: 0.9938\n",
      "Epoch 28/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1579 - accuracy: 0.9547 - val_loss: 0.0395 - val_accuracy: 0.9938\n",
      "Epoch 29/100\n",
      "4625/4625 [==============================] - 13s 3ms/step - loss: 0.1575 - accuracy: 0.9545 - val_loss: 0.0395 - val_accuracy: 0.9938\n",
      "Epoch 30/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1564 - accuracy: 0.9552 - val_loss: 0.0394 - val_accuracy: 0.9938\n",
      "Epoch 31/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1567 - accuracy: 0.9543 - val_loss: 0.0396 - val_accuracy: 0.9938\n",
      "Epoch 32/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1559 - accuracy: 0.9549 - val_loss: 0.0394 - val_accuracy: 0.9938\n",
      "Epoch 33/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1549 - accuracy: 0.9553 - val_loss: 0.0399 - val_accuracy: 0.9938\n",
      "Epoch 34/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1548 - accuracy: 0.9555 - val_loss: 0.0399 - val_accuracy: 0.9938\n",
      "Epoch 35/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1564 - accuracy: 0.9552 - val_loss: 0.0396 - val_accuracy: 0.9938\n",
      "Epoch 36/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1553 - accuracy: 0.9549 - val_loss: 0.0396 - val_accuracy: 0.9938\n",
      "Epoch 37/100\n",
      "4625/4625 [==============================] - 12s 3ms/step - loss: 0.1553 - accuracy: 0.9552 - val_loss: 0.0403 - val_accuracy: 0.9938\n",
      "Epoch 38/100\n",
      "4625/4625 [==============================] - 12s 2ms/step - loss: 0.1561 - accuracy: 0.9550 - val_loss: 0.0401 - val_accuracy: 0.9938\n",
      "Epoch 39/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1555 - accuracy: 0.9546 - val_loss: 0.0400 - val_accuracy: 0.9938\n",
      "Epoch 40/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1556 - accuracy: 0.9550 - val_loss: 0.0398 - val_accuracy: 0.9938\n",
      "Epoch 41/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1540 - accuracy: 0.9553 - val_loss: 0.0395 - val_accuracy: 0.9938\n",
      "Epoch 42/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1549 - accuracy: 0.9553 - val_loss: 0.0396 - val_accuracy: 0.9938\n",
      "Epoch 43/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1548 - accuracy: 0.9546 - val_loss: 0.0400 - val_accuracy: 0.9938\n",
      "Epoch 44/100\n",
      "4625/4625 [==============================] - 12s 3ms/step - loss: 0.1535 - accuracy: 0.9553 - val_loss: 0.0397 - val_accuracy: 0.9938\n",
      "Epoch 45/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1547 - accuracy: 0.9554 - val_loss: 0.0400 - val_accuracy: 0.9938\n",
      "Epoch 46/100\n",
      "4625/4625 [==============================] - 11s 2ms/step - loss: 0.1541 - accuracy: 0.9554 - val_loss: 0.0397 - val_accuracy: 0.9938\n",
      "Epoch 47/100\n",
      "4625/4625 [==============================] - 12s 2ms/step - loss: 0.1545 - accuracy: 0.9552 - val_loss: 0.0396 - val_accuracy: 0.9938\n",
      "Epoch 48/100\n",
      "4625/4625 [==============================] - 12s 3ms/step - loss: 0.1530 - accuracy: 0.9554 - val_loss: 0.0398 - val_accuracy: 0.9938\n",
      "Epoch 49/100\n",
      "4625/4625 [==============================] - 12s 3ms/step - loss: 0.1532 - accuracy: 0.9552 - val_loss: 0.0397 - val_accuracy: 0.9938\n",
      "Epoch 50/100\n",
      "4625/4625 [==============================] - 12s 3ms/step - loss: 0.1527 - accuracy: 0.9557 - val_loss: 0.0399 - val_accuracy: 0.9938\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "h = model.fit(\n",
    "    # X_train_scaled_selected, y_train,\n",
    "    X_train, y_train,\n",
    "      epochs=100, \n",
    "      batch_size = batch_size,\n",
    "      # validation_data = (X_val_scaled_selected, y_val),\n",
    "      validation_data = (X_val, y_val),\n",
    "      callbacks = [earlystopping],\n",
    "      shuffle=True,\n",
    "      verbose=True\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(h.history['loss'],label='Train Loss')\n",
    "plt.plot(h.history['val_loss'],label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(h.history['accuracy'],label='Accuracy')\n",
    "plt.plot(h.history['val_accuracy'],label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Estimator needs to be fit before `predict` can be called",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[287], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred_probas \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test_scaled)\n\u001b[0;32m      2\u001b[0m y_pred \u001b[39m=\u001b[39m y_pred_probas\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:1048\u001b[0m, in \u001b[0;36mBaseWrapper.predict\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns predictions for the given test data.\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \n\u001b[0;32m   1026\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[39m    Predictions, of shape shape (n_samples,) or (n_samples, n_outputs).\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# predict with Keras model\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_raw(X\u001b[39m=\u001b[39mX, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1050\u001b[0m \u001b[39m# post process y\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_encoder_\u001b[39m.\u001b[39minverse_transform(y_pred)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\scikeras\\wrappers.py:990\u001b[0m, in \u001b[0;36mBaseWrapper._predict_raw\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[39m# check if fitted\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialized_:\n\u001b[1;32m--> 990\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(\n\u001b[0;32m    991\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEstimator needs to be fit before `predict` \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcan be called\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m     )\n\u001b[0;32m    993\u001b[0m \u001b[39m# basic input checks\u001b[39;00m\n\u001b[0;32m    994\u001b[0m X, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Estimator needs to be fit before `predict` can be called"
     ]
    }
   ],
   "source": [
    "y_pred_probas = model.predict(X_test_scaled)\n",
    "y_pred = y_pred_probas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred <= 0.6] = 0.\n",
    "y_pred[y_pred > 0.6] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 1s 2ms/step - loss: 0.2672 - accuracy: 0.8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2671751379966736, 0.8956447839736938]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29706963, 0.06842011],\n",
       "       [0.03553186, 0.5989784 ]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc = roc_auc_score(y_test, y_pred_probas)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred,normalize='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision, recall,thres = precision_recall_curve(y_test, y_pred_probas)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.plot(precision, recall, 'g', label = 'Precision = %0.2f\\nRecall = %0.2f' % (prec, rec))\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([1, 0], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_test = y_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61971</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169676</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136772</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  label    0\n",
       "0   36509    1.0  1.0\n",
       "1   61971    1.0  0.0\n",
       "2  136899    1.0  1.0\n",
       "3  169676    1.0  1.0\n",
       "4  136772    1.0  1.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.concat([y_test, y_pred], axis=1)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.rename(columns={0:'y_pred', 'label':'y_test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['y_pred'] = predictions['y_pred'].astype('int')\n",
    "predictions['y_test'] = predictions['y_test'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = predictions.loc[predictions['y_pred']==1]['index'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = predictions.loc[predictions['y_pred']!=predictions['y_test']]['index'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36509</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61971</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136899</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169676</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136772</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22313</th>\n",
       "      <td>12906</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22314</th>\n",
       "      <td>81842</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22315</th>\n",
       "      <td>150234</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22316</th>\n",
       "      <td>20165</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22317</th>\n",
       "      <td>196389</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22318 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  y_test  y_pred\n",
       "0       36509       1       1\n",
       "1       61971       1       0\n",
       "2      136899       1       1\n",
       "3      169676       1       1\n",
       "4      136772       1       1\n",
       "...       ...     ...     ...\n",
       "22313   12906       1       1\n",
       "22314   81842       1       1\n",
       "22315  150234       1       1\n",
       "22316   20165       1       1\n",
       "22317  196389       1       1\n",
       "\n",
       "[22318 rows x 3 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>days_in_index</th>\n",
       "      <th>period_total_return</th>\n",
       "      <th>calmar_ratio</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>period_cumulative_return</th>\n",
       "      <th>total_cumulative_return</th>\n",
       "      <th>days_since_last_max</th>\n",
       "      <th>days_since_last_min</th>\n",
       "      <th>...</th>\n",
       "      <th>treynor_ratio</th>\n",
       "      <th>benchmark_distance</th>\n",
       "      <th>information_ratio</th>\n",
       "      <th>modigliani_measure</th>\n",
       "      <th>alfa_jensen</th>\n",
       "      <th>sharpe_ratio_next_period</th>\n",
       "      <th>returns_next_period</th>\n",
       "      <th>alfa_next_period</th>\n",
       "      <th>alfa_signal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187449</th>\n",
       "      <td>SGRE</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>880</td>\n",
       "      <td>-0.005072</td>\n",
       "      <td>-0.017551</td>\n",
       "      <td>-0.721979</td>\n",
       "      <td>0.210634</td>\n",
       "      <td>0.699315</td>\n",
       "      <td>4</td>\n",
       "      <td>1134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015996</td>\n",
       "      <td>0.025859</td>\n",
       "      <td>0.247731</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.177445</td>\n",
       "      <td>0.020454</td>\n",
       "      <td>0.026028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187973</th>\n",
       "      <td>SLR</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>53</td>\n",
       "      <td>0.040986</td>\n",
       "      <td>-0.045590</td>\n",
       "      <td>-0.338655</td>\n",
       "      <td>0.293682</td>\n",
       "      <td>0.347050</td>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184788</td>\n",
       "      <td>0.028319</td>\n",
       "      <td>0.333083</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.046705</td>\n",
       "      <td>0.033991</td>\n",
       "      <td>-0.085201</td>\n",
       "      <td>-0.079294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209623</th>\n",
       "      <td>VIS_3</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>1160</td>\n",
       "      <td>-0.019335</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.397681</td>\n",
       "      <td>0.018488</td>\n",
       "      <td>0.290058</td>\n",
       "      <td>144</td>\n",
       "      <td>1515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.014178</td>\n",
       "      <td>-0.006008</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.014094</td>\n",
       "      <td>0.483266</td>\n",
       "      <td>0.016791</td>\n",
       "      <td>0.022173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46968</th>\n",
       "      <td>BKIA_1</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>1795</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>-0.005658</td>\n",
       "      <td>-0.851932</td>\n",
       "      <td>-0.024886</td>\n",
       "      <td>-0.937831</td>\n",
       "      <td>2492</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>-0.121577</td>\n",
       "      <td>-0.002716</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.331531</td>\n",
       "      <td>-0.000465</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187974</th>\n",
       "      <td>SLR</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>54</td>\n",
       "      <td>0.100811</td>\n",
       "      <td>-0.046933</td>\n",
       "      <td>-0.358075</td>\n",
       "      <td>0.334668</td>\n",
       "      <td>0.427720</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087235</td>\n",
       "      <td>0.031404</td>\n",
       "      <td>0.355727</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.101765</td>\n",
       "      <td>-0.022479</td>\n",
       "      <td>-0.107057</td>\n",
       "      <td>-0.164280</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44856</th>\n",
       "      <td>BBVA</td>\n",
       "      <td>2023-06-06</td>\n",
       "      <td>7552</td>\n",
       "      <td>0.026206</td>\n",
       "      <td>0.037347</td>\n",
       "      <td>-0.934365</td>\n",
       "      <td>-0.050567</td>\n",
       "      <td>2.142191</td>\n",
       "      <td>5964</td>\n",
       "      <td>10568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.016013</td>\n",
       "      <td>-0.082882</td>\n",
       "      <td>0.020889</td>\n",
       "      <td>-0.041705</td>\n",
       "      <td>-2.881897</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>-0.091947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181703</th>\n",
       "      <td>SAN_1</td>\n",
       "      <td>2023-06-06</td>\n",
       "      <td>5495</td>\n",
       "      <td>0.062201</td>\n",
       "      <td>0.047595</td>\n",
       "      <td>-0.766811</td>\n",
       "      <td>-0.098591</td>\n",
       "      <td>0.045586</td>\n",
       "      <td>4894</td>\n",
       "      <td>986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>0.013130</td>\n",
       "      <td>-0.226362</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>-0.003898</td>\n",
       "      <td>-1.945593</td>\n",
       "      <td>-0.004048</td>\n",
       "      <td>-0.122331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174797</th>\n",
       "      <td>SAB</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>4832</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>0.035218</td>\n",
       "      <td>-0.938345</td>\n",
       "      <td>-0.002003</td>\n",
       "      <td>-0.623825</td>\n",
       "      <td>5951</td>\n",
       "      <td>951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>-0.017114</td>\n",
       "      <td>0.022597</td>\n",
       "      <td>-0.110744</td>\n",
       "      <td>-2.366340</td>\n",
       "      <td>-0.009365</td>\n",
       "      <td>-0.075852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174798</th>\n",
       "      <td>SAB</td>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>4833</td>\n",
       "      <td>-0.023737</td>\n",
       "      <td>0.035503</td>\n",
       "      <td>-0.938345</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>-0.616234</td>\n",
       "      <td>5952</td>\n",
       "      <td>952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.018201</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.023031</td>\n",
       "      <td>-0.149335</td>\n",
       "      <td>-2.386220</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>-0.063308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174799</th>\n",
       "      <td>SAB</td>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>4834</td>\n",
       "      <td>0.070521</td>\n",
       "      <td>0.036079</td>\n",
       "      <td>-0.938345</td>\n",
       "      <td>-0.018149</td>\n",
       "      <td>-0.622022</td>\n",
       "      <td>5953</td>\n",
       "      <td>953</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>-0.032137</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>-0.013503</td>\n",
       "      <td>-2.372145</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>-0.084052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2320 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ticker       date  days_in_index  period_total_return  calmar_ratio  \\\n",
       "187449    SGRE 2021-01-04            880            -0.005072     -0.017551   \n",
       "187973     SLR 2021-01-04             53             0.040986     -0.045590   \n",
       "209623   VIS_3 2021-01-04           1160            -0.019335     -0.015757   \n",
       "46968   BKIA_1 2021-01-04           1795             0.008211     -0.005658   \n",
       "187974     SLR 2021-01-05             54             0.100811     -0.046933   \n",
       "...        ...        ...            ...                  ...           ...   \n",
       "44856     BBVA 2023-06-06           7552             0.026206      0.037347   \n",
       "181703   SAN_1 2023-06-06           5495             0.062201      0.047595   \n",
       "174797     SAB 2023-06-07           4832             0.007591      0.035218   \n",
       "174798     SAB 2023-06-08           4833            -0.023737      0.035503   \n",
       "174799     SAB 2023-06-09           4834             0.070521      0.036079   \n",
       "\n",
       "        max_drawdown  period_cumulative_return  total_cumulative_return  \\\n",
       "187449     -0.721979                  0.210634                 0.699315   \n",
       "187973     -0.338655                  0.293682                 0.347050   \n",
       "209623     -0.397681                  0.018488                 0.290058   \n",
       "46968      -0.851932                 -0.024886                -0.937831   \n",
       "187974     -0.358075                  0.334668                 0.427720   \n",
       "...              ...                       ...                      ...   \n",
       "44856      -0.934365                 -0.050567                 2.142191   \n",
       "181703     -0.766811                 -0.098591                 0.045586   \n",
       "174797     -0.938345                 -0.002003                -0.623825   \n",
       "174798     -0.938345                  0.005588                -0.616234   \n",
       "174799     -0.938345                 -0.018149                -0.622022   \n",
       "\n",
       "        days_since_last_max  days_since_last_min  ...  treynor_ratio  \\\n",
       "187449                    4                 1134  ...      -0.015996   \n",
       "187973                    5                   68  ...      -0.184788   \n",
       "209623                  144                 1515  ...       0.001955   \n",
       "46968                  2492                  228  ...      -0.000686   \n",
       "187974                    1                   69  ...      -0.087235   \n",
       "...                     ...                  ...  ...            ...   \n",
       "44856                  5964                10568  ...      -0.000755   \n",
       "181703                 4894                  986  ...      -0.001553   \n",
       "174797                 5951                  951  ...      -0.000028   \n",
       "174798                 5952                  952  ...       0.000078   \n",
       "174799                 5953                  953  ...      -0.000257   \n",
       "\n",
       "        benchmark_distance  information_ratio  modigliani_measure  \\\n",
       "187449            0.025859           0.247731            0.001257   \n",
       "187973            0.028319           0.333083            0.001330   \n",
       "209623            0.014178          -0.006008           -0.000168   \n",
       "46968             0.012487          -0.121577           -0.002716   \n",
       "187974            0.031404           0.355727            0.001170   \n",
       "...                    ...                ...                 ...   \n",
       "44856             0.016013          -0.082882            0.020889   \n",
       "181703            0.013130          -0.226362            0.018507   \n",
       "174797            0.018112          -0.017114            0.022597   \n",
       "174798            0.018201           0.001441            0.023031   \n",
       "174799            0.017981          -0.032137            0.022491   \n",
       "\n",
       "        alfa_jensen  sharpe_ratio_next_period  returns_next_period  \\\n",
       "187449     0.001147                  0.177445             0.020454   \n",
       "187973     0.046705                  0.033991            -0.085201   \n",
       "209623    -0.014094                  0.483266             0.016791   \n",
       "46968      0.012291                  0.331531            -0.000465   \n",
       "187974     0.101765                 -0.022479            -0.107057   \n",
       "...             ...                       ...                  ...   \n",
       "44856     -0.041705                 -2.881897            -0.009674   \n",
       "181703    -0.003898                 -1.945593            -0.004048   \n",
       "174797    -0.110744                 -2.366340            -0.009365   \n",
       "174798    -0.149335                 -2.386220             0.003121   \n",
       "174799    -0.013503                 -2.372145            -0.003985   \n",
       "\n",
       "        alfa_next_period  alfa_signal  label  \n",
       "187449          0.026028          0.0    1.0  \n",
       "187973         -0.079294          1.0    1.0  \n",
       "209623          0.022173          0.0    0.0  \n",
       "46968           0.005266          0.0    0.0  \n",
       "187974         -0.164280          1.0    1.0  \n",
       "...                  ...          ...    ...  \n",
       "44856          -0.091947          0.0    0.0  \n",
       "181703         -0.122331          0.0    0.0  \n",
       "174797         -0.075852          0.0    0.0  \n",
       "174798         -0.063308          0.0    0.0  \n",
       "174799         -0.084052          0.0    0.0  \n",
       "\n",
       "[2320 rows x 29 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2.loc[indexes].sort_values('date')#.loc[dataset_2['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>days_in_index</th>\n",
       "      <th>period_total_return</th>\n",
       "      <th>calmar_ratio</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>period_cumulative_return</th>\n",
       "      <th>total_cumulative_return</th>\n",
       "      <th>days_since_last_max</th>\n",
       "      <th>days_since_last_min</th>\n",
       "      <th>...</th>\n",
       "      <th>benchmark_distance</th>\n",
       "      <th>information_ratio</th>\n",
       "      <th>modigliani_measure</th>\n",
       "      <th>alfa_jensen</th>\n",
       "      <th>total_alfa_jensen</th>\n",
       "      <th>sharpe_ratio_next_period</th>\n",
       "      <th>returns_next_period</th>\n",
       "      <th>alfa_next_period</th>\n",
       "      <th>label</th>\n",
       "      <th>alfa_signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12267</th>\n",
       "      <td>ACS</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>5772.0</td>\n",
       "      <td>0.018905</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>-0.925131</td>\n",
       "      <td>0.056353</td>\n",
       "      <td>1.596892</td>\n",
       "      <td>615.0</td>\n",
       "      <td>7630.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015735</td>\n",
       "      <td>0.073656</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>0.032911</td>\n",
       "      <td>-0.071149</td>\n",
       "      <td>-0.035855</td>\n",
       "      <td>-0.041753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12268</th>\n",
       "      <td>ACS</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>5773.0</td>\n",
       "      <td>-0.012737</td>\n",
       "      <td>-0.001750</td>\n",
       "      <td>-0.925131</td>\n",
       "      <td>0.075258</td>\n",
       "      <td>1.613051</td>\n",
       "      <td>616.0</td>\n",
       "      <td>7631.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015963</td>\n",
       "      <td>0.126516</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.015015</td>\n",
       "      <td>0.068915</td>\n",
       "      <td>-0.128239</td>\n",
       "      <td>-0.036950</td>\n",
       "      <td>-0.073530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12269</th>\n",
       "      <td>ACS</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>5774.0</td>\n",
       "      <td>-0.062881</td>\n",
       "      <td>-0.001323</td>\n",
       "      <td>-0.925131</td>\n",
       "      <td>0.062520</td>\n",
       "      <td>1.634673</td>\n",
       "      <td>617.0</td>\n",
       "      <td>7632.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014820</td>\n",
       "      <td>0.038311</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>-0.078582</td>\n",
       "      <td>0.011093</td>\n",
       "      <td>-0.189008</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.014285</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12270</th>\n",
       "      <td>ACS</td>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>5775.0</td>\n",
       "      <td>-0.002153</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>-0.925131</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>1.623560</td>\n",
       "      <td>618.0</td>\n",
       "      <td>7633.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013931</td>\n",
       "      <td>-0.070089</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>-0.003656</td>\n",
       "      <td>0.025461</td>\n",
       "      <td>-0.186632</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12271</th>\n",
       "      <td>ACS</td>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>5776.0</td>\n",
       "      <td>0.040186</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>-0.925131</td>\n",
       "      <td>-0.002514</td>\n",
       "      <td>1.626080</td>\n",
       "      <td>619.0</td>\n",
       "      <td>7634.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>-0.075156</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.039716</td>\n",
       "      <td>0.028185</td>\n",
       "      <td>-0.172103</td>\n",
       "      <td>-0.003542</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203424</th>\n",
       "      <td>VIS_3</td>\n",
       "      <td>2021-09-20</td>\n",
       "      <td>1343.0</td>\n",
       "      <td>-0.008251</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>-0.397681</td>\n",
       "      <td>-0.013525</td>\n",
       "      <td>0.268172</td>\n",
       "      <td>403.0</td>\n",
       "      <td>1774.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009084</td>\n",
       "      <td>0.037924</td>\n",
       "      <td>-0.000566</td>\n",
       "      <td>-0.006615</td>\n",
       "      <td>0.024058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203425</th>\n",
       "      <td>VIS_3</td>\n",
       "      <td>2021-09-21</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>-0.023420</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>-0.397681</td>\n",
       "      <td>-0.021776</td>\n",
       "      <td>0.273265</td>\n",
       "      <td>404.0</td>\n",
       "      <td>1775.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>-0.020333</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>-0.024415</td>\n",
       "      <td>0.006423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203426</th>\n",
       "      <td>VIS_3</td>\n",
       "      <td>2021-09-22</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>-0.397681</td>\n",
       "      <td>-0.045196</td>\n",
       "      <td>0.262197</td>\n",
       "      <td>405.0</td>\n",
       "      <td>1776.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>-0.091476</td>\n",
       "      <td>-0.002054</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>0.019329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203427</th>\n",
       "      <td>VIS_3</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>1346.0</td>\n",
       "      <td>-0.012826</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>-0.397681</td>\n",
       "      <td>-0.032790</td>\n",
       "      <td>0.263908</td>\n",
       "      <td>406.0</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>-0.075031</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>-0.013671</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203428</th>\n",
       "      <td>VIS_3</td>\n",
       "      <td>2021-09-24</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.004578</td>\n",
       "      <td>-0.397681</td>\n",
       "      <td>-0.045616</td>\n",
       "      <td>0.252734</td>\n",
       "      <td>407.0</td>\n",
       "      <td>1778.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>-0.108680</td>\n",
       "      <td>-0.002097</td>\n",
       "      <td>0.005671</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20896 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ticker       date  days_in_index  period_total_return  calmar_ratio  \\\n",
       "12267     ACS 2021-01-04         5772.0             0.018905     -0.001122   \n",
       "12268     ACS 2021-01-05         5773.0            -0.012737     -0.001750   \n",
       "12269     ACS 2021-01-06         5774.0            -0.062881     -0.001323   \n",
       "12270     ACS 2021-01-07         5775.0            -0.002153      0.000943   \n",
       "12271     ACS 2021-01-08         5776.0             0.040186      0.001020   \n",
       "...       ...        ...            ...                  ...           ...   \n",
       "203424  VIS_3 2021-09-20         1343.0            -0.008251      0.001762   \n",
       "203425  VIS_3 2021-09-21         1344.0            -0.023420      0.002580   \n",
       "203426  VIS_3 2021-09-22         1345.0             0.012406      0.004543   \n",
       "203427  VIS_3 2021-09-23         1346.0            -0.012826      0.003629   \n",
       "203428  VIS_3 2021-09-24         1347.0             0.002407      0.004578   \n",
       "\n",
       "        max_drawdown  period_cumulative_return  total_cumulative_return  \\\n",
       "12267      -0.925131                  0.056353                 1.596892   \n",
       "12268      -0.925131                  0.075258                 1.613051   \n",
       "12269      -0.925131                  0.062520                 1.634673   \n",
       "12270      -0.925131                 -0.000360                 1.623560   \n",
       "12271      -0.925131                 -0.002514                 1.626080   \n",
       "...              ...                       ...                      ...   \n",
       "203424     -0.397681                 -0.013525                 0.268172   \n",
       "203425     -0.397681                 -0.021776                 0.273265   \n",
       "203426     -0.397681                 -0.045196                 0.262197   \n",
       "203427     -0.397681                 -0.032790                 0.263908   \n",
       "203428     -0.397681                 -0.045616                 0.252734   \n",
       "\n",
       "        days_since_last_max  days_since_last_min  ...  benchmark_distance  \\\n",
       "12267                 615.0               7630.0  ...            0.015735   \n",
       "12268                 616.0               7631.0  ...            0.015963   \n",
       "12269                 617.0               7632.0  ...            0.014820   \n",
       "12270                 618.0               7633.0  ...            0.013931   \n",
       "12271                 619.0               7634.0  ...            0.013926   \n",
       "...                     ...                  ...  ...                 ...   \n",
       "203424                403.0               1774.0  ...            0.009084   \n",
       "203425                404.0               1775.0  ...            0.008990   \n",
       "203426                405.0               1776.0  ...            0.009488   \n",
       "203427                406.0               1777.0  ...            0.009335   \n",
       "203428                407.0               1778.0  ...            0.009538   \n",
       "\n",
       "        information_ratio  modigliani_measure  alfa_jensen  total_alfa_jensen  \\\n",
       "12267            0.073656            0.001446     0.010740           0.032911   \n",
       "12268            0.126516            0.001824     0.015015           0.068915   \n",
       "12269            0.038311            0.001678    -0.078582           0.011093   \n",
       "12270           -0.070089            0.000217    -0.003656           0.025461   \n",
       "12271           -0.075156            0.000164     0.039716           0.028185   \n",
       "...                   ...                 ...          ...                ...   \n",
       "203424           0.037924           -0.000566    -0.006615           0.024058   \n",
       "203425          -0.020333           -0.001007    -0.024415           0.006423   \n",
       "203426          -0.091476           -0.002054     0.013174           0.019329   \n",
       "203427          -0.075031           -0.001625    -0.013671           0.005097   \n",
       "203428          -0.108680           -0.002097     0.005671           0.016992   \n",
       "\n",
       "        sharpe_ratio_next_period  returns_next_period  alfa_next_period  \\\n",
       "12267                  -0.071149            -0.035855         -0.041753   \n",
       "12268                  -0.128239            -0.036950         -0.073530   \n",
       "12269                  -0.189008            -0.000038         -0.014285   \n",
       "12270                  -0.186632             0.007442          0.016766   \n",
       "12271                  -0.172103            -0.003542         -0.002900   \n",
       "...                          ...                  ...               ...   \n",
       "203424                  0.000000             0.000000          0.000000   \n",
       "203425                  0.000000             0.000000          0.000000   \n",
       "203426                  0.000000             0.000000          0.000000   \n",
       "203427                  0.000000             0.000000          0.000000   \n",
       "203428                  0.000000             0.000000          0.000000   \n",
       "\n",
       "        label  alfa_signal  \n",
       "12267     1.0          1.0  \n",
       "12268     1.0          1.0  \n",
       "12269     1.0          0.0  \n",
       "12270     1.0          0.0  \n",
       "12271     1.0          1.0  \n",
       "...       ...          ...  \n",
       "203424    0.0          0.0  \n",
       "203425    0.0          0.0  \n",
       "203426    0.0          1.0  \n",
       "203427    0.0          0.0  \n",
       "203428    0.0          1.0  \n",
       "\n",
       "[20896 rows x 30 columns]"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_results = dataset.iloc[predictions['index']].sort_index()\n",
    "dataset_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1618\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1617\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1618\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_take_with_is_copy(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1619\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1620\u001b[0m     \u001b[39m# re-raise with different error message\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\pandas\\core\\generic.py:3948\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3941\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3942\u001b[0m \u001b[39mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[39mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3946\u001b[0m \u001b[39mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3947\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3948\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3949\u001b[0m \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\pandas\\core\\generic.py:3932\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3930\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 3932\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m   3933\u001b[0m     indices,\n\u001b[0;32m   3934\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[0;32m   3935\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   3936\u001b[0m     convert_indices\u001b[39m=\u001b[39;49mconvert_indices,\n\u001b[0;32m   3937\u001b[0m )\n\u001b[0;32m   3938\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:960\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[39mif\u001b[39;00m convert_indices:\n\u001b[1;32m--> 960\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39;49mverify)\n\u001b[0;32m    962\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\pandas\\core\\indexers\\utils.py:284\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n, verify)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m--> 284\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mindices are out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m indices\n",
      "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m portfolios \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39miloc[dataset\u001b[39m.\u001b[39;49miloc[predictions[\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39mindex]\u001b[39m.\u001b[39msort_index()[[\u001b[39m'\u001b[39m\u001b[39mticker\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m portfolios\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m portfolios\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1100\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1102\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1645\u001b[0m \u001b[39m# a list of integers\u001b[39;00m\n\u001b[0;32m   1646\u001b[0m \u001b[39melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_list_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1649\u001b[0m \u001b[39m# a single integer\u001b[39;00m\n\u001b[0;32m   1650\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1651\u001b[0m     key \u001b[39m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\Documents\\GitHub\\TFM-Algoritmo-con-redes-evolutivas\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1621\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1618\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_take_with_is_copy(key, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m   1619\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1620\u001b[0m     \u001b[39m# re-raise with different error message\u001b[39;00m\n\u001b[1;32m-> 1621\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpositional indexers are out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "portfolios = dataset.iloc[dataset.iloc[predictions['index']].index].sort_index()[['ticker', 'date', 'label']]\n",
    "portfolios.set_index('date', inplace=True, drop=True)\n",
    "portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios = pd.get_dummies(portfolios, dtype=int).groupby('date').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
