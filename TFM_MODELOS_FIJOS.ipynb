{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tintenderete/TFM-Algoritmo-con-redes-evolutivas/blob/main/TFM_MODELOS_FIJOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2lLfxoRGrAF"
      },
      "source": [
        "# DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jbkv6mZOO3Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def get_h_price():\n",
        "\n",
        "    symbols = [\n",
        "    \"ANA.MC\", \"ACX.MC\", \"ACS.MC\", \"AENA.MC\", \"AMS.MC\", \"MTS.MC\", \"SAB.MC\", \"BKIA.MC\", \"BKT.MC\", \"BBVA.MC\", \"CABK.MC\",\n",
        "    \"CLNX.MC\", \"CIE.MC\", \"ENG.MC\", \"ELE.MC\", \"FER.MC\", \"GRF.MC\", \"IAG.MC\", \"IBE.MC\", \"ITX.MC\", \"IDR.MC\", \"COL.MC\",\n",
        "    \"MEL.MC\", \"MRL.MC\", \"NTGY.MC\", \"PHM.MC\", \"REE.MC\", \"REP.MC\", \"SGRE.MC\", \"SLR.MC\", \"TRE.MC\", \"TEF.MC\", \"VIS.MC\",\n",
        "    \"VWS.CO\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Descargar datos\n",
        "    start_date = \"2016-01-01\"\n",
        "    end_date = \"2023-01-01\"\n",
        "\n",
        "\n",
        "    data = yf.download(symbols, start=start_date, end=end_date)\n",
        "\n",
        "\n",
        "    adj_close = data['Adj Close']\n",
        "\n",
        "\n",
        "    adj_close = adj_close.drop([\"BKIA.MC\", \"REE.MC\", \"SGRE.MC\"  ], axis = 1)\n",
        "\n",
        "\n",
        "    adj_close = adj_close.fillna(method=\"ffill\")\n",
        "\n",
        "    return adj_close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXiZjpYrOVWZ"
      },
      "outputs": [],
      "source": [
        "def calculate_sharpe_ratio(returns, risk_free_rate):\n",
        "    \"\"\"\n",
        "    Calcular el ratio de Sharpe\n",
        "\n",
        "    Par치metros:\n",
        "    returns (np.array): Array de rendimientos de la inversi칩n\n",
        "    risk_free_rate (float): Tasa de rendimiento sin riesgo\n",
        "\n",
        "    Devuelve:\n",
        "    sharpe_ratio (float): Ratio de Sharpe\n",
        "    \"\"\"\n",
        "\n",
        "    # Calcular el rendimiento promedio\n",
        "    avg_returns = np.mean(returns)\n",
        "\n",
        "    # Calcular la desviaci칩n est치ndar de los rendimientos\n",
        "    std_returns = np.std(returns)\n",
        "\n",
        "    # Calcular el ratio de Sharpe\n",
        "    sharpe_ratio = (avg_returns - risk_free_rate) / std_returns\n",
        "\n",
        "    return sharpe_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKUXhG5HCnL"
      },
      "outputs": [],
      "source": [
        "def h_price_to_data(h_price, days_backward = 150, days_forward = 30, days_steps = 1  ):\n",
        "    datos_analisis = h_price\n",
        "    X_DATA = []\n",
        "    Y_DATA = []\n",
        "\n",
        "    for i in range(days_backward, len(datos_analisis), days_steps):\n",
        "        # retornos logaritmicos\n",
        "        X_data = datos_analisis[i-days_backward:i]\n",
        "        X_data = np.log(X_data).diff().dropna()\n",
        "        X_DATA.append(X_data)\n",
        "\n",
        "        # y\n",
        "        data_forward = datos_analisis[i:i+days_forward]\n",
        "\n",
        "        rs = calculate_sharpe_ratio(np.log(data_forward).diff().dropna(), 0)\n",
        "        Y_data = np.argsort(np.argsort(rs))\n",
        "        Y_DATA.append(Y_data)\n",
        "\n",
        "    X_DATA = np.array(X_DATA)\n",
        "    Y_DATA = np.array(Y_DATA)\n",
        "\n",
        "    return X_DATA, Y_DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5eb_vzJHER4"
      },
      "source": [
        "# X e Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACypR4ePHGp3",
        "outputId": "d73ff135-5624-4faf-f5e2-fbde16948b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  34 of 34 completed"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:yfinance:\n",
            "3 Failed downloads:\n",
            "ERROR:yfinance:['BKIA.MC', 'REE.MC', 'SGRE.MC']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ibex_data = get_h_price()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCPimtXFHLoG",
        "outputId": "9c081843-4ea0-446f-94ed-e5e0305ce8a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "X, Y = h_price_to_data(ibex_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA8g7KwbHOAs",
        "outputId": "759d8aa4-b8dc-487e-bb00-a58ad94cddc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0]), array([30]))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r_min = np.min(Y, axis=1)[:, np.newaxis]\n",
        "r_max = np.max(Y, axis=1)[:, np.newaxis]\n",
        "r_min[-1], r_max[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_t5FNhcHSio"
      },
      "outputs": [],
      "source": [
        "Y_NORM = 2 * (Y - 0) / (30 - 0) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt7-WuoiHVJ2"
      },
      "source": [
        "# TRAIN AND VALID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ0PQjGWHYx7",
        "outputId": "31424ad3-f134-4d54-a5fb-f19d3de96df8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1647, 149, 31)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avR-NUbzHWfe"
      },
      "outputs": [],
      "source": [
        "X_TRAIN = X[:800]\n",
        "Y_TRAIN = Y_NORM[:800]\n",
        "\n",
        "X_VALID = X[800:1000]\n",
        "Y_VALID = Y_NORM[800:1000]\n",
        "\n",
        "X_TEST = X[1000:]\n",
        "Y_TEST = Y_NORM[1000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK4DQXm0Gx7y"
      },
      "source": [
        "# GUARDAR CARGAR MODELOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9vhxVSC4GnBR",
        "outputId": "457f76a9-2e32-4e12-bdf8-f403929a6f37"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Guardar el modelo\\nmodel.save('my_model.h5')\\n\\n# Cargar el modelo\\nfrom keras.models import load_model\\nloaded_model = load_model('my_model.h5')\\n\\n\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Guardar el modelo\n",
        "model.save('my_model.h5')\n",
        "\n",
        "# Cargar el modelo\n",
        "from keras.models import load_model\n",
        "loaded_model = load_model('my_model.h5')\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5fWfMEGG47g"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Amprm6UHcOy"
      },
      "source": [
        "# ---- MODELOS  ----- > > >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "469aQVV_O2DN"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "hp_top = 10\n",
        "hp_incremento_top = 2\n",
        "def top_is_target_31(y_true, y_pred ):\n",
        "  num_samples = K.shape(y_true)[0]\n",
        "\n",
        "  w = K.arange(31, dtype='float32')\n",
        "  w = K.reverse(w, axes=0) + 1\n",
        "\n",
        "  w = tf.where(K.arange(31) < hp_top, w * hp_incremento_top, w)\n",
        "  # Replicar w a lo largo del eje 0 (batch)\n",
        "  #w = K.repeat_elements(K.expand_dims(w, 0), num_samples, axis=0)\n",
        "  w = tf.tile(K.expand_dims(w, 0), [num_samples, 1])\n",
        "\n",
        "  # Calcular la p칠rdida\n",
        "  r = K.cast(y_true, 'float32')\n",
        "  r_pred = K.cast(y_pred, 'float32')\n",
        "\n",
        "  return K.sum(w * K.square(r - r_pred), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aod3OFbTOtxp"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from tensorflow.keras.layers import Dense, GRU, Dropout, Flatten\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSfITz1BRGui"
      },
      "source": [
        "# 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNA9dDmzHgOG",
        "outputId": "6a71e799-0645-40b1-b001-3e27116a0306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 149, 31)]         0         \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 10)                1290      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 10)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 31)                341       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,631\n",
            "Trainable params: 1,631\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(X_TRAIN.shape[1:]))\n",
        "m = inputs\n",
        "\n",
        "mA = GRU(units=10)(m)\n",
        "\n",
        "\n",
        "m = Flatten()(mA)\n",
        "\n",
        "# A침adir capa Dense de salida\n",
        "out = Dense(Y_TRAIN.shape[1], activation='tanh')(m)\n",
        "\n",
        "model_GRU = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_GRU.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse',\n",
        "        metrics=[])\n",
        "\n",
        "model_GRU.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq-6mfvOO5-d",
        "outputId": "da58eb31-acd8-4b4b-c10e-4cdc17b08e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3556 - val_loss: 0.3566\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.3522 - val_loss: 0.3581\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.3489 - val_loss: 0.3608\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.3458 - val_loss: 0.3644\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 0.3433 - val_loss: 0.3687\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 1s 578ms/step - loss: 0.3415 - val_loss: 0.3716\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.3403 - val_loss: 0.3721\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 0.3395 - val_loss: 0.3704\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.3388 - val_loss: 0.3673\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.3382 - val_loss: 0.3638\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.3376 - val_loss: 0.3606\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.3373 - val_loss: 0.3580\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.3371 - val_loss: 0.3561\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 0.3370 - val_loss: 0.3548\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.3369 - val_loss: 0.3541\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.3369 - val_loss: 0.3537\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.3368 - val_loss: 0.3537\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.3367 - val_loss: 0.3540\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.3366 - val_loss: 0.3546\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.3365 - val_loss: 0.3554\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.3364 - val_loss: 0.3563\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3363 - val_loss: 0.3572\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.3361 - val_loss: 0.3581\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.3359 - val_loss: 0.3588\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3357 - val_loss: 0.3594\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.3354 - val_loss: 0.3600\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3352 - val_loss: 0.3606\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3350 - val_loss: 0.3613\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.3347 - val_loss: 0.3621\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.3344 - val_loss: 0.3627\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.3340 - val_loss: 0.3632\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.3335 - val_loss: 0.3642\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3330 - val_loss: 0.3651\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.3323 - val_loss: 0.3656\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3315 - val_loss: 0.3729\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.3313 - val_loss: 0.3616\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.3337 - val_loss: 0.3682\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.3293 - val_loss: 0.3851\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3328 - val_loss: 0.3698\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.3284 - val_loss: 0.3642\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3294 - val_loss: 0.3632\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.3299 - val_loss: 0.3645\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.3289 - val_loss: 0.3677\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.3277 - val_loss: 0.3724\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3273 - val_loss: 0.3769\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.3275 - val_loss: 0.3786\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.3271 - val_loss: 0.3767\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.3256 - val_loss: 0.3736\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.3244 - val_loss: 0.3728\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.3243 - val_loss: 0.3768\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3229 - val_loss: 0.3914\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.3224 - val_loss: 0.3928\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.3218 - val_loss: 0.3831\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.3232 - val_loss: 0.3873\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.3205 - val_loss: 0.3961\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3217 - val_loss: 0.3863\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3199 - val_loss: 0.3812\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3205 - val_loss: 0.3813\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.3202 - val_loss: 0.3855\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.3192 - val_loss: 0.3917\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3191 - val_loss: 0.3917\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.3181 - val_loss: 0.3869\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.3175 - val_loss: 0.3895\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.3171 - val_loss: 0.4049\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.3182 - val_loss: 0.3845\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.3192 - val_loss: 0.3848\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.3168 - val_loss: 0.3946\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.3162 - val_loss: 0.3987\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.3174 - val_loss: 0.3906\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.3156 - val_loss: 0.3850\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.3158 - val_loss: 0.3843\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.3158 - val_loss: 0.3885\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.3140 - val_loss: 0.3987\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.3134 - val_loss: 0.4031\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.3132 - val_loss: 0.3918\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3122 - val_loss: 0.3921\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3123 - val_loss: 0.4073\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3113 - val_loss: 0.4004\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.3097 - val_loss: 0.3954\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.3096 - val_loss: 0.3991\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.3085 - val_loss: 0.4062\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.3083 - val_loss: 0.4038\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.3071 - val_loss: 0.4019\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.3064 - val_loss: 0.4099\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.3051 - val_loss: 0.4211\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.3047 - val_loss: 0.4078\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.3049 - val_loss: 0.4270\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.3028 - val_loss: 0.4271\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3021 - val_loss: 0.4164\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.3024 - val_loss: 0.4311\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.3010 - val_loss: 0.4269\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3000 - val_loss: 0.4193\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.2995 - val_loss: 0.4293\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2988 - val_loss: 0.4166\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.2982 - val_loss: 0.4255\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.2973 - val_loss: 0.4231\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2964 - val_loss: 0.4299\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.2957 - val_loss: 0.4329\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.2949 - val_loss: 0.4373\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.2942 - val_loss: 0.4414\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.2937 - val_loss: 0.4425\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2930 - val_loss: 0.4489\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.2923 - val_loss: 0.4279\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.2954 - val_loss: 0.4747\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3069 - val_loss: 0.4323\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.3073 - val_loss: 0.4328\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.3162 - val_loss: 0.4280\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3141 - val_loss: 0.4264\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.3096 - val_loss: 0.4289\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3058 - val_loss: 0.4341\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.3045 - val_loss: 0.4384\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.3041 - val_loss: 0.4402\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3041 - val_loss: 0.4410\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.3050 - val_loss: 0.4421\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.3052 - val_loss: 0.4433\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.3044 - val_loss: 0.4429\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.3037 - val_loss: 0.4394\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.3023 - val_loss: 0.4346\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.3003 - val_loss: 0.4312\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.2989 - val_loss: 0.4299\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2976 - val_loss: 0.4298\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.2966 - val_loss: 0.4290\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.2958 - val_loss: 0.4273\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.2950 - val_loss: 0.4270\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2945 - val_loss: 0.4293\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.2935 - val_loss: 0.4334\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.2926 - val_loss: 0.4357\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.2914 - val_loss: 0.4356\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.2901 - val_loss: 0.4372\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.2888 - val_loss: 0.4404\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.2879 - val_loss: 0.4410\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.2869 - val_loss: 0.4408\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2864 - val_loss: 0.4430\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2858 - val_loss: 0.4455\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.2854 - val_loss: 0.4459\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.2846 - val_loss: 0.4467\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.2839 - val_loss: 0.4493\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.2828 - val_loss: 0.4516\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2820 - val_loss: 0.4516\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2812 - val_loss: 0.4528\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2805 - val_loss: 0.4555\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2799 - val_loss: 0.4561\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2793 - val_loss: 0.4565\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2788 - val_loss: 0.4599\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.2781 - val_loss: 0.4622\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.2775 - val_loss: 0.4632\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.2769 - val_loss: 0.4672\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.2763 - val_loss: 0.4672\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.2756 - val_loss: 0.4676\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2751 - val_loss: 0.4691\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.2746 - val_loss: 0.4668\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.2741 - val_loss: 0.4674\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2736 - val_loss: 0.4676\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.2731 - val_loss: 0.4667\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.2726 - val_loss: 0.4686\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.2720 - val_loss: 0.4693\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.2715 - val_loss: 0.4702\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.2710 - val_loss: 0.4730\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.2706 - val_loss: 0.4722\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.2701 - val_loss: 0.4744\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.2697 - val_loss: 0.4719\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.2692 - val_loss: 0.4739\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.2687 - val_loss: 0.4715\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.2682 - val_loss: 0.4748\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2677 - val_loss: 0.4730\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.2672 - val_loss: 0.4768\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.2668 - val_loss: 0.4751\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.2662 - val_loss: 0.4791\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.2658 - val_loss: 0.4775\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.2653 - val_loss: 0.4825\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.2652 - val_loss: 0.4789\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.2653 - val_loss: 0.4851\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.2665 - val_loss: 0.4783\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.2649 - val_loss: 0.4810\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2629 - val_loss: 0.4833\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.2624 - val_loss: 0.4841\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.2630 - val_loss: 0.4904\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.2628 - val_loss: 0.4904\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.2608 - val_loss: 0.4926\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2600 - val_loss: 0.4953\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.2606 - val_loss: 0.4945\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.2607 - val_loss: 0.4958\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.2606 - val_loss: 0.4944\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.2586 - val_loss: 0.4947\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.2578 - val_loss: 0.4979\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2584 - val_loss: 0.4993\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2583 - val_loss: 0.5020\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.2573 - val_loss: 0.5026\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.2559 - val_loss: 0.5045\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.2554 - val_loss: 0.5061\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.2556 - val_loss: 0.5065\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.2560 - val_loss: 0.5073\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.2573 - val_loss: 0.5066\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.2559 - val_loss: 0.5036\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2548 - val_loss: 0.5056\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.2558 - val_loss: 0.5076\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.2547 - val_loss: 0.5102\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.2525 - val_loss: 0.5120\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.2538 - val_loss: 0.5112\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.2547 - val_loss: 0.5160\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.2563 - val_loss: 0.5128\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.2524 - val_loss: 0.5042\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.2557 - val_loss: 0.5054\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2549 - val_loss: 0.5049\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.2508 - val_loss: 0.5065\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.2544 - val_loss: 0.5054\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2494 - val_loss: 0.5055\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2518 - val_loss: 0.5034\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2492 - val_loss: 0.5041\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2496 - val_loss: 0.5081\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2486 - val_loss: 0.5074\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2485 - val_loss: 0.5045\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2476 - val_loss: 0.5063\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.2469 - val_loss: 0.5101\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.2470 - val_loss: 0.5091\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.2460 - val_loss: 0.5066\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.2462 - val_loss: 0.5087\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.2444 - val_loss: 0.5108\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.2449 - val_loss: 0.5098\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.2436 - val_loss: 0.5085\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.2442 - val_loss: 0.5120\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.2428 - val_loss: 0.5130\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.2428 - val_loss: 0.5131\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.2418 - val_loss: 0.5141\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.2417 - val_loss: 0.5152\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2413 - val_loss: 0.5154\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.2405 - val_loss: 0.5128\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.2404 - val_loss: 0.5141\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.2394 - val_loss: 0.5138\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2393 - val_loss: 0.5140\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.2387 - val_loss: 0.5134\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2385 - val_loss: 0.5173\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.2390 - val_loss: 0.5134\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.2412 - val_loss: 0.5156\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.2379 - val_loss: 0.5225\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2430 - val_loss: 0.5161\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.2410 - val_loss: 0.5145\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.2452 - val_loss: 0.5167\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.2427 - val_loss: 0.5182\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.2396 - val_loss: 0.5202\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.2399 - val_loss: 0.5230\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.2414 - val_loss: 0.5238\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.2402 - val_loss: 0.5229\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.2378 - val_loss: 0.5216\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.2373 - val_loss: 0.5180\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2374 - val_loss: 0.5135\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.2365 - val_loss: 0.5126\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.2348 - val_loss: 0.5149\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.2345 - val_loss: 0.5183\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.2343 - val_loss: 0.5207\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.2331 - val_loss: 0.5204\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.2330 - val_loss: 0.5184\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.2323 - val_loss: 0.5187\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.2311 - val_loss: 0.5214\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.2314 - val_loss: 0.5227\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.2307 - val_loss: 0.5234\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.2301 - val_loss: 0.5251\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.2300 - val_loss: 0.5276\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.2293 - val_loss: 0.5292\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.2293 - val_loss: 0.5285\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.2281 - val_loss: 0.5284\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.2280 - val_loss: 0.5303\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.2277 - val_loss: 0.5327\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2273 - val_loss: 0.5338\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.2266 - val_loss: 0.5345\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.2264 - val_loss: 0.5353\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.2260 - val_loss: 0.5357\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2256 - val_loss: 0.5354\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.2252 - val_loss: 0.5350\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2248 - val_loss: 0.5355\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2244 - val_loss: 0.5371\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2239 - val_loss: 0.5388\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.2236 - val_loss: 0.5391\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2233 - val_loss: 0.5386\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.2228 - val_loss: 0.5383\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.2225 - val_loss: 0.5380\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.2222 - val_loss: 0.5378\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.2217 - val_loss: 0.5390\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.2214 - val_loss: 0.5412\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.2211 - val_loss: 0.5416\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.2206 - val_loss: 0.5420\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.2204 - val_loss: 0.5421\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.2202 - val_loss: 0.5407\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.2197 - val_loss: 0.5405\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2193 - val_loss: 0.5416\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.2191 - val_loss: 0.5415\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2187 - val_loss: 0.5424\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.2184 - val_loss: 0.5424\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.2182 - val_loss: 0.5409\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2180 - val_loss: 0.5412\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.2177 - val_loss: 0.5407\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.2172 - val_loss: 0.5410\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.2168 - val_loss: 0.5423\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.2167 - val_loss: 0.5420\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.2164 - val_loss: 0.5421\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.2162 - val_loss: 0.5419\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.2156 - val_loss: 0.5410\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2152 - val_loss: 0.5412\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.2151 - val_loss: 0.5416\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.2148 - val_loss: 0.5419\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2146 - val_loss: 0.5426\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.2142 - val_loss: 0.5430\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.2139 - val_loss: 0.5428\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.2136 - val_loss: 0.5430\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2133 - val_loss: 0.5428\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.2130 - val_loss: 0.5427\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.2128 - val_loss: 0.5433\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.2125 - val_loss: 0.5431\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.2121 - val_loss: 0.5434\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.2118 - val_loss: 0.5436\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.2116 - val_loss: 0.5435\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2113 - val_loss: 0.5445\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2110 - val_loss: 0.5448\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.2107 - val_loss: 0.5452\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.2105 - val_loss: 0.5456\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.2102 - val_loss: 0.5454\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.2100 - val_loss: 0.5462\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.2097 - val_loss: 0.5467\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.2094 - val_loss: 0.5476\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.2091 - val_loss: 0.5486\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.2088 - val_loss: 0.5488\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.2086 - val_loss: 0.5496\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.2083 - val_loss: 0.5497\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.2080 - val_loss: 0.5507\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.2078 - val_loss: 0.5512\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.2075 - val_loss: 0.5522\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.2072 - val_loss: 0.5526\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.2070 - val_loss: 0.5531\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.2067 - val_loss: 0.5539\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2064 - val_loss: 0.5545\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.2062 - val_loss: 0.5556\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.2059 - val_loss: 0.5561\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.2056 - val_loss: 0.5571\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.2054 - val_loss: 0.5572\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.2052 - val_loss: 0.5588\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.2050 - val_loss: 0.5583\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.2052 - val_loss: 0.5613\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2061 - val_loss: 0.5577\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.2090 - val_loss: 0.5630\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2105 - val_loss: 0.5582\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.2082 - val_loss: 0.5603\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2050 - val_loss: 0.5625\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.2091 - val_loss: 0.5606\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.2060 - val_loss: 0.5646\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.2061 - val_loss: 0.5617\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.2100 - val_loss: 0.5651\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.2071 - val_loss: 0.5682\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.2050 - val_loss: 0.5682\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2040 - val_loss: 0.5597\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.2044 - val_loss: 0.5552\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.2045 - val_loss: 0.5578\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.2025 - val_loss: 0.5604\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.2038 - val_loss: 0.5612\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.2023 - val_loss: 0.5636\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2033 - val_loss: 0.5697\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2016 - val_loss: 0.5751\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2018 - val_loss: 0.5748\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2013 - val_loss: 0.5705\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.2010 - val_loss: 0.5683\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.2010 - val_loss: 0.5719\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1999 - val_loss: 0.5776\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.2006 - val_loss: 0.5801\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.1994 - val_loss: 0.5811\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1995 - val_loss: 0.5827\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.1994 - val_loss: 0.5851\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1986 - val_loss: 0.5873\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1988 - val_loss: 0.5883\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1982 - val_loss: 0.5880\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.1981 - val_loss: 0.5885\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1977 - val_loss: 0.5897\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.1976 - val_loss: 0.5889\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1972 - val_loss: 0.5880\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1969 - val_loss: 0.5898\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1969 - val_loss: 0.5938\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1963 - val_loss: 0.5965\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1964 - val_loss: 0.5955\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1959 - val_loss: 0.5914\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1958 - val_loss: 0.5898\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1955 - val_loss: 0.5913\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.1953 - val_loss: 0.5930\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1950 - val_loss: 0.5945\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1948 - val_loss: 0.5948\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.1945 - val_loss: 0.5939\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1943 - val_loss: 0.5925\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1941 - val_loss: 0.5903\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.1938 - val_loss: 0.5904\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1936 - val_loss: 0.5930\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.1933 - val_loss: 0.5942\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1931 - val_loss: 0.5943\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1929 - val_loss: 0.5932\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1927 - val_loss: 0.5931\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1924 - val_loss: 0.5941\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1922 - val_loss: 0.5937\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1919 - val_loss: 0.5946\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1917 - val_loss: 0.5949\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1915 - val_loss: 0.5962\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1913 - val_loss: 0.5959\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.1910 - val_loss: 0.5957\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1908 - val_loss: 0.5972\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1906 - val_loss: 0.5983\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1904 - val_loss: 0.5991\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.1901 - val_loss: 0.5982\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1899 - val_loss: 0.5993\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1897 - val_loss: 0.5977\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.1895 - val_loss: 0.6002\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1893 - val_loss: 0.5992\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1892 - val_loss: 0.6046\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.1892 - val_loss: 0.5977\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1894 - val_loss: 0.6079\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1898 - val_loss: 0.5967\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1909 - val_loss: 0.6107\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1899 - val_loss: 0.6002\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1886 - val_loss: 0.6038\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.1877 - val_loss: 0.6075\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1883 - val_loss: 0.5960\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.1892 - val_loss: 0.6059\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.1879 - val_loss: 0.6034\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.1869 - val_loss: 0.5994\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.1875 - val_loss: 0.6072\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.1876 - val_loss: 0.5996\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.1866 - val_loss: 0.6020\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.1864 - val_loss: 0.6093\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.1868 - val_loss: 0.5981\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.1864 - val_loss: 0.6020\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.1857 - val_loss: 0.6086\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.1859 - val_loss: 0.6018\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.1860 - val_loss: 0.6081\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1852 - val_loss: 0.6132\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1854 - val_loss: 0.6054\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1852 - val_loss: 0.6112\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1847 - val_loss: 0.6136\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1846 - val_loss: 0.6074\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1846 - val_loss: 0.6149\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1844 - val_loss: 0.6123\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1839 - val_loss: 0.6102\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1840 - val_loss: 0.6178\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1840 - val_loss: 0.6129\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1836 - val_loss: 0.6163\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.1835 - val_loss: 0.6179\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.1833 - val_loss: 0.6155\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1832 - val_loss: 0.6203\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.1830 - val_loss: 0.6209\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1827 - val_loss: 0.6196\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.1826 - val_loss: 0.6237\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1825 - val_loss: 0.6207\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1824 - val_loss: 0.6253\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1821 - val_loss: 0.6226\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1819 - val_loss: 0.6240\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1818 - val_loss: 0.6260\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1816 - val_loss: 0.6232\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1816 - val_loss: 0.6281\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.1814 - val_loss: 0.6247\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1812 - val_loss: 0.6272\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1811 - val_loss: 0.6277\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1809 - val_loss: 0.6268\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1808 - val_loss: 0.6304\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1809 - val_loss: 0.6219\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.1815 - val_loss: 0.6360\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.1842 - val_loss: 0.6074\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.1846 - val_loss: 0.6145\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 0.1827 - val_loss: 0.6135\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.1814 - val_loss: 0.6042\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 0.1848 - val_loss: 0.6052\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.1837 - val_loss: 0.6102\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.1817 - val_loss: 0.6028\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 382ms/step - loss: 0.1830 - val_loss: 0.6102\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 1s 615ms/step - loss: 0.1810 - val_loss: 0.6206\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 386ms/step - loss: 0.1816 - val_loss: 0.6099\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.1806 - val_loss: 0.6059\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.1807 - val_loss: 0.6169\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 1s 786ms/step - loss: 0.1808 - val_loss: 0.6063\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.1803 - val_loss: 0.6086\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.1796 - val_loss: 0.6200\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1802 - val_loss: 0.6137\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.1804 - val_loss: 0.6104\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.1805 - val_loss: 0.6146\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.1795 - val_loss: 0.6224\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1794 - val_loss: 0.6279\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1804 - val_loss: 0.6226\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1784 - val_loss: 0.6131\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1790 - val_loss: 0.6176\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1786 - val_loss: 0.6243\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1781 - val_loss: 0.6192\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1778 - val_loss: 0.6186\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1780 - val_loss: 0.6220\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1773 - val_loss: 0.6296\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1776 - val_loss: 0.6250\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1773 - val_loss: 0.6227\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1772 - val_loss: 0.6311\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1768 - val_loss: 0.6261\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1770 - val_loss: 0.6232\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1764 - val_loss: 0.6284\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1766 - val_loss: 0.6241\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1761 - val_loss: 0.6290\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.1760 - val_loss: 0.6340\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.1760 - val_loss: 0.6201\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.1759 - val_loss: 0.6256\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.1756 - val_loss: 0.6330\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.1755 - val_loss: 0.6255\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.1753 - val_loss: 0.6257\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.1753 - val_loss: 0.6257\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.1749 - val_loss: 0.6302\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1749 - val_loss: 0.6310\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.1748 - val_loss: 0.6263\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1746 - val_loss: 0.6310\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.1744 - val_loss: 0.6318\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1744 - val_loss: 0.6291\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.1742 - val_loss: 0.6321\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1741 - val_loss: 0.6304\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1738 - val_loss: 0.6329\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1737 - val_loss: 0.6322\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1736 - val_loss: 0.6301\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1735 - val_loss: 0.6342\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.1734 - val_loss: 0.6326\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1732 - val_loss: 0.6303\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1731 - val_loss: 0.6309\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1729 - val_loss: 0.6332\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.1727 - val_loss: 0.6312\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1725 - val_loss: 0.6290\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.1724 - val_loss: 0.6329\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1723 - val_loss: 0.6300\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1722 - val_loss: 0.6308\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1721 - val_loss: 0.6284\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1720 - val_loss: 0.6325\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1720 - val_loss: 0.6274\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.1721 - val_loss: 0.6328\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1726 - val_loss: 0.6263\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1736 - val_loss: 0.6331\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.1758 - val_loss: 0.6251\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.1753 - val_loss: 0.6232\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.1732 - val_loss: 0.6383\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.1718 - val_loss: 0.6202\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.1737 - val_loss: 0.6245\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.1754 - val_loss: 0.6190\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.1711 - val_loss: 0.6117\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.1760 - val_loss: 0.6296\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.1780 - val_loss: 0.6061\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.1756 - val_loss: 0.6013\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.1763 - val_loss: 0.6088\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.1737 - val_loss: 0.6103\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1738 - val_loss: 0.6011\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1738 - val_loss: 0.5986\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.1731 - val_loss: 0.6033\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1725 - val_loss: 0.6057\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1726 - val_loss: 0.6007\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1716 - val_loss: 0.5970\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1719 - val_loss: 0.5999\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1712 - val_loss: 0.6055\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1707 - val_loss: 0.6057\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1708 - val_loss: 0.6017\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1701 - val_loss: 0.6048\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1704 - val_loss: 0.6124\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.1697 - val_loss: 0.6171\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1699 - val_loss: 0.6136\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.1691 - val_loss: 0.6110\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1693 - val_loss: 0.6148\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.1689 - val_loss: 0.6198\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.1689 - val_loss: 0.6180\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1686 - val_loss: 0.6145\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.1684 - val_loss: 0.6152\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1683 - val_loss: 0.6188\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.1681 - val_loss: 0.6200\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1679 - val_loss: 0.6192\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1677 - val_loss: 0.6183\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1677 - val_loss: 0.6201\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1674 - val_loss: 0.6215\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1675 - val_loss: 0.6208\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1670 - val_loss: 0.6206\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1672 - val_loss: 0.6201\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1668 - val_loss: 0.6228\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1669 - val_loss: 0.6222\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1666 - val_loss: 0.6228\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.1665 - val_loss: 0.6237\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.1663 - val_loss: 0.6247\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1663 - val_loss: 0.6245\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1660 - val_loss: 0.6245\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.1660 - val_loss: 0.6259\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1658 - val_loss: 0.6244\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1658 - val_loss: 0.6258\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1657 - val_loss: 0.6247\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1658 - val_loss: 0.6278\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1658 - val_loss: 0.6258\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1661 - val_loss: 0.6314\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1667 - val_loss: 0.6241\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1675 - val_loss: 0.6324\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1671 - val_loss: 0.6288\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1658 - val_loss: 0.6306\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1648 - val_loss: 0.6318\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1650 - val_loss: 0.6280\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1659 - val_loss: 0.6314\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1662 - val_loss: 0.6268\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1657 - val_loss: 0.6336\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1649 - val_loss: 0.6302\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1642 - val_loss: 0.6322\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1642 - val_loss: 0.6348\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1645 - val_loss: 0.6282\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1649 - val_loss: 0.6345\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1644 - val_loss: 0.6318\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1638 - val_loss: 0.6326\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1634 - val_loss: 0.6356\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.1634 - val_loss: 0.6328\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.1635 - val_loss: 0.6344\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.1637 - val_loss: 0.6319\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.1641 - val_loss: 0.6383\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.1643 - val_loss: 0.6312\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.1644 - val_loss: 0.6412\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.1636 - val_loss: 0.6347\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.1629 - val_loss: 0.6359\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.1625 - val_loss: 0.6384\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.1625 - val_loss: 0.6336\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.1627 - val_loss: 0.6397\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.1631 - val_loss: 0.6369\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1637 - val_loss: 0.6399\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1641 - val_loss: 0.6330\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.1637 - val_loss: 0.6439\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1625 - val_loss: 0.6353\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.1619 - val_loss: 0.6391\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.1620 - val_loss: 0.6436\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.1628 - val_loss: 0.6332\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1636 - val_loss: 0.6442\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.1628 - val_loss: 0.6401\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1617 - val_loss: 0.6367\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.1612 - val_loss: 0.6423\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1613 - val_loss: 0.6371\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1616 - val_loss: 0.6413\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1619 - val_loss: 0.6413\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.1619 - val_loss: 0.6431\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1612 - val_loss: 0.6411\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1606 - val_loss: 0.6439\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1604 - val_loss: 0.6399\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1605 - val_loss: 0.6403\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.1607 - val_loss: 0.6452\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1612 - val_loss: 0.6369\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1618 - val_loss: 0.6517\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1620 - val_loss: 0.6385\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1616 - val_loss: 0.6439\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1601 - val_loss: 0.6476\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.1601 - val_loss: 0.6348\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1608 - val_loss: 0.6462\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1607 - val_loss: 0.6465\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1608 - val_loss: 0.6395\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1606 - val_loss: 0.6463\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.1597 - val_loss: 0.6448\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1594 - val_loss: 0.6409\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.1596 - val_loss: 0.6482\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1595 - val_loss: 0.6435\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.1596 - val_loss: 0.6387\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1597 - val_loss: 0.6515\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.1594 - val_loss: 0.6402\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1594 - val_loss: 0.6446\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1589 - val_loss: 0.6448\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1587 - val_loss: 0.6408\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.1590 - val_loss: 0.6462\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1589 - val_loss: 0.6475\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1582 - val_loss: 0.6407\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1584 - val_loss: 0.6497\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.1589 - val_loss: 0.6415\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1585 - val_loss: 0.6494\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.1592 - val_loss: 0.6428\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1611 - val_loss: 0.6458\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1599 - val_loss: 0.6434\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1587 - val_loss: 0.6444\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1579 - val_loss: 0.6402\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1582 - val_loss: 0.6461\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1589 - val_loss: 0.6484\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1595 - val_loss: 0.6349\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1607 - val_loss: 0.6523\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1596 - val_loss: 0.6375\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1579 - val_loss: 0.6378\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1572 - val_loss: 0.6480\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1586 - val_loss: 0.6335\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1584 - val_loss: 0.6456\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1578 - val_loss: 0.6499\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.1577 - val_loss: 0.6350\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.1577 - val_loss: 0.6466\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.1583 - val_loss: 0.6409\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.1576 - val_loss: 0.6401\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.1580 - val_loss: 0.6499\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.1577 - val_loss: 0.6441\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.1563 - val_loss: 0.6413\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1567 - val_loss: 0.6469\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.1575 - val_loss: 0.6420\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.1571 - val_loss: 0.6385\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.1575 - val_loss: 0.6503\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.1596 - val_loss: 0.6352\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1579 - val_loss: 0.6435\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1564 - val_loss: 0.6523\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.1564 - val_loss: 0.6386\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1565 - val_loss: 0.6404\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1561 - val_loss: 0.6473\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1560 - val_loss: 0.6404\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1556 - val_loss: 0.6402\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.1554 - val_loss: 0.6438\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1552 - val_loss: 0.6420\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1556 - val_loss: 0.6429\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1552 - val_loss: 0.6456\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1549 - val_loss: 0.6464\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1553 - val_loss: 0.6501\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1550 - val_loss: 0.6432\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1549 - val_loss: 0.6503\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1551 - val_loss: 0.6493\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1555 - val_loss: 0.6489\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1572 - val_loss: 0.6439\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1577 - val_loss: 0.6578\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1564 - val_loss: 0.6455\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1552 - val_loss: 0.6488\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.1548 - val_loss: 0.6516\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.1550 - val_loss: 0.6461\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1563 - val_loss: 0.6555\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1599 - val_loss: 0.6421\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.1570 - val_loss: 0.6472\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1553 - val_loss: 0.6573\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1583 - val_loss: 0.6315\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1591 - val_loss: 0.6391\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.1565 - val_loss: 0.6596\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1621 - val_loss: 0.6369\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1553 - val_loss: 0.6147\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1604 - val_loss: 0.6427\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1581 - val_loss: 0.6507\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1587 - val_loss: 0.6190\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1593 - val_loss: 0.6194\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1567 - val_loss: 0.6413\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.1579 - val_loss: 0.6355\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1553 - val_loss: 0.6216\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.1570 - val_loss: 0.6263\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1563 - val_loss: 0.6381\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1561 - val_loss: 0.6368\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1550 - val_loss: 0.6225\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1554 - val_loss: 0.6280\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1556 - val_loss: 0.6386\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1555 - val_loss: 0.6327\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1544 - val_loss: 0.6268\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.1545 - val_loss: 0.6355\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1543 - val_loss: 0.6425\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.1542 - val_loss: 0.6349\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.1540 - val_loss: 0.6335\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.1533 - val_loss: 0.6380\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1537 - val_loss: 0.6438\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1531 - val_loss: 0.6415\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.1532 - val_loss: 0.6414\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1530 - val_loss: 0.6416\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.1528 - val_loss: 0.6413\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.1527 - val_loss: 0.6415\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.1527 - val_loss: 0.6450\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.1522 - val_loss: 0.6488\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.1526 - val_loss: 0.6491\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.1520 - val_loss: 0.6453\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.1522 - val_loss: 0.6433\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.1522 - val_loss: 0.6469\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.1519 - val_loss: 0.6479\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.1519 - val_loss: 0.6476\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.1516 - val_loss: 0.6487\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1517 - val_loss: 0.6493\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1515 - val_loss: 0.6491\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1515 - val_loss: 0.6515\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.1514 - val_loss: 0.6513\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1512 - val_loss: 0.6503\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.1513 - val_loss: 0.6541\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1513 - val_loss: 0.6521\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1512 - val_loss: 0.6521\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.1514 - val_loss: 0.6547\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1514 - val_loss: 0.6552\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.1518 - val_loss: 0.6523\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.1524 - val_loss: 0.6579\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1539 - val_loss: 0.6537\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1529 - val_loss: 0.6541\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1516 - val_loss: 0.6559\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1508 - val_loss: 0.6513\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.1511 - val_loss: 0.6518\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1523 - val_loss: 0.6517\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1529 - val_loss: 0.6552\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1532 - val_loss: 0.6529\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1512 - val_loss: 0.6514\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1503 - val_loss: 0.6499\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1509 - val_loss: 0.6514\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1513 - val_loss: 0.6517\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.1516 - val_loss: 0.6485\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1509 - val_loss: 0.6549\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1505 - val_loss: 0.6499\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1503 - val_loss: 0.6505\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1502 - val_loss: 0.6522\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1503 - val_loss: 0.6489\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1507 - val_loss: 0.6550\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1514 - val_loss: 0.6486\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1502 - val_loss: 0.6543\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1496 - val_loss: 0.6545\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1497 - val_loss: 0.6483\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1497 - val_loss: 0.6532\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1498 - val_loss: 0.6517\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1500 - val_loss: 0.6525\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1501 - val_loss: 0.6505\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1499 - val_loss: 0.6539\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1499 - val_loss: 0.6535\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1495 - val_loss: 0.6514\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1493 - val_loss: 0.6517\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1491 - val_loss: 0.6545\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1492 - val_loss: 0.6485\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.1491 - val_loss: 0.6526\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1491 - val_loss: 0.6506\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1492 - val_loss: 0.6537\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1496 - val_loss: 0.6487\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1498 - val_loss: 0.6571\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1509 - val_loss: 0.6494\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.1505 - val_loss: 0.6536\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.1506 - val_loss: 0.6523\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.1492 - val_loss: 0.6519\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1487 - val_loss: 0.6509\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1486 - val_loss: 0.6460\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1489 - val_loss: 0.6519\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1496 - val_loss: 0.6444\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.1495 - val_loss: 0.6511\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.1501 - val_loss: 0.6473\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.1491 - val_loss: 0.6521\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.1483 - val_loss: 0.6500\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.1480 - val_loss: 0.6457\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.1484 - val_loss: 0.6501\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.1492 - val_loss: 0.6424\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.1495 - val_loss: 0.6496\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.1501 - val_loss: 0.6440\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.1488 - val_loss: 0.6503\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.1481 - val_loss: 0.6479\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1477 - val_loss: 0.6471\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1480 - val_loss: 0.6499\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.1487 - val_loss: 0.6443\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1487 - val_loss: 0.6462\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.1491 - val_loss: 0.6459\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1485 - val_loss: 0.6443\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1488 - val_loss: 0.6509\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1504 - val_loss: 0.6382\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1524 - val_loss: 0.6535\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1558 - val_loss: 0.6374\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1508 - val_loss: 0.6442\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1477 - val_loss: 0.6498\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1499 - val_loss: 0.6403\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1496 - val_loss: 0.6409\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1475 - val_loss: 0.6446\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1484 - val_loss: 0.6382\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1485 - val_loss: 0.6361\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.1473 - val_loss: 0.6400\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1478 - val_loss: 0.6388\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.1478 - val_loss: 0.6386\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1471 - val_loss: 0.6384\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.1476 - val_loss: 0.6396\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1479 - val_loss: 0.6410\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1475 - val_loss: 0.6434\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.1487 - val_loss: 0.6396\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1485 - val_loss: 0.6478\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1486 - val_loss: 0.6429\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1477 - val_loss: 0.6419\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1475 - val_loss: 0.6464\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1468 - val_loss: 0.6452\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1466 - val_loss: 0.6404\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1468 - val_loss: 0.6426\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1467 - val_loss: 0.6427\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1469 - val_loss: 0.6361\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1473 - val_loss: 0.6423\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1479 - val_loss: 0.6404\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1475 - val_loss: 0.6430\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1477 - val_loss: 0.6409\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1470 - val_loss: 0.6426\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1465 - val_loss: 0.6402\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1462 - val_loss: 0.6391\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1462 - val_loss: 0.6390\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1461 - val_loss: 0.6397\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1460 - val_loss: 0.6381\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1461 - val_loss: 0.6357\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1465 - val_loss: 0.6420\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1471 - val_loss: 0.6352\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1476 - val_loss: 0.6386\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1495 - val_loss: 0.6362\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.1482 - val_loss: 0.6400\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1486 - val_loss: 0.6345\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.1463 - val_loss: 0.6352\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1460 - val_loss: 0.6358\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.1469 - val_loss: 0.6312\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1468 - val_loss: 0.6314\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.1473 - val_loss: 0.6289\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.1462 - val_loss: 0.6332\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.1455 - val_loss: 0.6325\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.1458 - val_loss: 0.6275\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.1465 - val_loss: 0.6326\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.1466 - val_loss: 0.6284\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.1457 - val_loss: 0.6294\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.1455 - val_loss: 0.6307\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.1454 - val_loss: 0.6268\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.1455 - val_loss: 0.6262\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.1461 - val_loss: 0.6276\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.1462 - val_loss: 0.6323\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.1468 - val_loss: 0.6248\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.1460 - val_loss: 0.6309\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.1458 - val_loss: 0.6269\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.1454 - val_loss: 0.6286\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.1453 - val_loss: 0.6248\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.1457 - val_loss: 0.6248\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.1459 - val_loss: 0.6229\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.1458 - val_loss: 0.6237\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.1453 - val_loss: 0.6260\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1451 - val_loss: 0.6224\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1449 - val_loss: 0.6243\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 0.1449 - val_loss: 0.6213\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.1454 - val_loss: 0.6282\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.1461 - val_loss: 0.6156\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1465 - val_loss: 0.6261\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1474 - val_loss: 0.6158\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1474 - val_loss: 0.6231\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.1493 - val_loss: 0.6160\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.1487 - val_loss: 0.6257\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1501 - val_loss: 0.6265\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1469 - val_loss: 0.6171\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1460 - val_loss: 0.6213\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1468 - val_loss: 0.6149\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.1460 - val_loss: 0.6057\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.1457 - val_loss: 0.6109\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.1453 - val_loss: 0.6106\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.1452 - val_loss: 0.6102\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.1456 - val_loss: 0.6134\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1450 - val_loss: 0.6099\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.1448 - val_loss: 0.6120\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1450 - val_loss: 0.6141\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.1449 - val_loss: 0.6134\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1446 - val_loss: 0.6122\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1442 - val_loss: 0.6085\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1445 - val_loss: 0.6135\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1452 - val_loss: 0.6149\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.1446 - val_loss: 0.6105\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1441 - val_loss: 0.6134\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1438 - val_loss: 0.6175\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1441 - val_loss: 0.6099\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1442 - val_loss: 0.6068\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1440 - val_loss: 0.6109\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.1439 - val_loss: 0.6067\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1438 - val_loss: 0.6080\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1439 - val_loss: 0.6066\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1437 - val_loss: 0.6093\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.1435 - val_loss: 0.6064\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.1434 - val_loss: 0.6090\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.1434 - val_loss: 0.6083\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1434 - val_loss: 0.6056\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1434 - val_loss: 0.6083\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1434 - val_loss: 0.6076\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.1435 - val_loss: 0.6013\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.1440 - val_loss: 0.6100\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.1458 - val_loss: 0.5974\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.1492 - val_loss: 0.6096\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.1565 - val_loss: 0.5908\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.1507 - val_loss: 0.6106\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.1464 - val_loss: 0.6055\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.1526 - val_loss: 0.5957\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.1464 - val_loss: 0.5983\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.1499 - val_loss: 0.5916\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.1591 - val_loss: 0.5909\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.1473 - val_loss: 0.5787\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.1558 - val_loss: 0.5763\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1517 - val_loss: 0.5909\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1563 - val_loss: 0.5939\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1464 - val_loss: 0.5757\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.1574 - val_loss: 0.5613\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1495 - val_loss: 0.5711\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1506 - val_loss: 0.5851\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1501 - val_loss: 0.5883\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1477 - val_loss: 0.5730\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1493 - val_loss: 0.5663\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1484 - val_loss: 0.5684\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1479 - val_loss: 0.5862\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.1475 - val_loss: 0.5852\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1467 - val_loss: 0.5708\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.1480 - val_loss: 0.5643\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1450 - val_loss: 0.5733\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1472 - val_loss: 0.5872\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1454 - val_loss: 0.5855\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.1455 - val_loss: 0.5770\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.1460 - val_loss: 0.5774\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1440 - val_loss: 0.5818\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1457 - val_loss: 0.5819\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.1440 - val_loss: 0.5814\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.1447 - val_loss: 0.5874\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.1445 - val_loss: 0.5931\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.1437 - val_loss: 0.5890\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.1443 - val_loss: 0.5802\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1435 - val_loss: 0.5817\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.1439 - val_loss: 0.5940\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.1434 - val_loss: 0.6005\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.1434 - val_loss: 0.5930\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.1433 - val_loss: 0.5848\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.1432 - val_loss: 0.5896\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.1430 - val_loss: 0.5999\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.1431 - val_loss: 0.6003\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.1429 - val_loss: 0.5924\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.1427 - val_loss: 0.5899\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.1429 - val_loss: 0.5958\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.1425 - val_loss: 0.5999\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.1427 - val_loss: 0.5957\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.1424 - val_loss: 0.5906\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.1425 - val_loss: 0.5929\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.1423 - val_loss: 0.5979\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1423 - val_loss: 0.5978\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1422 - val_loss: 0.5937\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.1422 - val_loss: 0.5918\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1421 - val_loss: 0.5930\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1421 - val_loss: 0.5940\n"
          ]
        }
      ],
      "source": [
        "h_gru = model_GRU.fit(X_TRAIN,Y_TRAIN,\n",
        "          epochs=1000,\n",
        "          batch_size=X_TRAIN.shape[0],\n",
        "          validation_data = (X_VALID, Y_VALID),\n",
        "          verbose = True\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "Z8VRfq5zO9TN",
        "outputId": "a77aef26-c487-4037-ac0c-ce8593ab3add"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b1231afa500>]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h_gru.history[\"loss\"])\n",
        "plt.plot(h_gru.history[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiIeCfP_RWTo"
      },
      "source": [
        "# 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrE8xwdBRaME",
        "outputId": "39661230-f72b-43c0-aa69-61dd41196f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 149, 31)]         0         \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 10)                1290      \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 10)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 31)                341       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,631\n",
            "Trainable params: 1,631\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(X_TRAIN.shape[1:]))\n",
        "m = inputs\n",
        "\n",
        "mA = GRU(units=10, kernel_regularizer=regularizers.l1_l2(l1=0.2, l2=0.2))(m)\n",
        "\n",
        "#mA = Dropout(0.1)(mA)\n",
        "\n",
        "#mA = Dense(units=1, activation = 'relu')(mA)\n",
        "\n",
        "m = Flatten()(mA)\n",
        "\n",
        "# A침adir capa Dense de salida\n",
        "out = Dense(Y_TRAIN.shape[1], activation='tanh')(m)\n",
        "\n",
        "model_GRU_2 = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_GRU_2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse',\n",
        "        metrics=[])\n",
        "\n",
        "model_GRU_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNNNHUFqRcE6",
        "outputId": "49e70000-3eb9-4915-af22-55157196d716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 36.8758 - val_loss: 34.4894\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 34.4873 - val_loss: 32.2010\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 32.1961 - val_loss: 30.0022\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 29.9941 - val_loss: 27.8802\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 27.8684 - val_loss: 25.8513\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 25.8358 - val_loss: 23.9517\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 23.9327 - val_loss: 22.1470\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 22.1253 - val_loss: 20.4261\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 20.4025 - val_loss: 18.7966\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 18.7720 - val_loss: 17.2335\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 17.2083 - val_loss: 15.7524\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 15.7270 - val_loss: 14.3224\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 14.2971 - val_loss: 13.0091\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 12.9840 - val_loss: 11.8040\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 11.7792 - val_loss: 10.6981\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 10.6736 - val_loss: 9.6919\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 9.6678 - val_loss: 8.7340\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 8.7105 - val_loss: 7.8511\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 7.8282 - val_loss: 7.0182\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 6.9960 - val_loss: 6.2747\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 6.2532 - val_loss: 5.6299\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 5.6089 - val_loss: 5.0574\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 5.0368 - val_loss: 4.5432\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 4.5229 - val_loss: 4.0444\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 4.0242 - val_loss: 3.6245\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 3.6044 - val_loss: 3.3197\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 3.2996 - val_loss: 3.0643\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 3.0442 - val_loss: 2.8456\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 2.8254 - val_loss: 2.6779\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 2.6578 - val_loss: 2.5406\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 2.5204 - val_loss: 2.4419\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 2.4217 - val_loss: 2.3979\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 2.3775 - val_loss: 2.4105\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 2.3899 - val_loss: 2.3805\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 2.3597 - val_loss: 2.3239\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 2.3029 - val_loss: 2.2779\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 2.2566 - val_loss: 2.1831\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 2.1615 - val_loss: 2.0619\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 2.0402 - val_loss: 1.9389\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 1.9171 - val_loss: 1.8212\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 1.7994 - val_loss: 1.6972\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 1.6754 - val_loss: 1.5704\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 1.5487 - val_loss: 1.4554\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 1.4337 - val_loss: 1.3597\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 1.3382 - val_loss: 1.2770\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 1.2556 - val_loss: 1.2034\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 1.1821 - val_loss: 1.1254\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 1.1042 - val_loss: 1.0935\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 1.0723 - val_loss: 1.1052\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 1.0840 - val_loss: 1.1299\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 1.1087 - val_loss: 1.0986\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 1.0773 - val_loss: 1.0352\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 1.0138 - val_loss: 0.9772\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.9557 - val_loss: 0.9409\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.9193 - val_loss: 0.9063\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.8846 - val_loss: 0.8932\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.8714 - val_loss: 0.8643\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.8424 - val_loss: 0.8313\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.8093 - val_loss: 0.7974\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.7755 - val_loss: 0.7891\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.7672 - val_loss: 0.7900\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.7681 - val_loss: 0.7807\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.7590 - val_loss: 0.7618\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.7401 - val_loss: 0.7416\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.7201 - val_loss: 0.7278\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.7063 - val_loss: 0.7186\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.6972 - val_loss: 0.7099\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.6885 - val_loss: 0.6997\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.6784 - val_loss: 0.6875\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.6662 - val_loss: 0.6831\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.6618 - val_loss: 0.6780\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.6567 - val_loss: 0.6834\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.6621 - val_loss: 0.6750\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.6537 - val_loss: 0.6575\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.6362 - val_loss: 0.6486\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.6273 - val_loss: 0.6529\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.6316 - val_loss: 0.6553\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.6340 - val_loss: 0.6474\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.6261 - val_loss: 0.6279\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.6067 - val_loss: 0.6193\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.5980 - val_loss: 0.6286\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.6073 - val_loss: 0.6367\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.6154 - val_loss: 0.6282\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.6068 - val_loss: 0.6211\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.5997 - val_loss: 0.6255\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.6040 - val_loss: 0.6307\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.6092 - val_loss: 0.6200\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5985 - val_loss: 0.6102\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5888 - val_loss: 0.6153\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5938 - val_loss: 0.6317\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.6102 - val_loss: 0.6327\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.6112 - val_loss: 0.6184\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.5969 - val_loss: 0.6108\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5893 - val_loss: 0.6108\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5893 - val_loss: 0.6111\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5896 - val_loss: 0.6196\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.5981 - val_loss: 0.6231\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.6016 - val_loss: 0.6148\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5933 - val_loss: 0.6025\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5810 - val_loss: 0.6075\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5860 - val_loss: 0.6060\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5845 - val_loss: 0.6018\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.5803 - val_loss: 0.5904\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.5690 - val_loss: 0.5895\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5681 - val_loss: 0.6052\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5838 - val_loss: 0.6113\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5898 - val_loss: 0.5984\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5769 - val_loss: 0.5908\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5694 - val_loss: 0.6023\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5808 - val_loss: 0.6011\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5797 - val_loss: 0.5965\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5751 - val_loss: 0.6071\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5857 - val_loss: 0.6173\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5959 - val_loss: 0.6163\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5949 - val_loss: 0.6050\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5836 - val_loss: 0.6015\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5801 - val_loss: 0.5947\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.5732 - val_loss: 0.6020\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5806 - val_loss: 0.5995\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5781 - val_loss: 0.5926\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5712 - val_loss: 0.6045\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5831 - val_loss: 0.6006\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5792 - val_loss: 0.5886\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5672 - val_loss: 0.5929\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5714 - val_loss: 0.6110\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5896 - val_loss: 0.6105\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5890 - val_loss: 0.6006\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5792 - val_loss: 0.5974\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5760 - val_loss: 0.5950\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5735 - val_loss: 0.6055\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5841 - val_loss: 0.6070\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.5855 - val_loss: 0.6009\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5795 - val_loss: 0.6103\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5888 - val_loss: 0.6145\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5931 - val_loss: 0.6046\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5832 - val_loss: 0.6028\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5813 - val_loss: 0.6091\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5877 - val_loss: 0.6125\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5911 - val_loss: 0.6006\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.5792 - val_loss: 0.5951\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5736 - val_loss: 0.5971\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5757 - val_loss: 0.6027\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.5812 - val_loss: 0.5956\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5742 - val_loss: 0.5908\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5693 - val_loss: 0.6048\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5833 - val_loss: 0.6063\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5849 - val_loss: 0.5977\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5763 - val_loss: 0.5962\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5747 - val_loss: 0.6063\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5849 - val_loss: 0.6099\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.5885 - val_loss: 0.5972\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.5758 - val_loss: 0.5927\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.5713 - val_loss: 0.5965\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.5751 - val_loss: 0.5978\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.5763 - val_loss: 0.5905\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.5690 - val_loss: 0.5977\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.5762 - val_loss: 0.6015\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.5801 - val_loss: 0.5964\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.5749 - val_loss: 0.5827\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.5613 - val_loss: 0.5894\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.5680 - val_loss: 0.6123\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5909 - val_loss: 0.6162\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5948 - val_loss: 0.6021\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5806 - val_loss: 0.5881\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5667 - val_loss: 0.5999\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5785 - val_loss: 0.6106\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.5891 - val_loss: 0.6030\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5816 - val_loss: 0.6030\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5816 - val_loss: 0.6089\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5874 - val_loss: 0.6079\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5865 - val_loss: 0.5933\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5719 - val_loss: 0.5977\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5762 - val_loss: 0.6057\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5843 - val_loss: 0.6069\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5855 - val_loss: 0.5985\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5771 - val_loss: 0.5939\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5725 - val_loss: 0.5949\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5735 - val_loss: 0.5961\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.5747 - val_loss: 0.5925\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5710 - val_loss: 0.5935\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5721 - val_loss: 0.6091\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5877 - val_loss: 0.6102\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5887 - val_loss: 0.5944\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5730 - val_loss: 0.5965\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5751 - val_loss: 0.6080\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5866 - val_loss: 0.6159\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5944 - val_loss: 0.6004\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5790 - val_loss: 0.5926\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5712 - val_loss: 0.5953\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5738 - val_loss: 0.6028\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5814 - val_loss: 0.5969\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5755 - val_loss: 0.5923\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5709 - val_loss: 0.5993\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5778 - val_loss: 0.5960\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.5746 - val_loss: 0.5880\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5666 - val_loss: 0.5969\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5755 - val_loss: 0.6082\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5868 - val_loss: 0.6114\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.5900 - val_loss: 0.5984\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5769 - val_loss: 0.5953\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5738 - val_loss: 0.6058\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5844 - val_loss: 0.6131\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5917 - val_loss: 0.6052\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5837 - val_loss: 0.6030\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.5816 - val_loss: 0.6114\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5900 - val_loss: 0.6093\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5878 - val_loss: 0.5981\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5767 - val_loss: 0.5956\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5742 - val_loss: 0.6016\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5802 - val_loss: 0.6089\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5874 - val_loss: 0.5975\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.5760 - val_loss: 0.5918\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5703 - val_loss: 0.5919\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5705 - val_loss: 0.5953\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5738 - val_loss: 0.5929\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5715 - val_loss: 0.5922\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5708 - val_loss: 0.6039\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5825 - val_loss: 0.6061\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5847 - val_loss: 0.5906\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5692 - val_loss: 0.5911\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5696 - val_loss: 0.6098\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5884 - val_loss: 0.6205\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.5991 - val_loss: 0.6028\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.5814 - val_loss: 0.5972\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.5758 - val_loss: 0.6037\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.5822 - val_loss: 0.6142\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.5928 - val_loss: 0.5994\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.5779 - val_loss: 0.5948\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.5733 - val_loss: 0.6022\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.5808 - val_loss: 0.6045\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.5831 - val_loss: 0.5977\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.5763 - val_loss: 0.5958\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5743 - val_loss: 0.6041\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5827 - val_loss: 0.6028\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5814 - val_loss: 0.6019\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.5805 - val_loss: 0.6009\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5795 - val_loss: 0.6007\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5793 - val_loss: 0.6081\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5867 - val_loss: 0.6016\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5802 - val_loss: 0.6012\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5798 - val_loss: 0.6080\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5866 - val_loss: 0.6065\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.5850 - val_loss: 0.5904\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5690 - val_loss: 0.5944\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5730 - val_loss: 0.6096\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5882 - val_loss: 0.6182\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5968 - val_loss: 0.6005\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5790 - val_loss: 0.5889\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5675 - val_loss: 0.5960\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5746 - val_loss: 0.6057\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5842 - val_loss: 0.6043\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5828 - val_loss: 0.6032\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5817 - val_loss: 0.6120\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.5905 - val_loss: 0.6085\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5870 - val_loss: 0.5945\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5731 - val_loss: 0.6022\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5807 - val_loss: 0.6155\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5941 - val_loss: 0.6171\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5957 - val_loss: 0.6026\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5812 - val_loss: 0.6044\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5829 - val_loss: 0.6107\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5893 - val_loss: 0.6081\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5867 - val_loss: 0.5977\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5763 - val_loss: 0.5953\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5738 - val_loss: 0.6059\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5845 - val_loss: 0.6091\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5877 - val_loss: 0.5964\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5749 - val_loss: 0.5947\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.5733 - val_loss: 0.5986\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5772 - val_loss: 0.6073\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5858 - val_loss: 0.6073\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5859 - val_loss: 0.6031\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.5816 - val_loss: 0.6049\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5834 - val_loss: 0.6092\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5878 - val_loss: 0.6064\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5850 - val_loss: 0.6038\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5824 - val_loss: 0.6087\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5873 - val_loss: 0.6078\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5864 - val_loss: 0.5953\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5739 - val_loss: 0.6011\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.5797 - val_loss: 0.6097\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5883 - val_loss: 0.6121\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5906 - val_loss: 0.5953\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5738 - val_loss: 0.5903\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5689 - val_loss: 0.5999\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.5784 - val_loss: 0.6098\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.5884 - val_loss: 0.6048\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5834 - val_loss: 0.6034\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.5819 - val_loss: 0.6117\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5903 - val_loss: 0.6076\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5861 - val_loss: 0.5976\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5762 - val_loss: 0.6013\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.5798 - val_loss: 0.6164\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.5950 - val_loss: 0.6205\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.5991 - val_loss: 0.6069\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.5854 - val_loss: 0.6058\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.5843 - val_loss: 0.6041\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.5826 - val_loss: 0.6058\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.5843 - val_loss: 0.5953\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.5738 - val_loss: 0.5991\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.5777 - val_loss: 0.6129\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.5914 - val_loss: 0.6116\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.5902 - val_loss: 0.5933\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5718 - val_loss: 0.5879\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.5665 - val_loss: 0.6051\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5837 - val_loss: 0.6137\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5923 - val_loss: 0.6060\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5846 - val_loss: 0.6043\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5829 - val_loss: 0.6018\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5804 - val_loss: 0.6115\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5901 - val_loss: 0.6111\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5896 - val_loss: 0.6098\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5883 - val_loss: 0.6095\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5881 - val_loss: 0.6084\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5869 - val_loss: 0.6036\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5822 - val_loss: 0.6084\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5869 - val_loss: 0.6117\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5902 - val_loss: 0.6087\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.5873 - val_loss: 0.5954\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5739 - val_loss: 0.5951\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5736 - val_loss: 0.6032\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5817 - val_loss: 0.6071\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.5857 - val_loss: 0.5975\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5761 - val_loss: 0.5949\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5734 - val_loss: 0.6074\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5859 - val_loss: 0.6108\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5894 - val_loss: 0.5970\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5755 - val_loss: 0.5979\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5765 - val_loss: 0.6148\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5934 - val_loss: 0.6247\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.6033 - val_loss: 0.6082\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5867 - val_loss: 0.6011\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5797 - val_loss: 0.6062\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5848 - val_loss: 0.6100\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5886 - val_loss: 0.6010\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5795 - val_loss: 0.6071\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5857 - val_loss: 0.6138\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5924 - val_loss: 0.6091\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5877 - val_loss: 0.5918\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5704 - val_loss: 0.5950\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5735 - val_loss: 0.6115\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.5900 - val_loss: 0.6181\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5966 - val_loss: 0.6094\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5880 - val_loss: 0.6017\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5802 - val_loss: 0.6060\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5845 - val_loss: 0.6114\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5900 - val_loss: 0.6061\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5846 - val_loss: 0.6059\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5845 - val_loss: 0.6085\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5870 - val_loss: 0.6106\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5891 - val_loss: 0.6024\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5810 - val_loss: 0.6045\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5831 - val_loss: 0.6035\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5820 - val_loss: 0.6083\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5868 - val_loss: 0.6020\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5806 - val_loss: 0.5988\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5774 - val_loss: 0.6031\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5817 - val_loss: 0.6100\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5886 - val_loss: 0.6011\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5797 - val_loss: 0.5996\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5782 - val_loss: 0.6172\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5958 - val_loss: 0.6136\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.5921 - val_loss: 0.5964\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5749 - val_loss: 0.6019\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5805 - val_loss: 0.6191\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.5976 - val_loss: 0.6271\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.6056 - val_loss: 0.6082\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.5868 - val_loss: 0.6029\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.5815 - val_loss: 0.6052\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.5838 - val_loss: 0.6122\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.5907 - val_loss: 0.6067\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.5852 - val_loss: 0.6043\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.5829 - val_loss: 0.6081\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.5866 - val_loss: 0.5997\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.5782 - val_loss: 0.5900\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.5686 - val_loss: 0.5992\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.5778 - val_loss: 0.6126\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5912 - val_loss: 0.6144\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5930 - val_loss: 0.6030\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5816 - val_loss: 0.6026\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5811 - val_loss: 0.6077\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5863 - val_loss: 0.6143\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5929 - val_loss: 0.6084\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5869 - val_loss: 0.6044\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.5830 - val_loss: 0.6139\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5925 - val_loss: 0.6195\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5981 - val_loss: 0.6027\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5813 - val_loss: 0.5960\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5745 - val_loss: 0.6058\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5843 - val_loss: 0.6162\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.5948 - val_loss: 0.6079\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5865 - val_loss: 0.6026\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5812 - val_loss: 0.6027\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.5813 - val_loss: 0.6050\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5835 - val_loss: 0.6014\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5799 - val_loss: 0.6024\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5809 - val_loss: 0.6112\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5897 - val_loss: 0.6125\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.5911 - val_loss: 0.5995\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5781 - val_loss: 0.6053\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5839 - val_loss: 0.6212\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5998 - val_loss: 0.6223\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.6009 - val_loss: 0.6039\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5825 - val_loss: 0.6009\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5794 - val_loss: 0.6091\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.5876 - val_loss: 0.6170\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5955 - val_loss: 0.6041\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5826 - val_loss: 0.6011\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5797 - val_loss: 0.6023\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5808 - val_loss: 0.6013\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5798 - val_loss: 0.5933\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5719 - val_loss: 0.5962\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5747 - val_loss: 0.6092\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5878 - val_loss: 0.6134\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5920 - val_loss: 0.6059\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5845 - val_loss: 0.6006\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5791 - val_loss: 0.6064\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5849 - val_loss: 0.6132\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5918 - val_loss: 0.6072\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5858 - val_loss: 0.6148\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5934 - val_loss: 0.6221\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.6006 - val_loss: 0.6193\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5978 - val_loss: 0.6005\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5791 - val_loss: 0.6019\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5804 - val_loss: 0.6116\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5901 - val_loss: 0.6203\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5988 - val_loss: 0.6100\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5885 - val_loss: 0.6016\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5801 - val_loss: 0.6014\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5800 - val_loss: 0.6052\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5838 - val_loss: 0.6023\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5809 - val_loss: 0.6018\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5803 - val_loss: 0.6132\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5918 - val_loss: 0.6095\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5881 - val_loss: 0.5967\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5753 - val_loss: 0.6032\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.5818 - val_loss: 0.6112\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.5897 - val_loss: 0.6184\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.5970 - val_loss: 0.6026\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.5812 - val_loss: 0.6002\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.5788 - val_loss: 0.6104\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.5889 - val_loss: 0.6206\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.5991 - val_loss: 0.6050\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.5836 - val_loss: 0.5963\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.5748 - val_loss: 0.6084\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.5870 - val_loss: 0.6138\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.5923 - val_loss: 0.6009\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5795 - val_loss: 0.6019\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5805 - val_loss: 0.6106\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5892 - val_loss: 0.6162\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5948 - val_loss: 0.6099\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5884 - val_loss: 0.6045\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5831 - val_loss: 0.6044\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5829 - val_loss: 0.6116\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.5902 - val_loss: 0.6145\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5931 - val_loss: 0.6167\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5953 - val_loss: 0.6172\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5958 - val_loss: 0.6093\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5879 - val_loss: 0.5928\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5714 - val_loss: 0.6002\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5788 - val_loss: 0.6150\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5936 - val_loss: 0.6190\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5975 - val_loss: 0.6020\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5806 - val_loss: 0.5973\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5759 - val_loss: 0.6016\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5802 - val_loss: 0.6091\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5876 - val_loss: 0.6026\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5811 - val_loss: 0.6013\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5799 - val_loss: 0.6164\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5950 - val_loss: 0.6197\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5983 - val_loss: 0.6052\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5838 - val_loss: 0.6044\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5830 - val_loss: 0.6183\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5968 - val_loss: 0.6207\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5993 - val_loss: 0.6069\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5855 - val_loss: 0.6098\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5883 - val_loss: 0.6116\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.5902 - val_loss: 0.6147\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5933 - val_loss: 0.5991\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5777 - val_loss: 0.6009\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5795 - val_loss: 0.6111\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5897 - val_loss: 0.6077\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5863 - val_loss: 0.5947\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5733 - val_loss: 0.5962\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5748 - val_loss: 0.6106\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5891 - val_loss: 0.6167\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.5953 - val_loss: 0.6092\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5878 - val_loss: 0.6013\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5798 - val_loss: 0.6005\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5791 - val_loss: 0.6135\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5921 - val_loss: 0.6147\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5932 - val_loss: 0.6137\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5922 - val_loss: 0.6106\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5891 - val_loss: 0.6092\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5877 - val_loss: 0.6023\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5809 - val_loss: 0.6069\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5855 - val_loss: 0.6133\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5919 - val_loss: 0.6183\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5969 - val_loss: 0.6035\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5821 - val_loss: 0.5945\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5731 - val_loss: 0.6040\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5825 - val_loss: 0.6111\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5896 - val_loss: 0.6015\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5800 - val_loss: 0.6006\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5791 - val_loss: 0.6171\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.5957 - val_loss: 0.6177\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.5963 - val_loss: 0.5989\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.5774 - val_loss: 0.6038\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.5823 - val_loss: 0.6176\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.5962 - val_loss: 0.6252\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.6038 - val_loss: 0.6150\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.5936 - val_loss: 0.6116\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.5902 - val_loss: 0.6126\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.5911 - val_loss: 0.6093\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.5879 - val_loss: 0.6018\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5804 - val_loss: 0.6069\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5855 - val_loss: 0.6136\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5921 - val_loss: 0.6092\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5878 - val_loss: 0.5921\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5706 - val_loss: 0.5974\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5760 - val_loss: 0.6103\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5888 - val_loss: 0.6135\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5921 - val_loss: 0.6049\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5834 - val_loss: 0.6007\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5793 - val_loss: 0.6090\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5875 - val_loss: 0.6199\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5984 - val_loss: 0.6137\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5922 - val_loss: 0.6072\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5858 - val_loss: 0.6095\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5881 - val_loss: 0.6143\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5928 - val_loss: 0.6065\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.5851 - val_loss: 0.6101\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.5886 - val_loss: 0.6134\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5920 - val_loss: 0.6168\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5954 - val_loss: 0.6061\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5847 - val_loss: 0.6024\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5810 - val_loss: 0.6043\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5828 - val_loss: 0.6088\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5874 - val_loss: 0.6050\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5836 - val_loss: 0.6054\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5840 - val_loss: 0.6178\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5963 - val_loss: 0.6120\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5906 - val_loss: 0.5960\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5745 - val_loss: 0.5997\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5782 - val_loss: 0.6187\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.5973 - val_loss: 0.6271\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.6057 - val_loss: 0.6108\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5893 - val_loss: 0.6057\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5843 - val_loss: 0.6046\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.5832 - val_loss: 0.6125\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5910 - val_loss: 0.6046\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5831 - val_loss: 0.6039\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5824 - val_loss: 0.6134\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5920 - val_loss: 0.6087\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5872 - val_loss: 0.5948\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.5734 - val_loss: 0.5990\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 0.5776 - val_loss: 0.6140\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5926 - val_loss: 0.6187\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5973 - val_loss: 0.6092\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.5878 - val_loss: 0.6089\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5874 - val_loss: 0.6133\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5918 - val_loss: 0.6183\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5969 - val_loss: 0.6096\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5881 - val_loss: 0.6076\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5862 - val_loss: 0.6153\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.5939 - val_loss: 0.6180\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5966 - val_loss: 0.6068\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5854 - val_loss: 0.6065\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5850 - val_loss: 0.6084\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.5870 - val_loss: 0.6096\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.5881 - val_loss: 0.6035\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.5821 - val_loss: 0.6002\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.5788 - val_loss: 0.6019\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.5805 - val_loss: 0.6076\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.5861 - val_loss: 0.6050\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.5836 - val_loss: 0.6068\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.5854 - val_loss: 0.6117\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.5903 - val_loss: 0.6094\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.5879 - val_loss: 0.5959\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.5744 - val_loss: 0.6037\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.5823 - val_loss: 0.6242\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.6028 - val_loss: 0.6302\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.6088 - val_loss: 0.6093\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5879 - val_loss: 0.5986\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5772 - val_loss: 0.6096\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5882 - val_loss: 0.6193\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5978 - val_loss: 0.6090\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5876 - val_loss: 0.6074\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5860 - val_loss: 0.6129\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5914 - val_loss: 0.6118\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5904 - val_loss: 0.5989\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.5774 - val_loss: 0.5991\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5776 - val_loss: 0.6093\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.5878 - val_loss: 0.6119\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5904 - val_loss: 0.6100\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5885 - val_loss: 0.6123\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5909 - val_loss: 0.6132\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5917 - val_loss: 0.6128\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5914 - val_loss: 0.6059\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5845 - val_loss: 0.6117\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5903 - val_loss: 0.6209\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5995 - val_loss: 0.6195\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5981 - val_loss: 0.6025\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.5810 - val_loss: 0.6015\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5801 - val_loss: 0.6079\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5865 - val_loss: 0.6166\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.5952 - val_loss: 0.6047\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5833 - val_loss: 0.5961\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5747 - val_loss: 0.6021\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.5807 - val_loss: 0.6114\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.5900 - val_loss: 0.6077\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.5863 - val_loss: 0.6020\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.5806 - val_loss: 0.6137\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.5923 - val_loss: 0.6106\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.5892 - val_loss: 0.6001\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.5787 - val_loss: 0.6107\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.5893 - val_loss: 0.6217\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.6003 - val_loss: 0.6262\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.6048 - val_loss: 0.6072\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.5858 - val_loss: 0.6051\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.5837 - val_loss: 0.6112\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5897 - val_loss: 0.6183\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5968 - val_loss: 0.6084\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.5870 - val_loss: 0.6052\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5838 - val_loss: 0.6144\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5930 - val_loss: 0.6091\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5877 - val_loss: 0.5963\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5749 - val_loss: 0.5984\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5769 - val_loss: 0.6093\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5879 - val_loss: 0.6162\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5948 - val_loss: 0.6138\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.5924 - val_loss: 0.6079\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.5865 - val_loss: 0.6029\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.5815 - val_loss: 0.6078\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.5864 - val_loss: 0.6088\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.5874 - val_loss: 0.6120\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.5906 - val_loss: 0.6193\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.5978 - val_loss: 0.6199\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.5985 - val_loss: 0.6012\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.5797 - val_loss: 0.6029\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.5815 - val_loss: 0.6165\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.5951 - val_loss: 0.6200\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5985 - val_loss: 0.6039\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5825 - val_loss: 0.6012\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5797 - val_loss: 0.6078\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5864 - val_loss: 0.6155\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5941 - val_loss: 0.6064\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5850 - val_loss: 0.6018\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5803 - val_loss: 0.6123\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5909 - val_loss: 0.6130\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5915 - val_loss: 0.6050\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5835 - val_loss: 0.6088\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5873 - val_loss: 0.6207\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5993 - val_loss: 0.6190\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5976 - val_loss: 0.6066\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5852 - val_loss: 0.6067\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5853 - val_loss: 0.6094\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5880 - val_loss: 0.6129\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5915 - val_loss: 0.6004\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5790 - val_loss: 0.6037\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.5822 - val_loss: 0.6100\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5885 - val_loss: 0.6091\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5876 - val_loss: 0.5960\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5745 - val_loss: 0.5962\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5748 - val_loss: 0.6133\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5918 - val_loss: 0.6233\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.6019 - val_loss: 0.6140\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5926 - val_loss: 0.6032\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5818 - val_loss: 0.6067\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5852 - val_loss: 0.6168\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5954 - val_loss: 0.6166\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5952 - val_loss: 0.6171\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5957 - val_loss: 0.6178\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5964 - val_loss: 0.6133\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.5919 - val_loss: 0.6027\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5813 - val_loss: 0.6073\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.5858 - val_loss: 0.6126\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5912 - val_loss: 0.6172\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5957 - val_loss: 0.6057\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.5842 - val_loss: 0.6015\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5801 - val_loss: 0.6090\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.5876 - val_loss: 0.6080\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5866 - val_loss: 0.5981\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5767 - val_loss: 0.5996\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.5782 - val_loss: 0.6170\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5955 - val_loss: 0.6190\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5976 - val_loss: 0.6020\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5806 - val_loss: 0.6053\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5839 - val_loss: 0.6149\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.5934 - val_loss: 0.6225\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6011 - val_loss: 0.6123\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5909 - val_loss: 0.6073\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5859 - val_loss: 0.6122\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.5908 - val_loss: 0.6163\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.5948 - val_loss: 0.6051\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5837 - val_loss: 0.6030\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5816 - val_loss: 0.6121\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5907 - val_loss: 0.6078\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5864 - val_loss: 0.5926\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5712 - val_loss: 0.6009\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5795 - val_loss: 0.6157\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.5943 - val_loss: 0.6188\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.5974 - val_loss: 0.6084\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.5870 - val_loss: 0.6013\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.5799 - val_loss: 0.6057\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.5842 - val_loss: 0.6196\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.5981 - val_loss: 0.6200\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.5986 - val_loss: 0.6149\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.5935 - val_loss: 0.6135\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.5921 - val_loss: 0.6126\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.5912 - val_loss: 0.6049\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5834 - val_loss: 0.6080\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5865 - val_loss: 0.6134\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5919 - val_loss: 0.6142\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5928 - val_loss: 0.6063\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5849 - val_loss: 0.6045\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5831 - val_loss: 0.6017\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5803 - val_loss: 0.6059\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5844 - val_loss: 0.6007\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5793 - val_loss: 0.6011\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5797 - val_loss: 0.6201\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5987 - val_loss: 0.6194\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.5980 - val_loss: 0.5978\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5764 - val_loss: 0.5960\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5746 - val_loss: 0.6183\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5968 - val_loss: 0.6289\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.6074 - val_loss: 0.6118\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5904 - val_loss: 0.6106\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5892 - val_loss: 0.6131\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5917 - val_loss: 0.6180\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5965 - val_loss: 0.6062\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5847 - val_loss: 0.6062\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5847 - val_loss: 0.6134\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5919 - val_loss: 0.6080\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5866 - val_loss: 0.6006\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5791 - val_loss: 0.6054\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5840 - val_loss: 0.6148\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.5933 - val_loss: 0.6121\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5907 - val_loss: 0.6024\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5810 - val_loss: 0.6027\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5813 - val_loss: 0.6105\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5891 - val_loss: 0.6180\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5966 - val_loss: 0.6113\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5898 - val_loss: 0.6091\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5877 - val_loss: 0.6113\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5899 - val_loss: 0.6130\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5916 - val_loss: 0.6022\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5808 - val_loss: 0.6024\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5810 - val_loss: 0.6089\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.5875 - val_loss: 0.6204\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5990 - val_loss: 0.6109\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.5894 - val_loss: 0.5981\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5766 - val_loss: 0.6022\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5807 - val_loss: 0.6081\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.5867 - val_loss: 0.6079\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5865 - val_loss: 0.6122\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5908 - val_loss: 0.6219\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.6005 - val_loss: 0.6184\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5970 - val_loss: 0.6002\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5788 - val_loss: 0.6046\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5832 - val_loss: 0.6212\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.5998 - val_loss: 0.6264\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.6050 - val_loss: 0.6117\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5903 - val_loss: 0.6069\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5855 - val_loss: 0.6123\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5909 - val_loss: 0.6157\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5943 - val_loss: 0.6038\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5823 - val_loss: 0.6022\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5808 - val_loss: 0.6097\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5883 - val_loss: 0.6090\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.5876 - val_loss: 0.5976\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.5762 - val_loss: 0.6013\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.5799 - val_loss: 0.6085\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.5871 - val_loss: 0.6099\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.5885 - val_loss: 0.6064\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.5850 - val_loss: 0.6071\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.5857 - val_loss: 0.6129\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.5915 - val_loss: 0.6199\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.5985 - val_loss: 0.6118\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.5903 - val_loss: 0.6087\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5872 - val_loss: 0.6193\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5978 - val_loss: 0.6192\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5977 - val_loss: 0.6023\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5808 - val_loss: 0.6049\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5835 - val_loss: 0.6152\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5938 - val_loss: 0.6210\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5996 - val_loss: 0.6059\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5845 - val_loss: 0.5978\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5763 - val_loss: 0.5985\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.5770 - val_loss: 0.6081\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.5866 - val_loss: 0.6107\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5892 - val_loss: 0.6074\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5859 - val_loss: 0.6157\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5943 - val_loss: 0.6090\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5875 - val_loss: 0.5991\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5776 - val_loss: 0.6082\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5867 - val_loss: 0.6217\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.6002 - val_loss: 0.6265\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.6051 - val_loss: 0.6097\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5883 - val_loss: 0.6088\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5873 - val_loss: 0.6120\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5906 - val_loss: 0.6157\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5942 - val_loss: 0.6024\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.5809 - val_loss: 0.5995\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.5780 - val_loss: 0.6134\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5920 - val_loss: 0.6164\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.5950 - val_loss: 0.6021\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5806 - val_loss: 0.5971\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.5756 - val_loss: 0.6088\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5874 - val_loss: 0.6162\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.5948 - val_loss: 0.6129\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.5914 - val_loss: 0.6096\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.5882 - val_loss: 0.6088\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5873 - val_loss: 0.6129\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.5914 - val_loss: 0.6092\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.5877 - val_loss: 0.6144\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5930 - val_loss: 0.6189\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.5974 - val_loss: 0.6155\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5941 - val_loss: 0.6018\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5804 - val_loss: 0.6081\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.5867 - val_loss: 0.6189\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5975 - val_loss: 0.6184\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5970 - val_loss: 0.6032\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5817 - val_loss: 0.5956\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5742 - val_loss: 0.6043\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.5829 - val_loss: 0.6130\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.5916 - val_loss: 0.6053\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5838 - val_loss: 0.6018\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5804 - val_loss: 0.6092\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5878 - val_loss: 0.6118\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5904 - val_loss: 0.6027\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.5813 - val_loss: 0.6064\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5850 - val_loss: 0.6207\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5993 - val_loss: 0.6237\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.6023 - val_loss: 0.6105\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.5890 - val_loss: 0.6072\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.5858 - val_loss: 0.6112\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.5898 - val_loss: 0.6164\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.5949 - val_loss: 0.6052\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.5838 - val_loss: 0.6074\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.5860 - val_loss: 0.6165\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.5951 - val_loss: 0.6130\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.5916 - val_loss: 0.5962\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.5748 - val_loss: 0.5971\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.5757 - val_loss: 0.6132\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.5917 - val_loss: 0.6214\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6000 - val_loss: 0.6162\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5948 - val_loss: 0.6089\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.5875 - val_loss: 0.6074\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5860 - val_loss: 0.6110\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.5895 - val_loss: 0.6126\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.5911 - val_loss: 0.6130\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.5916 - val_loss: 0.6149\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.5934 - val_loss: 0.6128\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5914 - val_loss: 0.6012\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5798 - val_loss: 0.6055\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5840 - val_loss: 0.6084\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5869 - val_loss: 0.6124\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5910 - val_loss: 0.5995\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5780 - val_loss: 0.5986\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5772 - val_loss: 0.6103\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5889 - val_loss: 0.6167\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5952 - val_loss: 0.6045\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5831 - val_loss: 0.5978\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5763 - val_loss: 0.6170\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5956 - val_loss: 0.6195\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5981 - val_loss: 0.6033\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5819 - val_loss: 0.6077\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5862 - val_loss: 0.6205\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5990 - val_loss: 0.6275\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6061 - val_loss: 0.6127\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.5913 - val_loss: 0.6075\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5860 - val_loss: 0.6102\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5887 - val_loss: 0.6126\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5912 - val_loss: 0.6072\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5858 - val_loss: 0.6095\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5880 - val_loss: 0.6133\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5918 - val_loss: 0.6031\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5817 - val_loss: 0.5898\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.5684 - val_loss: 0.5977\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5763 - val_loss: 0.6139\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5925 - val_loss: 0.6184\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5970 - val_loss: 0.6098\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.5883 - val_loss: 0.6040\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5825 - val_loss: 0.6052\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5837 - val_loss: 0.6175\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5961 - val_loss: 0.6154\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.5940 - val_loss: 0.6131\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.5917 - val_loss: 0.6184\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.5969 - val_loss: 0.6198\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.5983 - val_loss: 0.6077\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5863 - val_loss: 0.6037\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5823 - val_loss: 0.6092\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.5878 - val_loss: 0.6140\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.5925 - val_loss: 0.6076\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.5861 - val_loss: 0.6061\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.5847 - val_loss: 0.6071\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5857 - val_loss: 0.6103\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5889 - val_loss: 0.5998\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5784 - val_loss: 0.6009\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5794 - val_loss: 0.6177\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.5963 - val_loss: 0.6171\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.5956 - val_loss: 0.6020\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.5805 - val_loss: 0.6060\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.5846 - val_loss: 0.6230\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.6016 - val_loss: 0.6249\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.6035 - val_loss: 0.6076\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.5862 - val_loss: 0.6052\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.5838 - val_loss: 0.6093\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.5879 - val_loss: 0.6171\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.5956 - val_loss: 0.6090\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.5876 - val_loss: 0.6069\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.5855 - val_loss: 0.6085\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5871 - val_loss: 0.6026\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5812 - val_loss: 0.5928\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.5714 - val_loss: 0.5993\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5778 - val_loss: 0.6152\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5938 - val_loss: 0.6196\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5981 - val_loss: 0.6072\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5858 - val_loss: 0.6008\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5794 - val_loss: 0.6100\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.5886 - val_loss: 0.6174\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.5960 - val_loss: 0.6131\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5917 - val_loss: 0.6129\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.5915 - val_loss: 0.6186\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5972 - val_loss: 0.6230\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6016 - val_loss: 0.6085\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.5871 - val_loss: 0.6054\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.5840 - val_loss: 0.6081\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.5867 - val_loss: 0.6172\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.5958 - val_loss: 0.6130\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5915 - val_loss: 0.6070\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5856 - val_loss: 0.6064\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5849 - val_loss: 0.6034\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5820 - val_loss: 0.6017\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5802 - val_loss: 0.6062\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5848 - val_loss: 0.6182\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5968 - val_loss: 0.6147\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5933 - val_loss: 0.5975\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5760 - val_loss: 0.6043\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.5829 - val_loss: 0.6188\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.5974 - val_loss: 0.6239\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6025 - val_loss: 0.6058\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5844 - val_loss: 0.5998\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5784 - val_loss: 0.6115\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.5901 - val_loss: 0.6212\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.5998 - val_loss: 0.6065\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5850 - val_loss: 0.6018\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.5803 - val_loss: 0.6115\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.5900 - val_loss: 0.6107\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.5893 - val_loss: 0.5996\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5782 - val_loss: 0.6054\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.5840 - val_loss: 0.6161\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.5947 - val_loss: 0.6163\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.5948 - val_loss: 0.6094\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5879 - val_loss: 0.6082\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5868 - val_loss: 0.6087\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5873 - val_loss: 0.6155\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.5941 - val_loss: 0.6121\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5906 - val_loss: 0.6136\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5922 - val_loss: 0.6207\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5992 - val_loss: 0.6134\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5920 - val_loss: 0.5990\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.5776 - val_loss: 0.6010\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5795 - val_loss: 0.6110\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5896 - val_loss: 0.6200\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5985 - val_loss: 0.6098\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.5884 - val_loss: 0.6002\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.5787 - val_loss: 0.5964\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.5749 - val_loss: 0.6072\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.5857 - val_loss: 0.6077\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.5862 - val_loss: 0.6054\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.5839 - val_loss: 0.6168\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.5954 - val_loss: 0.6167\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.5952 - val_loss: 0.6051\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.5837 - val_loss: 0.6082\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.5868 - val_loss: 0.6224\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.6010 - val_loss: 0.6260\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.6045 - val_loss: 0.6094\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.5879 - val_loss: 0.6107\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5893 - val_loss: 0.6158\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5943 - val_loss: 0.6181\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5967 - val_loss: 0.6019\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5804 - val_loss: 0.5994\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5779 - val_loss: 0.6110\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5896 - val_loss: 0.6105\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5891 - val_loss: 0.5990\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5776 - val_loss: 0.5998\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5784 - val_loss: 0.6112\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.5897 - val_loss: 0.6134\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.5920 - val_loss: 0.6088\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5874 - val_loss: 0.6073\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.5858 - val_loss: 0.6105\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5890 - val_loss: 0.6133\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.5919 - val_loss: 0.6106\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5891 - val_loss: 0.6161\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5946 - val_loss: 0.6154\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5939 - val_loss: 0.6142\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.5928 - val_loss: 0.6009\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5795 - val_loss: 0.6049\n"
          ]
        }
      ],
      "source": [
        "h_gru_2 = model_GRU_2.fit(X_TRAIN,Y_TRAIN,\n",
        "          epochs=1000,\n",
        "          batch_size=X_TRAIN.shape[0],\n",
        "          validation_data = (X_VALID, Y_VALID),\n",
        "          verbose = True\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "bMyP6OdGRegf",
        "outputId": "2eae84b9-0555-46e7-db1a-f705a725b038"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b1236db39d0>]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h_gru_2.history[\"loss\"])\n",
        "plt.plot(h_gru_2.history[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgQSVby9Vh4c"
      },
      "source": [
        "# 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T--cXfDhVimf",
        "outputId": "c3854c22-b5de-4e85-cc22-901781cc66bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 149, 31)]         0         \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (None, 10)                1290      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 10)                0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 10)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 31)                341       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,631\n",
            "Trainable params: 1,631\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(X_TRAIN.shape[1:]))\n",
        "m = inputs\n",
        "\n",
        "mA = GRU(units=10, kernel_regularizer=regularizers.l1_l2(l1=0.1, l2=0.1))(m)\n",
        "\n",
        "mA = Dropout(0.1)(mA)\n",
        "\n",
        "#mA = Dense(units=1, activation = 'relu')(mA)\n",
        "\n",
        "m = Flatten()(mA)\n",
        "\n",
        "# A침adir capa Dense de salida\n",
        "out = Dense(Y_TRAIN.shape[1], activation='tanh')(m)\n",
        "\n",
        "model_GRU_3 = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_GRU_3.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse',\n",
        "        metrics=[])\n",
        "\n",
        "model_GRU_3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25aEiW9lVn1I",
        "outputId": "18651f84-e785-4dc7-c6c8-e5ac2b62c9e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 3s 3s/step - loss: 17.6632 - val_loss: 16.4929\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 16.4908 - val_loss: 15.3693\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 15.3644 - val_loss: 14.2906\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 14.2826 - val_loss: 13.2714\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 13.2608 - val_loss: 12.3077\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 12.2944 - val_loss: 11.3995\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 11.3846 - val_loss: 10.5384\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 10.5232 - val_loss: 9.7013\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 9.6855 - val_loss: 8.9108\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 8.8951 - val_loss: 8.1634\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 8.1475 - val_loss: 7.4764\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 7.4601 - val_loss: 6.8370\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 6.8199 - val_loss: 6.2428\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 6.2257 - val_loss: 5.7028\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 5.6850 - val_loss: 5.2009\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 5.1821 - val_loss: 4.7269\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 4.7074 - val_loss: 4.2816\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 4.2611 - val_loss: 3.8663\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 3.8453 - val_loss: 3.4678\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 3.4461 - val_loss: 3.1103\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 3.0879 - val_loss: 2.7998\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 2.7772 - val_loss: 2.5190\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 2.4961 - val_loss: 2.2815\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 2.2583 - val_loss: 2.0693\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 2.0461 - val_loss: 1.9009\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 1.8774 - val_loss: 1.7682\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 1.7447 - val_loss: 1.6544\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 1.6308 - val_loss: 1.5590\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 1.5354 - val_loss: 1.4775\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 1.4539 - val_loss: 1.4097\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 1.3862 - val_loss: 1.3518\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 1.3283 - val_loss: 1.3416\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 1.3181 - val_loss: 1.3459\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 1.3227 - val_loss: 1.3288\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 1.3055 - val_loss: 1.2994\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 1.2764 - val_loss: 1.2712\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 1.2483 - val_loss: 1.2198\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 1.1972 - val_loss: 1.1567\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 1.1342 - val_loss: 1.1101\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 1.0879 - val_loss: 1.0677\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 1.0459 - val_loss: 1.0108\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.9892 - val_loss: 0.9489\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.9275 - val_loss: 0.8845\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.8633 - val_loss: 0.8361\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.8150 - val_loss: 0.7956\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.7748 - val_loss: 0.7585\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.7377 - val_loss: 0.7284\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.7076 - val_loss: 0.7090\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.6880 - val_loss: 0.7152\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.6944 - val_loss: 0.7244\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.7035 - val_loss: 0.7112\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.6903 - val_loss: 0.6819\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.6611 - val_loss: 0.6606\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.6398 - val_loss: 0.6417\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.6211 - val_loss: 0.6246\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.6038 - val_loss: 0.6135\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5927 - val_loss: 0.6036\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5827 - val_loss: 0.5889\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5677 - val_loss: 0.5748\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.5534 - val_loss: 0.5751\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5537 - val_loss: 0.5731\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5516 - val_loss: 0.5685\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5469 - val_loss: 0.5565\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5348 - val_loss: 0.5439\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.5221 - val_loss: 0.5415\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.5196 - val_loss: 0.5410\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.5192 - val_loss: 0.5334\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.5117 - val_loss: 0.5262\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.5046 - val_loss: 0.5212\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4996 - val_loss: 0.5195\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4979 - val_loss: 0.5162\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4947 - val_loss: 0.5160\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4943 - val_loss: 0.5115\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4899 - val_loss: 0.5105\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4889 - val_loss: 0.5135\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4920 - val_loss: 0.5077\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4863 - val_loss: 0.5030\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4816 - val_loss: 0.4995\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4782 - val_loss: 0.4945\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.4733 - val_loss: 0.4923\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4712 - val_loss: 0.4955\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4744 - val_loss: 0.4927\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.4714 - val_loss: 0.4904\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.4689 - val_loss: 0.4928\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.4712 - val_loss: 0.4968\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4750 - val_loss: 0.4963\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.4746 - val_loss: 0.4903\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.4686 - val_loss: 0.4843\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4627 - val_loss: 0.4883\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4668 - val_loss: 0.4948\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4732 - val_loss: 0.4890\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4676 - val_loss: 0.4874\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4658 - val_loss: 0.4877\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4662 - val_loss: 0.4859\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4643 - val_loss: 0.4845\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4630 - val_loss: 0.4871\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4656 - val_loss: 0.4887\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4672 - val_loss: 0.4846\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4631 - val_loss: 0.4817\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4602 - val_loss: 0.4831\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4617 - val_loss: 0.4826\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4613 - val_loss: 0.4815\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4602 - val_loss: 0.4756\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4545 - val_loss: 0.4765\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4554 - val_loss: 0.4821\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4611 - val_loss: 0.4839\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.4628 - val_loss: 0.4795\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.4584 - val_loss: 0.4784\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4571 - val_loss: 0.4801\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.4587 - val_loss: 0.4781\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.4567 - val_loss: 0.4809\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4594 - val_loss: 0.4848\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4632 - val_loss: 0.4866\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4649 - val_loss: 0.4852\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4635 - val_loss: 0.4808\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4590 - val_loss: 0.4767\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.4551 - val_loss: 0.4806\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4591 - val_loss: 0.4804\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4589 - val_loss: 0.4762\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4548 - val_loss: 0.4743\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4530 - val_loss: 0.4784\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4571 - val_loss: 0.4781\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4568 - val_loss: 0.4748\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4535 - val_loss: 0.4768\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4555 - val_loss: 0.4830\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4616 - val_loss: 0.4845\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4631 - val_loss: 0.4809\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4595 - val_loss: 0.4757\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4543 - val_loss: 0.4763\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4548 - val_loss: 0.4821\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4606 - val_loss: 0.4830\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4616 - val_loss: 0.4819\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.4604 - val_loss: 0.4811\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4596 - val_loss: 0.4794\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4580 - val_loss: 0.4797\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4584 - val_loss: 0.4797\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4585 - val_loss: 0.4824\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4613 - val_loss: 0.4790\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4580 - val_loss: 0.4753\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.4543 - val_loss: 0.4788\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4578 - val_loss: 0.4795\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4583 - val_loss: 0.4787\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4574 - val_loss: 0.4751\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4537 - val_loss: 0.4742\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4527 - val_loss: 0.4795\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4580 - val_loss: 0.4802\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4587 - val_loss: 0.4770\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.4556 - val_loss: 0.4745\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.4531 - val_loss: 0.4786\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.4574 - val_loss: 0.4787\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.4574 - val_loss: 0.4752\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4539 - val_loss: 0.4765\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4549 - val_loss: 0.4790\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.4574 - val_loss: 0.4814\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.4597 - val_loss: 0.4799\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.4584 - val_loss: 0.4787\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.4573 - val_loss: 0.4800\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4588 - val_loss: 0.4792\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4580 - val_loss: 0.4726\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.4514 - val_loss: 0.4741\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4529 - val_loss: 0.4852\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4640 - val_loss: 0.4843\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4630 - val_loss: 0.4759\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4546 - val_loss: 0.4715\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4500 - val_loss: 0.4771\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4556 - val_loss: 0.4825\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4611 - val_loss: 0.4808\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4593 - val_loss: 0.4764\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4549 - val_loss: 0.4787\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4573 - val_loss: 0.4809\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4595 - val_loss: 0.4740\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4528 - val_loss: 0.4733\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4521 - val_loss: 0.4768\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4557 - val_loss: 0.4770\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.4557 - val_loss: 0.4763\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.4548 - val_loss: 0.4773\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4556 - val_loss: 0.4762\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4545 - val_loss: 0.4728\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4510 - val_loss: 0.4742\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4524 - val_loss: 0.4769\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4552 - val_loss: 0.4825\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4609 - val_loss: 0.4803\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4587 - val_loss: 0.4745\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4531 - val_loss: 0.4765\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4551 - val_loss: 0.4824\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4611 - val_loss: 0.4808\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4595 - val_loss: 0.4750\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4538 - val_loss: 0.4759\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4547 - val_loss: 0.4787\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4575 - val_loss: 0.4807\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4595 - val_loss: 0.4753\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4541 - val_loss: 0.4732\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4520 - val_loss: 0.4803\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4588 - val_loss: 0.4791\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4576 - val_loss: 0.4745\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4529 - val_loss: 0.4742\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4525 - val_loss: 0.4808\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4592 - val_loss: 0.4785\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.4569 - val_loss: 0.4727\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.4511 - val_loss: 0.4742\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4527 - val_loss: 0.4796\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4582 - val_loss: 0.4806\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.4593 - val_loss: 0.4786\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4574 - val_loss: 0.4760\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.4549 - val_loss: 0.4801\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4589 - val_loss: 0.4809\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4597 - val_loss: 0.4760\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4547 - val_loss: 0.4731\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4518 - val_loss: 0.4754\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4541 - val_loss: 0.4803\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4590 - val_loss: 0.4782\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4569 - val_loss: 0.4786\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4574 - val_loss: 0.4755\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4543 - val_loss: 0.4741\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4528 - val_loss: 0.4752\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4540 - val_loss: 0.4782\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4569 - val_loss: 0.4828\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4614 - val_loss: 0.4799\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.4584 - val_loss: 0.4739\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4524 - val_loss: 0.4754\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4539 - val_loss: 0.4818\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.4602 - val_loss: 0.4811\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4595 - val_loss: 0.4752\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.4536 - val_loss: 0.4764\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.4548 - val_loss: 0.4822\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.4606 - val_loss: 0.4828\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.4612 - val_loss: 0.4744\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.4529 - val_loss: 0.4729\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4515 - val_loss: 0.4808\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.4595 - val_loss: 0.4816\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.4607 - val_loss: 0.4754\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.4545 - val_loss: 0.4741\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4530 - val_loss: 0.4798\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4585 - val_loss: 0.4777\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4561 - val_loss: 0.4784\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4567 - val_loss: 0.4781\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4563 - val_loss: 0.4811\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4593 - val_loss: 0.4812\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4595 - val_loss: 0.4779\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4562 - val_loss: 0.4813\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4596 - val_loss: 0.4834\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4618 - val_loss: 0.4834\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4618 - val_loss: 0.4770\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4554 - val_loss: 0.4772\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4554 - val_loss: 0.4812\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4595 - val_loss: 0.4829\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4612 - val_loss: 0.4799\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4582 - val_loss: 0.4765\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4548 - val_loss: 0.4754\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.4538 - val_loss: 0.4776\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4560 - val_loss: 0.4793\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4578 - val_loss: 0.4815\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.4599 - val_loss: 0.4839\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4624 - val_loss: 0.4766\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4552 - val_loss: 0.4742\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4529 - val_loss: 0.4790\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4576 - val_loss: 0.4830\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4616 - val_loss: 0.4806\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4591 - val_loss: 0.4756\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4540 - val_loss: 0.4801\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4585 - val_loss: 0.4826\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4610 - val_loss: 0.4801\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.4585 - val_loss: 0.4743\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.4527 - val_loss: 0.4736\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4522 - val_loss: 0.4830\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4618 - val_loss: 0.4855\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4645 - val_loss: 0.4755\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.4547 - val_loss: 0.4714\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4506 - val_loss: 0.4778\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4569 - val_loss: 0.4822\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4612 - val_loss: 0.4789\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4577 - val_loss: 0.4794\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4581 - val_loss: 0.4823\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.4611 - val_loss: 0.4824\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4612 - val_loss: 0.4799\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.4587 - val_loss: 0.4797\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4586 - val_loss: 0.4832\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.4620 - val_loss: 0.4801\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.4587 - val_loss: 0.4753\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4537 - val_loss: 0.4792\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4573 - val_loss: 0.4833\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.4614 - val_loss: 0.4822\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4604 - val_loss: 0.4757\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4541 - val_loss: 0.4761\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4546 - val_loss: 0.4783\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4569 - val_loss: 0.4802\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.4588 - val_loss: 0.4794\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4579 - val_loss: 0.4794\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.4579 - val_loss: 0.4822\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.4606 - val_loss: 0.4801\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4584 - val_loss: 0.4782\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4565 - val_loss: 0.4787\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.4572 - val_loss: 0.4825\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4611 - val_loss: 0.4808\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.4596 - val_loss: 0.4798\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.4586 - val_loss: 0.4825\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4613 - val_loss: 0.4810\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.4599 - val_loss: 0.4788\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.4576 - val_loss: 0.4742\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4528 - val_loss: 0.4783\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.4569 - val_loss: 0.4868\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.4652 - val_loss: 0.4848\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.4632 - val_loss: 0.4739\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.4524 - val_loss: 0.4714\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.4497 - val_loss: 0.4818\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4601 - val_loss: 0.4850\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4633 - val_loss: 0.4803\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.4586 - val_loss: 0.4782\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4566 - val_loss: 0.4804\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4590 - val_loss: 0.4854\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4639 - val_loss: 0.4817\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4602 - val_loss: 0.4804\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4590 - val_loss: 0.4821\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4608 - val_loss: 0.4810\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4598 - val_loss: 0.4797\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.4585 - val_loss: 0.4816\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4603 - val_loss: 0.4808\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4594 - val_loss: 0.4751\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4536 - val_loss: 0.4748\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4532 - val_loss: 0.4788\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4571 - val_loss: 0.4790\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4574 - val_loss: 0.4800\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4584 - val_loss: 0.4784\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4569 - val_loss: 0.4790\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4575 - val_loss: 0.4843\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4628 - val_loss: 0.4825\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4610 - val_loss: 0.4773\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4558 - val_loss: 0.4794\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4578 - val_loss: 0.4842\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4626 - val_loss: 0.4843\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4629 - val_loss: 0.4797\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4583 - val_loss: 0.4795\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4582 - val_loss: 0.4810\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4598 - val_loss: 0.4810\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4598 - val_loss: 0.4770\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4558 - val_loss: 0.4796\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4584 - val_loss: 0.4855\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4644 - val_loss: 0.4817\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4607 - val_loss: 0.4701\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4490 - val_loss: 0.4729\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4517 - val_loss: 0.4821\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.4607 - val_loss: 0.4827\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.4613 - val_loss: 0.4794\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4579 - val_loss: 0.4775\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4559 - val_loss: 0.4809\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4592 - val_loss: 0.4832\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4617 - val_loss: 0.4807\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4593 - val_loss: 0.4785\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4573 - val_loss: 0.4812\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.4602 - val_loss: 0.4841\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4631 - val_loss: 0.4808\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4596 - val_loss: 0.4810\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.4596 - val_loss: 0.4785\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4571 - val_loss: 0.4772\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4557 - val_loss: 0.4797\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4582 - val_loss: 0.4828\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.4614 - val_loss: 0.4813\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4599 - val_loss: 0.4793\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4580 - val_loss: 0.4768\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4556 - val_loss: 0.4792\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4579 - val_loss: 0.4845\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4630 - val_loss: 0.4823\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4608 - val_loss: 0.4768\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4551 - val_loss: 0.4786\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4569 - val_loss: 0.4862\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4645 - val_loss: 0.4862\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4646 - val_loss: 0.4782\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.4567 - val_loss: 0.4781\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4565 - val_loss: 0.4796\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4581 - val_loss: 0.4828\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.4614 - val_loss: 0.4791\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.4577 - val_loss: 0.4802\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.4588 - val_loss: 0.4841\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.4626 - val_loss: 0.4791\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4577 - val_loss: 0.4741\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.4527 - val_loss: 0.4783\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.4567 - val_loss: 0.4849\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4632 - val_loss: 0.4832\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.4615 - val_loss: 0.4787\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4569 - val_loss: 0.4805\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4586 - val_loss: 0.4814\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4597 - val_loss: 0.4835\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4619 - val_loss: 0.4803\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4589 - val_loss: 0.4798\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4586 - val_loss: 0.4857\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.4646 - val_loss: 0.4867\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4656 - val_loss: 0.4802\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4590 - val_loss: 0.4769\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4555 - val_loss: 0.4785\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4569 - val_loss: 0.4811\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4594 - val_loss: 0.4822\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4606 - val_loss: 0.4831\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.4615 - val_loss: 0.4799\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4583 - val_loss: 0.4782\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4567 - val_loss: 0.4786\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4571 - val_loss: 0.4827\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4613 - val_loss: 0.4861\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4647 - val_loss: 0.4804\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4590 - val_loss: 0.4765\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4550 - val_loss: 0.4835\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4620 - val_loss: 0.4877\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4662 - val_loss: 0.4829\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.4616 - val_loss: 0.4771\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4559 - val_loss: 0.4770\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4560 - val_loss: 0.4825\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4614 - val_loss: 0.4876\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4664 - val_loss: 0.4802\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4587 - val_loss: 0.4772\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4555 - val_loss: 0.4823\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4605 - val_loss: 0.4837\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4620 - val_loss: 0.4763\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4548 - val_loss: 0.4771\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4558 - val_loss: 0.4835\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.4623 - val_loss: 0.4828\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.4616 - val_loss: 0.4808\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4597 - val_loss: 0.4794\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4583 - val_loss: 0.4815\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.4603 - val_loss: 0.4830\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.4617 - val_loss: 0.4814\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4600 - val_loss: 0.4841\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.4626 - val_loss: 0.4872\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4656 - val_loss: 0.4839\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4624 - val_loss: 0.4757\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4544 - val_loss: 0.4773\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4562 - val_loss: 0.4811\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4600 - val_loss: 0.4822\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4611 - val_loss: 0.4817\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4605 - val_loss: 0.4815\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4601 - val_loss: 0.4773\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4557 - val_loss: 0.4784\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4567 - val_loss: 0.4804\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.4587 - val_loss: 0.4823\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4607 - val_loss: 0.4848\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4633 - val_loss: 0.4802\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4588 - val_loss: 0.4788\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4573 - val_loss: 0.4833\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4619 - val_loss: 0.4849\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.4636 - val_loss: 0.4816\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.4602 - val_loss: 0.4768\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.4554 - val_loss: 0.4831\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.4617 - val_loss: 0.4865\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.4650 - val_loss: 0.4867\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.4651 - val_loss: 0.4788\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.4572 - val_loss: 0.4759\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 0.4542 - val_loss: 0.4856\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.4638 - val_loss: 0.4867\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.4650 - val_loss: 0.4775\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.4558 - val_loss: 0.4766\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.4550 - val_loss: 0.4830\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.4616 - val_loss: 0.4846\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4632 - val_loss: 0.4787\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.4575 - val_loss: 0.4778\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.4566 - val_loss: 0.4798\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.4586 - val_loss: 0.4835\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4621 - val_loss: 0.4842\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4628 - val_loss: 0.4849\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4636 - val_loss: 0.4851\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4638 - val_loss: 0.4801\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4589 - val_loss: 0.4754\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4542 - val_loss: 0.4798\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4584 - val_loss: 0.4831\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4617 - val_loss: 0.4828\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.4611 - val_loss: 0.4810\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4592 - val_loss: 0.4802\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4584 - val_loss: 0.4785\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4567 - val_loss: 0.4814\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4598 - val_loss: 0.4815\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4601 - val_loss: 0.4828\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4615 - val_loss: 0.4858\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4647 - val_loss: 0.4832\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4623 - val_loss: 0.4792\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4583 - val_loss: 0.4807\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4596 - val_loss: 0.4843\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4632 - val_loss: 0.4810\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4597 - val_loss: 0.4800\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4586 - val_loss: 0.4845\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4630 - val_loss: 0.4855\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.4639 - val_loss: 0.4833\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4617 - val_loss: 0.4740\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4524 - val_loss: 0.4779\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4563 - val_loss: 0.4863\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4646 - val_loss: 0.4851\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4634 - val_loss: 0.4758\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4541 - val_loss: 0.4759\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4541 - val_loss: 0.4839\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4621 - val_loss: 0.4839\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4622 - val_loss: 0.4807\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4591 - val_loss: 0.4775\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.4561 - val_loss: 0.4800\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4587 - val_loss: 0.4874\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4661 - val_loss: 0.4857\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4645 - val_loss: 0.4834\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4622 - val_loss: 0.4824\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4610 - val_loss: 0.4801\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.4587 - val_loss: 0.4789\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4576 - val_loss: 0.4832\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4620 - val_loss: 0.4846\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4633 - val_loss: 0.4809\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4596 - val_loss: 0.4781\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4567 - val_loss: 0.4800\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4584 - val_loss: 0.4801\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4585 - val_loss: 0.4813\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4596 - val_loss: 0.4804\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4586 - val_loss: 0.4813\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.4595 - val_loss: 0.4874\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.4655 - val_loss: 0.4853\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.4635 - val_loss: 0.4781\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.4563 - val_loss: 0.4798\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.4581 - val_loss: 0.4835\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.4621 - val_loss: 0.4840\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.4628 - val_loss: 0.4817\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.4606 - val_loss: 0.4831\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.4621 - val_loss: 0.4821\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.4610 - val_loss: 0.4798\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4586 - val_loss: 0.4778\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4563 - val_loss: 0.4812\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4595 - val_loss: 0.4870\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.4653 - val_loss: 0.4838\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4622 - val_loss: 0.4745\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4530 - val_loss: 0.4777\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4564 - val_loss: 0.4836\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4623 - val_loss: 0.4833\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.4622 - val_loss: 0.4789\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4577 - val_loss: 0.4781\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4568 - val_loss: 0.4841\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4627 - val_loss: 0.4895\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4680 - val_loss: 0.4840\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4625 - val_loss: 0.4785\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4570 - val_loss: 0.4815\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4601 - val_loss: 0.4854\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4640 - val_loss: 0.4820\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4605 - val_loss: 0.4828\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4612 - val_loss: 0.4827\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4609 - val_loss: 0.4788\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4570 - val_loss: 0.4786\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4568 - val_loss: 0.4819\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4603 - val_loss: 0.4798\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4583 - val_loss: 0.4783\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.4571 - val_loss: 0.4791\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.4580 - val_loss: 0.4841\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4629 - val_loss: 0.4871\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4659 - val_loss: 0.4805\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4592 - val_loss: 0.4760\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4546 - val_loss: 0.4801\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4585 - val_loss: 0.4876\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.4659 - val_loss: 0.4884\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4668 - val_loss: 0.4824\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.4608 - val_loss: 0.4816\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4600 - val_loss: 0.4804\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4589 - val_loss: 0.4844\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4628 - val_loss: 0.4803\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4588 - val_loss: 0.4810\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4595 - val_loss: 0.4870\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4655 - val_loss: 0.4833\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4620 - val_loss: 0.4764\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4550 - val_loss: 0.4769\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4556 - val_loss: 0.4827\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4615 - val_loss: 0.4818\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4607 - val_loss: 0.4793\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4580 - val_loss: 0.4821\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4607 - val_loss: 0.4849\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4633 - val_loss: 0.4866\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4648 - val_loss: 0.4806\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4587 - val_loss: 0.4791\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4573 - val_loss: 0.4853\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4636 - val_loss: 0.4861\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4646 - val_loss: 0.4815\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4601 - val_loss: 0.4809\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.4597 - val_loss: 0.4798\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.4587 - val_loss: 0.4792\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.4581 - val_loss: 0.4803\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.4592 - val_loss: 0.4822\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.4610 - val_loss: 0.4799\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.4587 - val_loss: 0.4802\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4589 - val_loss: 0.4824\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.4609 - val_loss: 0.4848\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.4633 - val_loss: 0.4857\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.4642 - val_loss: 0.4801\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.4587 - val_loss: 0.4755\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.4543 - val_loss: 0.4827\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4617 - val_loss: 0.4887\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4678 - val_loss: 0.4860\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.4651 - val_loss: 0.4791\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4580 - val_loss: 0.4774\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4561 - val_loss: 0.4833\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4618 - val_loss: 0.4871\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4655 - val_loss: 0.4801\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.4584 - val_loss: 0.4791\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4574 - val_loss: 0.4851\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4636 - val_loss: 0.4837\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4622 - val_loss: 0.4746\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4531 - val_loss: 0.4763\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4549 - val_loss: 0.4825\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4611 - val_loss: 0.4819\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4605 - val_loss: 0.4822\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4607 - val_loss: 0.4835\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4621 - val_loss: 0.4834\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4620 - val_loss: 0.4821\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4607 - val_loss: 0.4797\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4584 - val_loss: 0.4823\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4610 - val_loss: 0.4867\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.4654 - val_loss: 0.4864\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4651 - val_loss: 0.4804\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4590 - val_loss: 0.4798\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4584 - val_loss: 0.4808\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4593 - val_loss: 0.4819\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.4602 - val_loss: 0.4818\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4600 - val_loss: 0.4820\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4603 - val_loss: 0.4802\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4585 - val_loss: 0.4831\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4615 - val_loss: 0.4826\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4612 - val_loss: 0.4816\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4604 - val_loss: 0.4838\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4627 - val_loss: 0.4793\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4582 - val_loss: 0.4784\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4574 - val_loss: 0.4845\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4634 - val_loss: 0.4880\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4667 - val_loss: 0.4825\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4611 - val_loss: 0.4756\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4540 - val_loss: 0.4814\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4597 - val_loss: 0.4854\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4635 - val_loss: 0.4864\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4645 - val_loss: 0.4794\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4575 - val_loss: 0.4776\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4559 - val_loss: 0.4858\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4642 - val_loss: 0.4844\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.4630 - val_loss: 0.4758\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4545 - val_loss: 0.4748\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4535 - val_loss: 0.4826\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4613 - val_loss: 0.4862\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4648 - val_loss: 0.4840\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4625 - val_loss: 0.4806\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4592 - val_loss: 0.4794\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4581 - val_loss: 0.4823\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.4612 - val_loss: 0.4823\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.4612 - val_loss: 0.4856\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.4644 - val_loss: 0.4884\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.4672 - val_loss: 0.4841\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.4627 - val_loss: 0.4778\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.4564 - val_loss: 0.4804\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.4590 - val_loss: 0.4824\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.4611 - val_loss: 0.4805\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.4591 - val_loss: 0.4800\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.4585 - val_loss: 0.4812\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4597 - val_loss: 0.4820\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4605 - val_loss: 0.4827\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.4612 - val_loss: 0.4788\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4571 - val_loss: 0.4797\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4581 - val_loss: 0.4838\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4623 - val_loss: 0.4828\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4614 - val_loss: 0.4811\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4597 - val_loss: 0.4845\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4631 - val_loss: 0.4857\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4643 - val_loss: 0.4792\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4578 - val_loss: 0.4786\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4572 - val_loss: 0.4829\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4615 - val_loss: 0.4849\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.4635 - val_loss: 0.4845\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4629 - val_loss: 0.4774\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4558 - val_loss: 0.4806\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4590 - val_loss: 0.4864\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4649 - val_loss: 0.4842\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4627 - val_loss: 0.4749\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4535 - val_loss: 0.4759\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4546 - val_loss: 0.4859\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4646 - val_loss: 0.4883\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4670 - val_loss: 0.4826\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4612 - val_loss: 0.4766\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4551 - val_loss: 0.4797\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4583 - val_loss: 0.4869\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4655 - val_loss: 0.4847\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.4634 - val_loss: 0.4835\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4620 - val_loss: 0.4856\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4641 - val_loss: 0.4824\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4609 - val_loss: 0.4776\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4562 - val_loss: 0.4812\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4598 - val_loss: 0.4818\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4605 - val_loss: 0.4791\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4577 - val_loss: 0.4796\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4582 - val_loss: 0.4838\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4624 - val_loss: 0.4811\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4597 - val_loss: 0.4790\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4576 - val_loss: 0.4786\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4572 - val_loss: 0.4800\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4586 - val_loss: 0.4871\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.4656 - val_loss: 0.4859\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4644 - val_loss: 0.4806\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4593 - val_loss: 0.4816\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4603 - val_loss: 0.4831\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4619 - val_loss: 0.4835\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.4622 - val_loss: 0.4816\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4602 - val_loss: 0.4844\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.4629 - val_loss: 0.4855\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4641 - val_loss: 0.4839\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4625 - val_loss: 0.4795\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4581 - val_loss: 0.4794\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4580 - val_loss: 0.4853\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.4641 - val_loss: 0.4823\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.4611 - val_loss: 0.4738\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.4527 - val_loss: 0.4792\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.4580 - val_loss: 0.4870\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4656 - val_loss: 0.4849\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.4634 - val_loss: 0.4777\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.4561 - val_loss: 0.4766\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.4550 - val_loss: 0.4826\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.4612 - val_loss: 0.4887\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.4674 - val_loss: 0.4853\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.4641 - val_loss: 0.4822\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.4608 - val_loss: 0.4837\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4621 - val_loss: 0.4830\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4613 - val_loss: 0.4802\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4585 - val_loss: 0.4817\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.4600 - val_loss: 0.4819\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4604 - val_loss: 0.4802\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4588 - val_loss: 0.4819\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.4605 - val_loss: 0.4840\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4626 - val_loss: 0.4794\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4579 - val_loss: 0.4780\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4564 - val_loss: 0.4780\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4563 - val_loss: 0.4835\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.4617 - val_loss: 0.4891\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4674 - val_loss: 0.4843\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4628 - val_loss: 0.4776\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4564 - val_loss: 0.4786\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.4575 - val_loss: 0.4858\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4646 - val_loss: 0.4869\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4655 - val_loss: 0.4823\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.4606 - val_loss: 0.4830\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4611 - val_loss: 0.4842\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.4623 - val_loss: 0.4863\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4645 - val_loss: 0.4788\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4573 - val_loss: 0.4790\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4577 - val_loss: 0.4845\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4635 - val_loss: 0.4811\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4602 - val_loss: 0.4766\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.4557 - val_loss: 0.4797\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.4587 - val_loss: 0.4837\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.4626 - val_loss: 0.4798\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4586 - val_loss: 0.4772\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4558 - val_loss: 0.4805\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4589 - val_loss: 0.4850\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4633 - val_loss: 0.4884\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4667 - val_loss: 0.4833\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4615 - val_loss: 0.4802\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4585 - val_loss: 0.4837\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4622 - val_loss: 0.4851\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4637 - val_loss: 0.4803\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4589 - val_loss: 0.4806\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4591 - val_loss: 0.4825\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4610 - val_loss: 0.4836\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4621 - val_loss: 0.4832\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4617 - val_loss: 0.4815\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4599 - val_loss: 0.4789\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4572 - val_loss: 0.4793\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4576 - val_loss: 0.4819\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4604 - val_loss: 0.4858\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4645 - val_loss: 0.4885\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4674 - val_loss: 0.4805\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4595 - val_loss: 0.4735\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4524 - val_loss: 0.4816\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4603 - val_loss: 0.4874\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.4660 - val_loss: 0.4844\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.4630 - val_loss: 0.4801\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.4587 - val_loss: 0.4816\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.4602 - val_loss: 0.4842\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.4629 - val_loss: 0.4845\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.4633 - val_loss: 0.4783\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.4572 - val_loss: 0.4769\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.4557 - val_loss: 0.4838\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.4626 - val_loss: 0.4859\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4647 - val_loss: 0.4787\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4575 - val_loss: 0.4781\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4568 - val_loss: 0.4815\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4602 - val_loss: 0.4803\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.4589 - val_loss: 0.4806\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4592 - val_loss: 0.4830\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4615 - val_loss: 0.4851\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4636 - val_loss: 0.4863\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4648 - val_loss: 0.4818\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.4602 - val_loss: 0.4820\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4603 - val_loss: 0.4862\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4646 - val_loss: 0.4850\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4634 - val_loss: 0.4795\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4580 - val_loss: 0.4805\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4591 - val_loss: 0.4840\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4626 - val_loss: 0.4832\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4616 - val_loss: 0.4798\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4580 - val_loss: 0.4795\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4577 - val_loss: 0.4779\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4563 - val_loss: 0.4819\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4604 - val_loss: 0.4834\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4621 - val_loss: 0.4844\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.4631 - val_loss: 0.4856\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4642 - val_loss: 0.4785\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4570 - val_loss: 0.4775\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4561 - val_loss: 0.4837\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4622 - val_loss: 0.4882\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.4668 - val_loss: 0.4839\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4625 - val_loss: 0.4789\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4575 - val_loss: 0.4828\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4615 - val_loss: 0.4835\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4622 - val_loss: 0.4845\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4632 - val_loss: 0.4774\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4560 - val_loss: 0.4779\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4564 - val_loss: 0.4885\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4670 - val_loss: 0.4884\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4668 - val_loss: 0.4777\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4561 - val_loss: 0.4740\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.4523 - val_loss: 0.4811\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4594 - val_loss: 0.4850\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4634 - val_loss: 0.4833\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4618 - val_loss: 0.4821\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4608 - val_loss: 0.4833\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4620 - val_loss: 0.4842\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.4628 - val_loss: 0.4814\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4601 - val_loss: 0.4831\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4617 - val_loss: 0.4857\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4643 - val_loss: 0.4836\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4622 - val_loss: 0.4798\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.4584 - val_loss: 0.4841\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4626 - val_loss: 0.4841\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4625 - val_loss: 0.4798\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4582 - val_loss: 0.4793\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4576 - val_loss: 0.4795\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4579 - val_loss: 0.4807\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.4592 - val_loss: 0.4842\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4627 - val_loss: 0.4829\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.4616 - val_loss: 0.4814\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.4600 - val_loss: 0.4827\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.4614 - val_loss: 0.4818\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.4605 - val_loss: 0.4794\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.4580 - val_loss: 0.4845\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.4630 - val_loss: 0.4875\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.4659 - val_loss: 0.4834\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.4618 - val_loss: 0.4808\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.4594 - val_loss: 0.4827\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4613 - val_loss: 0.4843\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4630 - val_loss: 0.4830\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4618 - val_loss: 0.4771\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.4560 - val_loss: 0.4806\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4595 - val_loss: 0.4885\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4674 - val_loss: 0.4855\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4643 - val_loss: 0.4731\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4517 - val_loss: 0.4739\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4523 - val_loss: 0.4849\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4633 - val_loss: 0.4873\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4658 - val_loss: 0.4834\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4619 - val_loss: 0.4800\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4586 - val_loss: 0.4806\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4593 - val_loss: 0.4846\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4635 - val_loss: 0.4839\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4628 - val_loss: 0.4827\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4615 - val_loss: 0.4846\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.4634 - val_loss: 0.4837\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.4623 - val_loss: 0.4814\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.4599 - val_loss: 0.4836\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.4621 - val_loss: 0.4812\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4598 - val_loss: 0.4782\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4567 - val_loss: 0.4784\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4568 - val_loss: 0.4845\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4629 - val_loss: 0.4845\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4629 - val_loss: 0.4835\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4621 - val_loss: 0.4802\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4588 - val_loss: 0.4784\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4568 - val_loss: 0.4855\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.4639 - val_loss: 0.4845\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.4630 - val_loss: 0.4803\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4590 - val_loss: 0.4820\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.4608 - val_loss: 0.4861\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4648 - val_loss: 0.4855\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4640 - val_loss: 0.4807\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4591 - val_loss: 0.4827\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.4610 - val_loss: 0.4831\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4614 - val_loss: 0.4832\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4615 - val_loss: 0.4805\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4591 - val_loss: 0.4821\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4607 - val_loss: 0.4865\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4653 - val_loss: 0.4803\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.4592 - val_loss: 0.4723\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4512 - val_loss: 0.4775\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4564 - val_loss: 0.4861\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4648 - val_loss: 0.4871\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4658 - val_loss: 0.4816\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4602 - val_loss: 0.4785\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4570 - val_loss: 0.4817\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.4602 - val_loss: 0.4873\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.4659 - val_loss: 0.4838\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4624 - val_loss: 0.4812\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.4598 - val_loss: 0.4851\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4637 - val_loss: 0.4869\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4654 - val_loss: 0.4824\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.4608 - val_loss: 0.4803\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.4587 - val_loss: 0.4803\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.4589 - val_loss: 0.4789\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4576 - val_loss: 0.4815\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4601 - val_loss: 0.4854\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4638 - val_loss: 0.4825\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.4609 - val_loss: 0.4790\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.4575 - val_loss: 0.4763\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4548 - val_loss: 0.4825\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.4610 - val_loss: 0.4881\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.4666 - val_loss: 0.4836\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4621 - val_loss: 0.4793\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4579 - val_loss: 0.4826\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4613 - val_loss: 0.4872\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4660 - val_loss: 0.4844\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4631 - val_loss: 0.4800\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4587 - val_loss: 0.4810\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4597 - val_loss: 0.4833\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4619 - val_loss: 0.4874\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.4660 - val_loss: 0.4820\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4607 - val_loss: 0.4806\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4594 - val_loss: 0.4837\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4625 - val_loss: 0.4811\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.4598 - val_loss: 0.4763\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4549 - val_loss: 0.4800\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4585 - val_loss: 0.4868\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4653 - val_loss: 0.4841\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.4624 - val_loss: 0.4791\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4573 - val_loss: 0.4795\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4575 - val_loss: 0.4833\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4614 - val_loss: 0.4878\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4661 - val_loss: 0.4835\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4620 - val_loss: 0.4818\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.4605 - val_loss: 0.4874\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4662 - val_loss: 0.4875\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4661 - val_loss: 0.4790\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4575 - val_loss: 0.4776\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4562 - val_loss: 0.4799\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4583 - val_loss: 0.4826\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4609 - val_loss: 0.4849\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4630 - val_loss: 0.4853\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4633 - val_loss: 0.4799\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4581 - val_loss: 0.4773\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.4556 - val_loss: 0.4799\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.4584 - val_loss: 0.4833\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.4620 - val_loss: 0.4872\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4660 - val_loss: 0.4823\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4611 - val_loss: 0.4776\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4563 - val_loss: 0.4836\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4622 - val_loss: 0.4866\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.4653 - val_loss: 0.4837\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4623 - val_loss: 0.4788\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.4574 - val_loss: 0.4810\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4595 - val_loss: 0.4861\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.4646 - val_loss: 0.4896\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4680 - val_loss: 0.4811\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.4596 - val_loss: 0.4767\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.4552 - val_loss: 0.4837\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.4623 - val_loss: 0.4851\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4637 - val_loss: 0.4785\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4571 - val_loss: 0.4801\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.4587 - val_loss: 0.4849\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4635 - val_loss: 0.4816\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4603 - val_loss: 0.4792\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4579 - val_loss: 0.4815\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4602 - val_loss: 0.4832\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4619 - val_loss: 0.4844\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4630 - val_loss: 0.4824\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4610 - val_loss: 0.4846\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.4632 - val_loss: 0.4868\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4654 - val_loss: 0.4837\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.4623 - val_loss: 0.4784\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.4570 - val_loss: 0.4793\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.4578 - val_loss: 0.4837\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.4622 - val_loss: 0.4852\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.4638 - val_loss: 0.4835\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.4623 - val_loss: 0.4810\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.4598 - val_loss: 0.4763\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.4552 - val_loss: 0.4805\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.4593 - val_loss: 0.4823\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.4611 - val_loss: 0.4846\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4634 - val_loss: 0.4877\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4664 - val_loss: 0.4824\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4610 - val_loss: 0.4794\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.4579 - val_loss: 0.4829\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4613 - val_loss: 0.4866\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.4649 - val_loss: 0.4831\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4614 - val_loss: 0.4784\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.4568 - val_loss: 0.4831\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.4616 - val_loss: 0.4866\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.4652 - val_loss: 0.4862\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.4648 - val_loss: 0.4756\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.4543 - val_loss: 0.4749\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.4537 - val_loss: 0.4849\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.4637 - val_loss: 0.4871\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4659 - val_loss: 0.4790\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.4577 - val_loss: 0.4774\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.4560 - val_loss: 0.4828\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4613 - val_loss: 0.4838\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.4622 - val_loss: 0.4834\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.4618 - val_loss: 0.4823\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.4606 - val_loss: 0.4829\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.4612 - val_loss: 0.4859\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4642 - val_loss: 0.4859\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4642 - val_loss: 0.4855\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.4639 - val_loss: 0.4846\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.4631 - val_loss: 0.4817\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.4603 - val_loss: 0.4773\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.4559 - val_loss: 0.4833\n"
          ]
        }
      ],
      "source": [
        "h_gru_3 = model_GRU_3.fit(X_TRAIN,Y_TRAIN,\n",
        "          epochs=1000,\n",
        "          batch_size=X_TRAIN.shape[0],\n",
        "          validation_data = (X_VALID, Y_VALID),\n",
        "          verbose = True\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "VakzaA1lVrrC",
        "outputId": "88a8d31b-f870-46f7-beb8-617038d811e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b1235f4ea10>]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h_gru_3.history[\"loss\"])\n",
        "plt.plot(h_gru_3.history[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H33Z77LWtft"
      },
      "source": [
        "# 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPxCclj4Wua-",
        "outputId": "bf3ad801-f814-4de5-de80-305dabd91ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 149, 31)]         0         \n",
            "                                                                 \n",
            " gru_3 (GRU)                 (None, 100)               39900     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 31)                3131      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43,031\n",
            "Trainable params: 43,031\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(X_TRAIN.shape[1:]))\n",
        "m = inputs\n",
        "\n",
        "mA = GRU(units=100)(m)\n",
        "\n",
        "m = Flatten()(mA)\n",
        "\n",
        "# A침adir capa Dense de salida\n",
        "out = Dense(Y_TRAIN.shape[1], activation='tanh')(m)\n",
        "\n",
        "model_GRU_4 = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_GRU_4.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse',\n",
        "        metrics=[])\n",
        "\n",
        "model_GRU_4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMMfcSdLZJFp",
        "outputId": "c58aadc2-5d74-4c06-cbbe-4c411248e94f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(800, 149, 31)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_TRAIN.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_zXtjI9WxrY",
        "outputId": "b0b0fa7f-0923-4e81-affd-109ee8fe125c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3555 - val_loss: 0.3544\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3463 - val_loss: 0.3618\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3380 - val_loss: 0.3755\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3422 - val_loss: 0.3565\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3366 - val_loss: 0.3540\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3367 - val_loss: 0.3559\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3365 - val_loss: 0.3597\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3358 - val_loss: 0.3642\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3348 - val_loss: 0.3684\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3338 - val_loss: 0.3719\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3328 - val_loss: 0.3745\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3316 - val_loss: 0.3762\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3298 - val_loss: 0.3887\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3266 - val_loss: 0.4484\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5127 - val_loss: 0.3845\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3262 - val_loss: 0.3876\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3302 - val_loss: 0.3871\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3328 - val_loss: 0.3846\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3328 - val_loss: 0.3815\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3319 - val_loss: 0.3784\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3307 - val_loss: 0.3754\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3298 - val_loss: 0.3722\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3290 - val_loss: 0.3690\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3285 - val_loss: 0.3660\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3282 - val_loss: 0.3638\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3280 - val_loss: 0.3629\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3275 - val_loss: 0.3635\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3264 - val_loss: 0.3655\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3249 - val_loss: 0.3652\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3227 - val_loss: 0.3620\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3220 - val_loss: 0.4273\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3762 - val_loss: 0.3631\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3282 - val_loss: 0.3655\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3349 - val_loss: 0.3669\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3334 - val_loss: 0.3686\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3304 - val_loss: 0.3711\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3278 - val_loss: 0.3741\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3260 - val_loss: 0.3776\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3248 - val_loss: 0.3812\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3240 - val_loss: 0.3847\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3236 - val_loss: 0.3878\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3232 - val_loss: 0.3904\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3228 - val_loss: 0.3925\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3222 - val_loss: 0.3941\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3215 - val_loss: 0.3954\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3206 - val_loss: 0.3966\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3195 - val_loss: 0.3982\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3182 - val_loss: 0.4009\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3167 - val_loss: 0.4066\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3150 - val_loss: 0.4191\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3131 - val_loss: 0.4443\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3119 - val_loss: 0.4682\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3116 - val_loss: 0.4545\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3121 - val_loss: 0.4587\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3086 - val_loss: 0.4681\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3081 - val_loss: 0.4426\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3059 - val_loss: 0.4284\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3056 - val_loss: 0.4255\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3050 - val_loss: 0.4319\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3035 - val_loss: 0.4475\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3020 - val_loss: 0.4659\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3005 - val_loss: 0.4720\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2982 - val_loss: 0.4961\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2974 - val_loss: 0.5463\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3025 - val_loss: 0.4116\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3206 - val_loss: 0.4023\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3208 - val_loss: 0.4044\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3146 - val_loss: 0.4089\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3107 - val_loss: 0.4145\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3079 - val_loss: 0.4209\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3063 - val_loss: 0.4265\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3061 - val_loss: 0.4281\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3059 - val_loss: 0.4249\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3050 - val_loss: 0.4210\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3048 - val_loss: 0.4204\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3040 - val_loss: 0.4248\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3019 - val_loss: 0.4311\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3008 - val_loss: 0.4301\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2988 - val_loss: 0.4258\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2977 - val_loss: 0.4283\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2953 - val_loss: 0.4354\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2938 - val_loss: 0.4322\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2917 - val_loss: 0.4294\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2904 - val_loss: 0.4404\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2879 - val_loss: 0.4507\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2865 - val_loss: 0.4474\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2842 - val_loss: 0.4490\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2817 - val_loss: 0.4574\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2795 - val_loss: 0.4620\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2773 - val_loss: 0.4919\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2792 - val_loss: 0.4393\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2954 - val_loss: 0.4571\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2763 - val_loss: 0.4577\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2797 - val_loss: 0.4559\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2813 - val_loss: 0.4649\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2761 - val_loss: 0.4720\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2731 - val_loss: 0.4688\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2701 - val_loss: 0.4721\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2672 - val_loss: 0.4789\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2662 - val_loss: 0.4849\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2628 - val_loss: 0.4795\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2607 - val_loss: 0.4817\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2577 - val_loss: 0.4972\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2552 - val_loss: 0.5050\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2516 - val_loss: 0.5045\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2488 - val_loss: 0.5219\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2452 - val_loss: 0.5149\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2413 - val_loss: 0.5231\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2372 - val_loss: 0.5330\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2345 - val_loss: 0.5378\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2312 - val_loss: 0.5417\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2275 - val_loss: 0.5489\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2245 - val_loss: 0.5586\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2215 - val_loss: 0.5629\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2186 - val_loss: 0.5559\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2152 - val_loss: 0.5508\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2120 - val_loss: 0.5511\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2097 - val_loss: 0.5518\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2080 - val_loss: 0.5467\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2127 - val_loss: 0.5508\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2070 - val_loss: 0.5426\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2014 - val_loss: 0.5365\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1999 - val_loss: 0.5359\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1972 - val_loss: 0.5366\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1943 - val_loss: 0.5356\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1930 - val_loss: 0.5327\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1895 - val_loss: 0.5312\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1867 - val_loss: 0.5314\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1834 - val_loss: 0.5323\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1813 - val_loss: 0.5342\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1788 - val_loss: 0.5341\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1758 - val_loss: 0.5297\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1728 - val_loss: 0.5298\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1715 - val_loss: 0.5312\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1676 - val_loss: 0.5294\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1662 - val_loss: 0.5306\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1649 - val_loss: 0.5308\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1629 - val_loss: 0.5339\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1593 - val_loss: 0.5362\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1560 - val_loss: 0.5438\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1534 - val_loss: 0.5496\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1518 - val_loss: 0.5403\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1498 - val_loss: 0.5415\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1476 - val_loss: 0.5464\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1463 - val_loss: 0.5560\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1411 - val_loss: 0.5459\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1399 - val_loss: 0.5522\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1393 - val_loss: 0.5583\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1389 - val_loss: 0.5625\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1408 - val_loss: 0.5539\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1347 - val_loss: 0.5578\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1360 - val_loss: 0.5708\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1308 - val_loss: 0.5598\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1293 - val_loss: 0.5700\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1246 - val_loss: 0.5740\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1232 - val_loss: 0.5702\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1213 - val_loss: 0.5673\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1195 - val_loss: 0.5741\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1159 - val_loss: 0.5844\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1146 - val_loss: 0.5820\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1133 - val_loss: 0.5788\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1102 - val_loss: 0.5840\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1089 - val_loss: 0.5841\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1068 - val_loss: 0.5867\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1056 - val_loss: 0.5944\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1034 - val_loss: 0.5908\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1020 - val_loss: 0.6015\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1001 - val_loss: 0.5936\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0982 - val_loss: 0.6000\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0968 - val_loss: 0.6078\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0952 - val_loss: 0.5917\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0939 - val_loss: 0.6141\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0931 - val_loss: 0.6034\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0937 - val_loss: 0.6140\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0931 - val_loss: 0.6069\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0900 - val_loss: 0.6026\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0879 - val_loss: 0.6202\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0877 - val_loss: 0.6105\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0852 - val_loss: 0.6159\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0841 - val_loss: 0.6146\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0821 - val_loss: 0.6117\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0804 - val_loss: 0.6234\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0797 - val_loss: 0.6167\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0782 - val_loss: 0.6202\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0766 - val_loss: 0.6152\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0758 - val_loss: 0.6244\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0740 - val_loss: 0.6207\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0731 - val_loss: 0.6191\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0719 - val_loss: 0.6263\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0708 - val_loss: 0.6225\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0698 - val_loss: 0.6269\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0684 - val_loss: 0.6218\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0674 - val_loss: 0.6209\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0665 - val_loss: 0.6227\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0655 - val_loss: 0.6325\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0652 - val_loss: 0.6125\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0670 - val_loss: 0.6399\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0728 - val_loss: 0.5994\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0694 - val_loss: 0.6047\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0647 - val_loss: 0.6204\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0666 - val_loss: 0.6136\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0627 - val_loss: 0.5959\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0638 - val_loss: 0.5963\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0611 - val_loss: 0.6120\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0604 - val_loss: 0.6188\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0596 - val_loss: 0.6032\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0584 - val_loss: 0.5924\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0577 - val_loss: 0.5963\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0567 - val_loss: 0.6019\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0561 - val_loss: 0.6033\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0554 - val_loss: 0.5988\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0548 - val_loss: 0.5906\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0547 - val_loss: 0.5968\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0538 - val_loss: 0.5966\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0529 - val_loss: 0.5963\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0522 - val_loss: 0.5975\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0518 - val_loss: 0.5953\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0511 - val_loss: 0.5979\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0506 - val_loss: 0.6033\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0499 - val_loss: 0.6012\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0496 - val_loss: 0.6034\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0487 - val_loss: 0.6052\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0485 - val_loss: 0.6020\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0480 - val_loss: 0.6041\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0475 - val_loss: 0.6048\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0470 - val_loss: 0.6061\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0465 - val_loss: 0.6062\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0461 - val_loss: 0.6051\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0458 - val_loss: 0.6078\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0455 - val_loss: 0.6074\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0453 - val_loss: 0.6089\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0449 - val_loss: 0.6062\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0448 - val_loss: 0.6085\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0453 - val_loss: 0.6121\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0484 - val_loss: 0.6039\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0551 - val_loss: 0.6163\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0590 - val_loss: 0.5966\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0507 - val_loss: 0.5901\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0541 - val_loss: 0.6030\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0496 - val_loss: 0.6115\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0492 - val_loss: 0.6062\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0487 - val_loss: 0.6001\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0475 - val_loss: 0.5988\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0468 - val_loss: 0.5998\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0461 - val_loss: 0.5986\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0451 - val_loss: 0.5957\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0449 - val_loss: 0.5962\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0441 - val_loss: 0.6013\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0436 - val_loss: 0.6071\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0433 - val_loss: 0.6081\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0424 - val_loss: 0.6067\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0423 - val_loss: 0.6077\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0418 - val_loss: 0.6091\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0410 - val_loss: 0.6062\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0410 - val_loss: 0.6015\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0407 - val_loss: 0.6018\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0403 - val_loss: 0.6048\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0397 - val_loss: 0.6051\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0395 - val_loss: 0.6050\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0393 - val_loss: 0.6049\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0389 - val_loss: 0.6032\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0386 - val_loss: 0.6057\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0382 - val_loss: 0.6087\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0381 - val_loss: 0.6079\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0378 - val_loss: 0.6082\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0374 - val_loss: 0.6105\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0373 - val_loss: 0.6135\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0370 - val_loss: 0.6170\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0368 - val_loss: 0.6158\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0365 - val_loss: 0.6144\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0364 - val_loss: 0.6159\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0361 - val_loss: 0.6153\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0359 - val_loss: 0.6158\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0357 - val_loss: 0.6170\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0355 - val_loss: 0.6184\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0353 - val_loss: 0.6187\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0351 - val_loss: 0.6179\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0349 - val_loss: 0.6177\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0347 - val_loss: 0.6183\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0347 - val_loss: 0.6189\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0349 - val_loss: 0.6194\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0364 - val_loss: 0.6208\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0410 - val_loss: 0.6175\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0415 - val_loss: 0.6107\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0380 - val_loss: 0.6138\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0392 - val_loss: 0.6123\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0377 - val_loss: 0.6060\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0370 - val_loss: 0.6016\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0367 - val_loss: 0.6062\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0366 - val_loss: 0.6094\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0354 - val_loss: 0.6157\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0358 - val_loss: 0.6164\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0349 - val_loss: 0.6195\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0348 - val_loss: 0.6189\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0343 - val_loss: 0.6200\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0342 - val_loss: 0.6220\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0339 - val_loss: 0.6230\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0336 - val_loss: 0.6228\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0333 - val_loss: 0.6233\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0332 - val_loss: 0.6258\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0329 - val_loss: 0.6239\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0327 - val_loss: 0.6221\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0325 - val_loss: 0.6252\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0322 - val_loss: 0.6279\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0321 - val_loss: 0.6264\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0319 - val_loss: 0.6237\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0316 - val_loss: 0.6237\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0316 - val_loss: 0.6260\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0313 - val_loss: 0.6300\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0313 - val_loss: 0.6311\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0310 - val_loss: 0.6294\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0309 - val_loss: 0.6293\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0307 - val_loss: 0.6318\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0306 - val_loss: 0.6334\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0304 - val_loss: 0.6327\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0303 - val_loss: 0.6296\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0302 - val_loss: 0.6310\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0300 - val_loss: 0.6315\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0300 - val_loss: 0.6334\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0300 - val_loss: 0.6325\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0305 - val_loss: 0.6336\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0308 - val_loss: 0.6295\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0307 - val_loss: 0.6315\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0297 - val_loss: 0.6317\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0294 - val_loss: 0.6303\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0297 - val_loss: 0.6317\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0297 - val_loss: 0.6317\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0290 - val_loss: 0.6338\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0290 - val_loss: 0.6364\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0292 - val_loss: 0.6348\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0288 - val_loss: 0.6361\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0285 - val_loss: 0.6358\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0286 - val_loss: 0.6356\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0286 - val_loss: 0.6389\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0286 - val_loss: 0.6344\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0289 - val_loss: 0.6342\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0299 - val_loss: 0.6377\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0317 - val_loss: 0.6327\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0301 - val_loss: 0.6282\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0297 - val_loss: 0.6306\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0306 - val_loss: 0.6282\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0293 - val_loss: 0.6263\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0294 - val_loss: 0.6263\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0290 - val_loss: 0.6322\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0292 - val_loss: 0.6308\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0286 - val_loss: 0.6270\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0288 - val_loss: 0.6297\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0282 - val_loss: 0.6368\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0287 - val_loss: 0.6345\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0276 - val_loss: 0.6270\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0281 - val_loss: 0.6323\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0274 - val_loss: 0.6367\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0277 - val_loss: 0.6351\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0272 - val_loss: 0.6321\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0275 - val_loss: 0.6355\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0272 - val_loss: 0.6378\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0270 - val_loss: 0.6371\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0267 - val_loss: 0.6349\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0267 - val_loss: 0.6354\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0267 - val_loss: 0.6399\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0268 - val_loss: 0.6363\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0270 - val_loss: 0.6356\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0269 - val_loss: 0.6356\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0265 - val_loss: 0.6395\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0264 - val_loss: 0.6418\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0262 - val_loss: 0.6356\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0264 - val_loss: 0.6403\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0262 - val_loss: 0.6390\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0260 - val_loss: 0.6388\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0258 - val_loss: 0.6411\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0261 - val_loss: 0.6354\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0269 - val_loss: 0.6417\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0272 - val_loss: 0.6323\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0268 - val_loss: 0.6385\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0263 - val_loss: 0.6390\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0264 - val_loss: 0.6319\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0263 - val_loss: 0.6336\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0263 - val_loss: 0.6431\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0259 - val_loss: 0.6354\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0258 - val_loss: 0.6377\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0255 - val_loss: 0.6418\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0254 - val_loss: 0.6338\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0253 - val_loss: 0.6371\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0251 - val_loss: 0.6415\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0253 - val_loss: 0.6378\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0246 - val_loss: 0.6375\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0247 - val_loss: 0.6414\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0247 - val_loss: 0.6386\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0243 - val_loss: 0.6373\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0244 - val_loss: 0.6429\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0243 - val_loss: 0.6391\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0240 - val_loss: 0.6370\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0240 - val_loss: 0.6423\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0241 - val_loss: 0.6434\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0242 - val_loss: 0.6367\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0247 - val_loss: 0.6461\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0259 - val_loss: 0.6346\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0287 - val_loss: 0.6382\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0269 - val_loss: 0.6372\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0275 - val_loss: 0.6247\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0280 - val_loss: 0.6322\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0263 - val_loss: 0.6312\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0271 - val_loss: 0.6256\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0252 - val_loss: 0.6170\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0257 - val_loss: 0.6204\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0250 - val_loss: 0.6335\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0253 - val_loss: 0.6321\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0246 - val_loss: 0.6247\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0247 - val_loss: 0.6276\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0244 - val_loss: 0.6318\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0242 - val_loss: 0.6315\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0239 - val_loss: 0.6331\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0242 - val_loss: 0.6307\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0237 - val_loss: 0.6352\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0239 - val_loss: 0.6373\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0233 - val_loss: 0.6336\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0235 - val_loss: 0.6366\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0236 - val_loss: 0.6389\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0234 - val_loss: 0.6356\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0229 - val_loss: 0.6395\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0232 - val_loss: 0.6420\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0229 - val_loss: 0.6420\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0228 - val_loss: 0.6413\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0226 - val_loss: 0.6408\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0227 - val_loss: 0.6426\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0224 - val_loss: 0.6407\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0222 - val_loss: 0.6428\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0222 - val_loss: 0.6457\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0222 - val_loss: 0.6440\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0221 - val_loss: 0.6424\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0220 - val_loss: 0.6409\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0219 - val_loss: 0.6439\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0218 - val_loss: 0.6440\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0218 - val_loss: 0.6424\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0218 - val_loss: 0.6467\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0219 - val_loss: 0.6412\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0219 - val_loss: 0.6454\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0220 - val_loss: 0.6410\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0227 - val_loss: 0.6462\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0241 - val_loss: 0.6401\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0239 - val_loss: 0.6446\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0228 - val_loss: 0.6454\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0227 - val_loss: 0.6444\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0238 - val_loss: 0.6374\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0224 - val_loss: 0.6452\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0224 - val_loss: 0.6431\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0225 - val_loss: 0.6437\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0215 - val_loss: 0.6503\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0222 - val_loss: 0.6412\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0215 - val_loss: 0.6438\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0217 - val_loss: 0.6488\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0214 - val_loss: 0.6439\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0212 - val_loss: 0.6452\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0213 - val_loss: 0.6501\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0209 - val_loss: 0.6504\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0212 - val_loss: 0.6482\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0211 - val_loss: 0.6452\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0211 - val_loss: 0.6517\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0214 - val_loss: 0.6527\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0225 - val_loss: 0.6446\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0228 - val_loss: 0.6497\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0224 - val_loss: 0.6456\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0221 - val_loss: 0.6502\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0227 - val_loss: 0.6410\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0220 - val_loss: 0.6395\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0215 - val_loss: 0.6438\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0220 - val_loss: 0.6370\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0215 - val_loss: 0.6393\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0213 - val_loss: 0.6472\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0213 - val_loss: 0.6434\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0210 - val_loss: 0.6401\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0210 - val_loss: 0.6420\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0209 - val_loss: 0.6442\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0209 - val_loss: 0.6467\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0208 - val_loss: 0.6433\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0205 - val_loss: 0.6460\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0204 - val_loss: 0.6492\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0202 - val_loss: 0.6477\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0202 - val_loss: 0.6513\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0202 - val_loss: 0.6517\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0202 - val_loss: 0.6497\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0199 - val_loss: 0.6542\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0197 - val_loss: 0.6553\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0200 - val_loss: 0.6539\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0197 - val_loss: 0.6508\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0198 - val_loss: 0.6552\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0199 - val_loss: 0.6488\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0205 - val_loss: 0.6543\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0213 - val_loss: 0.6441\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0212 - val_loss: 0.6559\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0210 - val_loss: 0.6527\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0208 - val_loss: 0.6493\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0216 - val_loss: 0.6503\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0208 - val_loss: 0.6514\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0202 - val_loss: 0.6550\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0207 - val_loss: 0.6474\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0210 - val_loss: 0.6487\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0203 - val_loss: 0.6486\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0206 - val_loss: 0.6420\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0199 - val_loss: 0.6493\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0208 - val_loss: 0.6423\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0199 - val_loss: 0.6498\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0202 - val_loss: 0.6486\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0195 - val_loss: 0.6439\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0198 - val_loss: 0.6511\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0197 - val_loss: 0.6471\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0196 - val_loss: 0.6501\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0193 - val_loss: 0.6495\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0193 - val_loss: 0.6455\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0194 - val_loss: 0.6490\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0191 - val_loss: 0.6506\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0189 - val_loss: 0.6482\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0191 - val_loss: 0.6472\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0191 - val_loss: 0.6462\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0189 - val_loss: 0.6441\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0191 - val_loss: 0.6514\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0200 - val_loss: 0.6424\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0196 - val_loss: 0.6443\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0195 - val_loss: 0.6508\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0193 - val_loss: 0.6409\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0194 - val_loss: 0.6518\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0194 - val_loss: 0.6498\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0190 - val_loss: 0.6482\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0189 - val_loss: 0.6459\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0186 - val_loss: 0.6493\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0187 - val_loss: 0.6475\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0184 - val_loss: 0.6464\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0182 - val_loss: 0.6459\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0186 - val_loss: 0.6498\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0186 - val_loss: 0.6433\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0186 - val_loss: 0.6489\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0183 - val_loss: 0.6498\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0184 - val_loss: 0.6485\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0183 - val_loss: 0.6428\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0185 - val_loss: 0.6563\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0185 - val_loss: 0.6471\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0183 - val_loss: 0.6537\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0182 - val_loss: 0.6548\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0183 - val_loss: 0.6462\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0183 - val_loss: 0.6569\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0189 - val_loss: 0.6416\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0187 - val_loss: 0.6527\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0186 - val_loss: 0.6494\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0181 - val_loss: 0.6470\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0184 - val_loss: 0.6541\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0185 - val_loss: 0.6503\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0180 - val_loss: 0.6456\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0177 - val_loss: 0.6565\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0181 - val_loss: 0.6519\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0175 - val_loss: 0.6488\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0174 - val_loss: 0.6558\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0176 - val_loss: 0.6492\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0174 - val_loss: 0.6534\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0173 - val_loss: 0.6529\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0176 - val_loss: 0.6540\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0177 - val_loss: 0.6451\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0185 - val_loss: 0.6538\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0191 - val_loss: 0.6483\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0196 - val_loss: 0.6514\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0192 - val_loss: 0.6502\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0202 - val_loss: 0.6414\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0187 - val_loss: 0.6472\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0192 - val_loss: 0.6497\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0189 - val_loss: 0.6502\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0183 - val_loss: 0.6445\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0187 - val_loss: 0.6401\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0179 - val_loss: 0.6513\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0181 - val_loss: 0.6535\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0179 - val_loss: 0.6462\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0178 - val_loss: 0.6549\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0175 - val_loss: 0.6477\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0174 - val_loss: 0.6449\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0174 - val_loss: 0.6492\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0171 - val_loss: 0.6550\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0173 - val_loss: 0.6576\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0172 - val_loss: 0.6486\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0174 - val_loss: 0.6562\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0172 - val_loss: 0.6549\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0177 - val_loss: 0.6504\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0180 - val_loss: 0.6492\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0184 - val_loss: 0.6501\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0176 - val_loss: 0.6526\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0172 - val_loss: 0.6442\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0168 - val_loss: 0.6476\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0173 - val_loss: 0.6444\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0168 - val_loss: 0.6416\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0166 - val_loss: 0.6483\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0166 - val_loss: 0.6495\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0165 - val_loss: 0.6463\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0163 - val_loss: 0.6502\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0161 - val_loss: 0.6522\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0163 - val_loss: 0.6457\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0161 - val_loss: 0.6495\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0160 - val_loss: 0.6568\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0160 - val_loss: 0.6556\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0159 - val_loss: 0.6550\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0161 - val_loss: 0.6543\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0164 - val_loss: 0.6532\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0169 - val_loss: 0.6609\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0176 - val_loss: 0.6398\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0176 - val_loss: 0.6589\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0170 - val_loss: 0.6505\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0168 - val_loss: 0.6457\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0174 - val_loss: 0.6519\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0166 - val_loss: 0.6553\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0170 - val_loss: 0.6598\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0177 - val_loss: 0.6489\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0172 - val_loss: 0.6516\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0185 - val_loss: 0.6558\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0178 - val_loss: 0.6536\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0182 - val_loss: 0.6546\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0171 - val_loss: 0.6537\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0172 - val_loss: 0.6509\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0171 - val_loss: 0.6590\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0166 - val_loss: 0.6654\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0165 - val_loss: 0.6554\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0166 - val_loss: 0.6562\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0164 - val_loss: 0.6586\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0159 - val_loss: 0.6531\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0162 - val_loss: 0.6577\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0163 - val_loss: 0.6567\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0159 - val_loss: 0.6508\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0158 - val_loss: 0.6626\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0158 - val_loss: 0.6577\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0155 - val_loss: 0.6551\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0157 - val_loss: 0.6616\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0155 - val_loss: 0.6564\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0154 - val_loss: 0.6585\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0154 - val_loss: 0.6587\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0156 - val_loss: 0.6575\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0156 - val_loss: 0.6600\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0160 - val_loss: 0.6493\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0163 - val_loss: 0.6513\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0163 - val_loss: 0.6574\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0157 - val_loss: 0.6543\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0156 - val_loss: 0.6564\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0159 - val_loss: 0.6507\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0152 - val_loss: 0.6582\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0151 - val_loss: 0.6579\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0156 - val_loss: 0.6528\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0152 - val_loss: 0.6600\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0149 - val_loss: 0.6544\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0153 - val_loss: 0.6611\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0149 - val_loss: 0.6561\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0149 - val_loss: 0.6598\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0152 - val_loss: 0.6563\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0148 - val_loss: 0.6619\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0147 - val_loss: 0.6606\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0152 - val_loss: 0.6601\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0154 - val_loss: 0.6611\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0158 - val_loss: 0.6602\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0166 - val_loss: 0.6689\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0178 - val_loss: 0.6491\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0168 - val_loss: 0.6609\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0165 - val_loss: 0.6589\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0168 - val_loss: 0.6526\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0163 - val_loss: 0.6550\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0160 - val_loss: 0.6624\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0162 - val_loss: 0.6613\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0156 - val_loss: 0.6564\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0160 - val_loss: 0.6517\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0156 - val_loss: 0.6651\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0153 - val_loss: 0.6649\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0155 - val_loss: 0.6599\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0154 - val_loss: 0.6563\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0151 - val_loss: 0.6649\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0151 - val_loss: 0.6635\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0150 - val_loss: 0.6608\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0149 - val_loss: 0.6580\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0146 - val_loss: 0.6650\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0144 - val_loss: 0.6655\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0145 - val_loss: 0.6625\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0144 - val_loss: 0.6615\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0143 - val_loss: 0.6679\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0142 - val_loss: 0.6615\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0140 - val_loss: 0.6637\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0139 - val_loss: 0.6660\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0140 - val_loss: 0.6636\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6638\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6655\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0140 - val_loss: 0.6670\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0144 - val_loss: 0.6615\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0156 - val_loss: 0.6676\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0170 - val_loss: 0.6701\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0178 - val_loss: 0.6578\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0159 - val_loss: 0.6659\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0158 - val_loss: 0.6604\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0161 - val_loss: 0.6648\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0148 - val_loss: 0.6693\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0158 - val_loss: 0.6631\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0150 - val_loss: 0.6682\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0146 - val_loss: 0.6706\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0153 - val_loss: 0.6727\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0143 - val_loss: 0.6583\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0151 - val_loss: 0.6752\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0142 - val_loss: 0.6671\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0146 - val_loss: 0.6677\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0143 - val_loss: 0.6672\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0142 - val_loss: 0.6739\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0142 - val_loss: 0.6804\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0140 - val_loss: 0.6640\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0142 - val_loss: 0.6754\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0140 - val_loss: 0.6713\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0143 - val_loss: 0.6718\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0142 - val_loss: 0.6720\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0137 - val_loss: 0.6735\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6783\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0141 - val_loss: 0.6676\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0139 - val_loss: 0.6774\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0143 - val_loss: 0.6706\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0144 - val_loss: 0.6742\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0143 - val_loss: 0.6719\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6684\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0137 - val_loss: 0.6742\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6716\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6718\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0136 - val_loss: 0.6794\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0137 - val_loss: 0.6679\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0136 - val_loss: 0.6798\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0135 - val_loss: 0.6701\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0135 - val_loss: 0.6804\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0140 - val_loss: 0.6725\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0142 - val_loss: 0.6839\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0141 - val_loss: 0.6639\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6813\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0137 - val_loss: 0.6697\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0134 - val_loss: 0.6700\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0135 - val_loss: 0.6787\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0136 - val_loss: 0.6665\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0137 - val_loss: 0.6756\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0139 - val_loss: 0.6702\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6692\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0139 - val_loss: 0.6735\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0135 - val_loss: 0.6623\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0136 - val_loss: 0.6744\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6742\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - val_loss: 0.6693\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0137 - val_loss: 0.6741\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0136 - val_loss: 0.6731\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0135 - val_loss: 0.6723\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0133 - val_loss: 0.6717\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - val_loss: 0.6731\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - val_loss: 0.6751\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0132 - val_loss: 0.6764\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0130 - val_loss: 0.6786\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0129 - val_loss: 0.6649\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0130 - val_loss: 0.6741\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0132 - val_loss: 0.6831\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0131 - val_loss: 0.6690\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0135 - val_loss: 0.6766\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0135 - val_loss: 0.6735\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0136 - val_loss: 0.6770\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0133 - val_loss: 0.6726\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0127 - val_loss: 0.6753\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0127 - val_loss: 0.6839\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - val_loss: 0.6680\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0128 - val_loss: 0.6765\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0125 - val_loss: 0.6770\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0127 - val_loss: 0.6771\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0128 - val_loss: 0.6754\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - val_loss: 0.6731\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0132 - val_loss: 0.6671\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0136 - val_loss: 0.6797\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0133 - val_loss: 0.6643\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0130 - val_loss: 0.6688\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0132 - val_loss: 0.6703\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0132 - val_loss: 0.6688\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0129 - val_loss: 0.6724\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0130 - val_loss: 0.6673\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0131 - val_loss: 0.6774\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0129 - val_loss: 0.6784\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0126 - val_loss: 0.6637\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - val_loss: 0.6820\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0133 - val_loss: 0.6667\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6821\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0147 - val_loss: 0.6615\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0147 - val_loss: 0.6780\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0137 - val_loss: 0.6758\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0146 - val_loss: 0.6616\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0138 - val_loss: 0.6769\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0137 - val_loss: 0.6665\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0138 - val_loss: 0.6627\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0136 - val_loss: 0.6773\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0133 - val_loss: 0.6735\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0133 - val_loss: 0.6696\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0130 - val_loss: 0.6671\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0127 - val_loss: 0.6716\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0128 - val_loss: 0.6716\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0126 - val_loss: 0.6679\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0123 - val_loss: 0.6763\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0124 - val_loss: 0.6768\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - val_loss: 0.6749\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0123 - val_loss: 0.6720\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0121 - val_loss: 0.6741\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0121 - val_loss: 0.6807\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0121 - val_loss: 0.6773\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0121 - val_loss: 0.6739\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0120 - val_loss: 0.6809\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0119 - val_loss: 0.6723\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0119 - val_loss: 0.6795\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0118 - val_loss: 0.6724\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0120 - val_loss: 0.6807\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0121 - val_loss: 0.6763\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0123 - val_loss: 0.6687\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0124 - val_loss: 0.6790\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - val_loss: 0.6722\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0119 - val_loss: 0.6762\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - val_loss: 0.6682\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0123 - val_loss: 0.6838\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0120 - val_loss: 0.6714\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0121 - val_loss: 0.6743\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0125 - val_loss: 0.6756\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0128 - val_loss: 0.6707\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0144 - val_loss: 0.6726\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0170 - val_loss: 0.6713\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0163 - val_loss: 0.6634\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0144 - val_loss: 0.6572\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0147 - val_loss: 0.6730\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0139 - val_loss: 0.6670\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0142 - val_loss: 0.6648\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0132 - val_loss: 0.6678\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0135 - val_loss: 0.6567\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0131 - val_loss: 0.6629\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - val_loss: 0.6671\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0127 - val_loss: 0.6674\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0127 - val_loss: 0.6720\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0124 - val_loss: 0.6686\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - val_loss: 0.6663\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - val_loss: 0.6688\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0119 - val_loss: 0.6719\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0119 - val_loss: 0.6727\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0117 - val_loss: 0.6669\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0118 - val_loss: 0.6712\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0116 - val_loss: 0.6755\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0118 - val_loss: 0.6742\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0119 - val_loss: 0.6734\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0123 - val_loss: 0.6723\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0119 - val_loss: 0.6763\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0119 - val_loss: 0.6712\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0117 - val_loss: 0.6711\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0117 - val_loss: 0.6687\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6697\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0116 - val_loss: 0.6744\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0119 - val_loss: 0.6698\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0118 - val_loss: 0.6661\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6731\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0116 - val_loss: 0.6654\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - val_loss: 0.6766\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - val_loss: 0.6687\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - val_loss: 0.6758\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0116 - val_loss: 0.6683\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - val_loss: 0.6735\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - val_loss: 0.6686\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0112 - val_loss: 0.6787\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0117 - val_loss: 0.6676\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - val_loss: 0.6777\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - val_loss: 0.6675\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0118 - val_loss: 0.6836\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6664\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0117 - val_loss: 0.6779\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0125 - val_loss: 0.6660\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0118 - val_loss: 0.6661\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0118 - val_loss: 0.6766\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0117 - val_loss: 0.6638\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0113 - val_loss: 0.6752\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - val_loss: 0.6659\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0112 - val_loss: 0.6745\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - val_loss: 0.6673\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - val_loss: 0.6647\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0109 - val_loss: 0.6763\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0109 - val_loss: 0.6666\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6702\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - val_loss: 0.6704\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6702\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0109 - val_loss: 0.6727\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6677\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0112 - val_loss: 0.6753\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0116 - val_loss: 0.6685\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - val_loss: 0.6690\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6716\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - val_loss: 0.6622\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0118 - val_loss: 0.6772\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0120 - val_loss: 0.6669\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6717\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0111 - val_loss: 0.6678\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6720\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - val_loss: 0.6699\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0107 - val_loss: 0.6729\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - val_loss: 0.6659\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - val_loss: 0.6728\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0108 - val_loss: 0.6734\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0112 - val_loss: 0.6675\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - val_loss: 0.6708\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0107 - val_loss: 0.6755\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0106 - val_loss: 0.6654\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0108 - val_loss: 0.6753\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0108 - val_loss: 0.6712\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0105 - val_loss: 0.6729\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - val_loss: 0.6720\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6786\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0109 - val_loss: 0.6737\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6762\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0108 - val_loss: 0.6784\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0112 - val_loss: 0.6654\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6863\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0116 - val_loss: 0.6780\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - val_loss: 0.6672\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - val_loss: 0.6794\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - val_loss: 0.6713\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0131 - val_loss: 0.6753\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0117 - val_loss: 0.6797\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0121 - val_loss: 0.6671\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0125 - val_loss: 0.6815\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0116 - val_loss: 0.6809\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0120 - val_loss: 0.6689\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0113 - val_loss: 0.6730\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0114 - val_loss: 0.6858\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6692\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6755\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0109 - val_loss: 0.6805\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6842\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0105 - val_loss: 0.6759\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0109 - val_loss: 0.6799\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0108 - val_loss: 0.6785\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0107 - val_loss: 0.6806\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0108 - val_loss: 0.6801\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0105 - val_loss: 0.6744\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0101 - val_loss: 0.6758\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0103 - val_loss: 0.6792\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0102 - val_loss: 0.6847\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0103 - val_loss: 0.6744\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0102 - val_loss: 0.6824\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0102 - val_loss: 0.6766\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0105 - val_loss: 0.6842\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0109 - val_loss: 0.6728\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0111 - val_loss: 0.6885\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0108 - val_loss: 0.6685\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0108 - val_loss: 0.6754\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0108 - val_loss: 0.6789\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0109 - val_loss: 0.6718\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6793\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0103 - val_loss: 0.6733\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0106 - val_loss: 0.6729\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0108 - val_loss: 0.6737\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0101 - val_loss: 0.6766\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0103 - val_loss: 0.6745\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0107 - val_loss: 0.6718\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0103 - val_loss: 0.6817\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0104 - val_loss: 0.6720\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0106 - val_loss: 0.6811\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0107 - val_loss: 0.6703\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0107 - val_loss: 0.6790\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0105 - val_loss: 0.6744\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0105 - val_loss: 0.6749\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0103 - val_loss: 0.6802\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0102 - val_loss: 0.6704\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0102 - val_loss: 0.6752\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0100 - val_loss: 0.6759\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0098 - val_loss: 0.6768\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0099 - val_loss: 0.6783\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0098 - val_loss: 0.6705\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0098 - val_loss: 0.6797\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0096 - val_loss: 0.6766\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0096 - val_loss: 0.6757\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0100 - val_loss: 0.6806\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0102 - val_loss: 0.6797\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6795\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0120 - val_loss: 0.6712\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0120 - val_loss: 0.6892\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0121 - val_loss: 0.6757\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0115 - val_loss: 0.6779\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0122 - val_loss: 0.6831\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0113 - val_loss: 0.6742\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6761\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0119 - val_loss: 0.6734\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0104 - val_loss: 0.6788\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0114 - val_loss: 0.6693\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0110 - val_loss: 0.6718\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0105 - val_loss: 0.6761\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0109 - val_loss: 0.6714\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0101 - val_loss: 0.6719\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0104 - val_loss: 0.6762\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0101 - val_loss: 0.6780\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0102 - val_loss: 0.6794\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0098 - val_loss: 0.6775\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0100 - val_loss: 0.6800\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0098 - val_loss: 0.6803\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0097 - val_loss: 0.6794\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0097 - val_loss: 0.6809\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0095 - val_loss: 0.6828\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0095 - val_loss: 0.6754\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0094 - val_loss: 0.6813\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0093 - val_loss: 0.6788\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0092 - val_loss: 0.6793\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0092 - val_loss: 0.6818\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0093 - val_loss: 0.6839\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0094 - val_loss: 0.6734\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0094 - val_loss: 0.6892\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0099 - val_loss: 0.6768\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0102 - val_loss: 0.6917\n"
          ]
        }
      ],
      "source": [
        "h_gru_4 = model_GRU_4.fit(X_TRAIN,Y_TRAIN,\n",
        "          epochs=1000,\n",
        "          batch_size=X_TRAIN.shape[0],\n",
        "          validation_data = (X_VALID, Y_VALID),\n",
        "          verbose = True\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8UvCpFGXW2dc",
        "outputId": "634fabd5-e769-4e91-b08a-56b76691a824"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b1234fed780>]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h_gru_4.history[\"loss\"])\n",
        "plt.plot(h_gru_4.history[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdIjyN_JdVSW"
      },
      "source": [
        "# 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MF9tvLapcpAC",
        "outputId": "0ec4a81a-1c17-474a-b87e-b9a6379c24a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 149, 31)]         0         \n",
            "                                                                 \n",
            " gru_4 (GRU)                 (None, 50)                12450     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 31)                1581      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,581\n",
            "Trainable params: 16,581\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(X_TRAIN.shape[1:]))\n",
        "m = inputs\n",
        "\n",
        "mA = GRU(units=50)(m)\n",
        "mA = Dense(50)(mA)\n",
        "m = Flatten()(mA)\n",
        "\n",
        "# A침adir capa Dense de salida\n",
        "out = Dense(Y_TRAIN.shape[1], activation='tanh')(m)\n",
        "\n",
        "model_GRU_5 = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_GRU_5.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse',\n",
        "        metrics=[])\n",
        "\n",
        "model_GRU_5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R94kQuCGcxJt",
        "outputId": "8500bee7-0668-4ee2-973d-b5d44c8e6c43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3556 - val_loss: 0.3592\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 0.3477 - val_loss: 0.3688\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 1s 598ms/step - loss: 0.3410 - val_loss: 0.3742\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.3393 - val_loss: 0.3645\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.3375 - val_loss: 0.3602\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.3362 - val_loss: 0.3610\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.3357 - val_loss: 0.3635\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 0.3355 - val_loss: 0.3662\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 1s 606ms/step - loss: 0.3350 - val_loss: 0.3698\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.3340 - val_loss: 0.3747\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.3327 - val_loss: 0.3813\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 1s 825ms/step - loss: 0.3312 - val_loss: 0.3961\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.3296 - val_loss: 0.3994\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.3466 - val_loss: 0.4122\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.3306 - val_loss: 0.4159\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 0.3362 - val_loss: 0.3989\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 0.3312 - val_loss: 0.3845\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 1s 574ms/step - loss: 0.3288 - val_loss: 0.3758\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 0.3288 - val_loss: 0.3714\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 0.3295 - val_loss: 0.3698\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 1s 589ms/step - loss: 0.3297 - val_loss: 0.3699\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 1s 584ms/step - loss: 0.3292 - val_loss: 0.3708\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.3280 - val_loss: 0.3723\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.3264 - val_loss: 0.3752\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 0.3245 - val_loss: 0.3813\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.3225 - val_loss: 0.3869\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 0.3203 - val_loss: 0.4025\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.3195 - val_loss: 0.4001\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.3668 - val_loss: 0.4462\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.3403 - val_loss: 0.4411\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.3408 - val_loss: 0.4167\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 0.3279 - val_loss: 0.3996\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 1s 828ms/step - loss: 0.3214 - val_loss: 0.3899\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 1s 703ms/step - loss: 0.3197 - val_loss: 0.3849\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.3200 - val_loss: 0.3830\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.3208 - val_loss: 0.3830\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.3212 - val_loss: 0.3844\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 0.3211 - val_loss: 0.3867\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.3203 - val_loss: 0.3897\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.3191 - val_loss: 0.3930\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.3175 - val_loss: 0.3966\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 0.3157 - val_loss: 0.4004\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 0.3138 - val_loss: 0.4049\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.3121 - val_loss: 0.4103\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 0.3104 - val_loss: 0.4167\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.3088 - val_loss: 0.4237\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 0.3072 - val_loss: 0.4306\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.3055 - val_loss: 0.4362\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 0.3036 - val_loss: 0.4402\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.3017 - val_loss: 0.4434\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 0.2997 - val_loss: 0.4482\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 0.2978 - val_loss: 0.4557\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.2957 - val_loss: 0.4649\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.2936 - val_loss: 0.4737\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 1s 783ms/step - loss: 0.2918 - val_loss: 0.4835\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.2903 - val_loss: 0.4917\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.2888 - val_loss: 0.4936\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.2872 - val_loss: 0.4977\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 0.2856 - val_loss: 0.4839\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.2843 - val_loss: 0.5041\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 0.2837 - val_loss: 0.4800\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.2817 - val_loss: 0.4866\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.2785 - val_loss: 0.4932\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.2777 - val_loss: 0.4818\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.2756 - val_loss: 0.4983\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.2727 - val_loss: 0.5218\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 1s 627ms/step - loss: 0.2715 - val_loss: 0.5138\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 1s 587ms/step - loss: 0.2706 - val_loss: 0.5606\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.2718 - val_loss: 0.5099\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 0.2744 - val_loss: 0.5899\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.3034 - val_loss: 0.5579\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 0.3114 - val_loss: 0.4846\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 0.2714 - val_loss: 0.4779\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 1s 782ms/step - loss: 0.2874 - val_loss: 0.4743\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 1s 927ms/step - loss: 0.2763 - val_loss: 0.4781\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 1s 606ms/step - loss: 0.2702 - val_loss: 0.4931\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.2750 - val_loss: 0.5032\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.2771 - val_loss: 0.4954\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.2741 - val_loss: 0.4755\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.2692 - val_loss: 0.4621\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.2675 - val_loss: 0.4620\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.2669 - val_loss: 0.4699\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.2656 - val_loss: 0.4754\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.2630 - val_loss: 0.4772\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.2602 - val_loss: 0.4839\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.2588 - val_loss: 0.4961\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.2552 - val_loss: 0.5070\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.2522 - val_loss: 0.5106\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.2491 - val_loss: 0.5139\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.2469 - val_loss: 0.5215\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.2445 - val_loss: 0.5338\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 0.2415 - val_loss: 0.5455\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 0.2393 - val_loss: 0.5518\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.2370 - val_loss: 0.5578\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 1s 793ms/step - loss: 0.2347 - val_loss: 0.5645\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.2318 - val_loss: 0.5660\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 1s 571ms/step - loss: 0.2294 - val_loss: 0.5619\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 0.2281 - val_loss: 0.5693\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 1s 599ms/step - loss: 0.2346 - val_loss: 0.5724\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 0.2640 - val_loss: 0.5736\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.2312 - val_loss: 0.5835\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.2391 - val_loss: 0.5744\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.2278 - val_loss: 0.5746\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.2281 - val_loss: 0.5827\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.2292 - val_loss: 0.5915\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 1s 552ms/step - loss: 0.2253 - val_loss: 0.5966\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 0.2242 - val_loss: 0.5907\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 1s 566ms/step - loss: 0.2213 - val_loss: 0.5828\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 0.2193 - val_loss: 0.5837\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.2168 - val_loss: 0.5900\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.2154 - val_loss: 0.5969\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.2140 - val_loss: 0.5981\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.2106 - val_loss: 0.5946\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 1s 586ms/step - loss: 0.2088 - val_loss: 0.5918\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 1s 788ms/step - loss: 0.2068 - val_loss: 0.5958\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 1s 960ms/step - loss: 0.2035 - val_loss: 0.6095\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 0.2014 - val_loss: 0.6251\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 0.1996 - val_loss: 0.6326\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 1s 574ms/step - loss: 0.1965 - val_loss: 0.6364\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.1944 - val_loss: 0.6378\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 0.1926 - val_loss: 0.6439\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 1s 569ms/step - loss: 0.1902 - val_loss: 0.6528\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.1887 - val_loss: 0.6592\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.1868 - val_loss: 0.6635\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 1s 572ms/step - loss: 0.1847 - val_loss: 0.6686\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 1s 566ms/step - loss: 0.1831 - val_loss: 0.6767\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 1s 594ms/step - loss: 0.1810 - val_loss: 0.6868\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 1s 574ms/step - loss: 0.1794 - val_loss: 0.6946\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.1780 - val_loss: 0.6974\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.1761 - val_loss: 0.6959\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 1s 567ms/step - loss: 0.1745 - val_loss: 0.6943\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.1731 - val_loss: 0.6966\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.1714 - val_loss: 0.7018\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 1s 591ms/step - loss: 0.1695 - val_loss: 0.7080\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.1679 - val_loss: 0.7120\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 1s 845ms/step - loss: 0.1663 - val_loss: 0.7137\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.1645 - val_loss: 0.7143\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.1628 - val_loss: 0.7138\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.1612 - val_loss: 0.7139\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.1594 - val_loss: 0.7152\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.1579 - val_loss: 0.7149\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 0.1562 - val_loss: 0.7131\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.1545 - val_loss: 0.7106\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.1528 - val_loss: 0.7105\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.1513 - val_loss: 0.7099\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 0.1498 - val_loss: 0.7091\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 0.1485 - val_loss: 0.7104\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.1480 - val_loss: 0.7144\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.1524 - val_loss: 0.7034\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.1491 - val_loss: 0.7018\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.1443 - val_loss: 0.7064\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 0.1459 - val_loss: 0.7059\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.1425 - val_loss: 0.7074\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.1422 - val_loss: 0.7101\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 0.1396 - val_loss: 0.7115\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 1s 848ms/step - loss: 0.1389 - val_loss: 0.7072\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 1s 765ms/step - loss: 0.1374 - val_loss: 0.6986\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.1355 - val_loss: 0.6972\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.1340 - val_loss: 0.7066\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.1328 - val_loss: 0.7180\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.1308 - val_loss: 0.7184\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 1s 627ms/step - loss: 0.1300 - val_loss: 0.7101\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.1282 - val_loss: 0.7100\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.1271 - val_loss: 0.7134\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.1253 - val_loss: 0.7136\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.1244 - val_loss: 0.7137\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.1227 - val_loss: 0.7152\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.1218 - val_loss: 0.7194\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.1202 - val_loss: 0.7185\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 0.1191 - val_loss: 0.7144\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.1178 - val_loss: 0.7122\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 0.1165 - val_loss: 0.7130\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.1153 - val_loss: 0.7141\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.1141 - val_loss: 0.7156\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 1s 632ms/step - loss: 0.1130 - val_loss: 0.7200\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 1s 844ms/step - loss: 0.1118 - val_loss: 0.7201\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 1s 792ms/step - loss: 0.1110 - val_loss: 0.7276\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 1s 590ms/step - loss: 0.1115 - val_loss: 0.7205\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.1173 - val_loss: 0.7387\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 1s 710ms/step - loss: 0.1298 - val_loss: 0.7060\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 1s 703ms/step - loss: 0.1182 - val_loss: 0.7037\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.1135 - val_loss: 0.7083\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.1128 - val_loss: 0.7115\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.1121 - val_loss: 0.7036\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.1073 - val_loss: 0.7004\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.1108 - val_loss: 0.7066\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 1s 608ms/step - loss: 0.1045 - val_loss: 0.7160\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 0.1063 - val_loss: 0.7135\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.1040 - val_loss: 0.7100\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.1027 - val_loss: 0.7155\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.1023 - val_loss: 0.7231\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.1000 - val_loss: 0.7258\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 0.1008 - val_loss: 0.7235\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0981 - val_loss: 0.7208\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 1s 769ms/step - loss: 0.0978 - val_loss: 0.7225\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0974 - val_loss: 0.7332\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 0.0953 - val_loss: 0.7374\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0954 - val_loss: 0.7325\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0940 - val_loss: 0.7309\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.0932 - val_loss: 0.7310\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 0.0927 - val_loss: 0.7365\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 1s 578ms/step - loss: 0.0914 - val_loss: 0.7417\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0914 - val_loss: 0.7346\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0905 - val_loss: 0.7372\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0906 - val_loss: 0.7371\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0903 - val_loss: 0.7477\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.0915 - val_loss: 0.7413\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 0.0888 - val_loss: 0.7421\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0870 - val_loss: 0.7438\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.0881 - val_loss: 0.7427\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.0862 - val_loss: 0.7527\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0856 - val_loss: 0.7512\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 1s 603ms/step - loss: 0.0848 - val_loss: 0.7455\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0839 - val_loss: 0.7491\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 1s 721ms/step - loss: 0.0837 - val_loss: 0.7497\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 1s 910ms/step - loss: 0.0830 - val_loss: 0.7524\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 1s 712ms/step - loss: 0.0818 - val_loss: 0.7509\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 0.0813 - val_loss: 0.7507\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0814 - val_loss: 0.7556\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 0.0803 - val_loss: 0.7543\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 1s 574ms/step - loss: 0.0795 - val_loss: 0.7539\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0790 - val_loss: 0.7545\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0788 - val_loss: 0.7582\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0779 - val_loss: 0.7630\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 0.0777 - val_loss: 0.7565\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.0766 - val_loss: 0.7582\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0761 - val_loss: 0.7628\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0762 - val_loss: 0.7598\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 1s 593ms/step - loss: 0.0754 - val_loss: 0.7598\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 1s 573ms/step - loss: 0.0743 - val_loss: 0.7594\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 1s 594ms/step - loss: 0.0739 - val_loss: 0.7615\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 1s 569ms/step - loss: 0.0736 - val_loss: 0.7631\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 1s 580ms/step - loss: 0.0732 - val_loss: 0.7615\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 0.0724 - val_loss: 0.7610\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 1s 945ms/step - loss: 0.0719 - val_loss: 0.7632\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0712 - val_loss: 0.7653\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 0.0708 - val_loss: 0.7625\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.0705 - val_loss: 0.7677\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 0.0700 - val_loss: 0.7638\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 1s 566ms/step - loss: 0.0692 - val_loss: 0.7665\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0686 - val_loss: 0.7684\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0681 - val_loss: 0.7675\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0676 - val_loss: 0.7692\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.0671 - val_loss: 0.7693\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.0670 - val_loss: 0.7670\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0674 - val_loss: 0.7720\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0707 - val_loss: 0.7624\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 1s 595ms/step - loss: 0.0730 - val_loss: 0.7644\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 1s 573ms/step - loss: 0.0750 - val_loss: 0.7569\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 0.0670 - val_loss: 0.7476\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 1s 604ms/step - loss: 0.0729 - val_loss: 0.7546\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 1s 564ms/step - loss: 0.0669 - val_loss: 0.7554\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.0685 - val_loss: 0.7500\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 1s 902ms/step - loss: 0.0664 - val_loss: 0.7501\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 0.0673 - val_loss: 0.7603\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 1s 617ms/step - loss: 0.0658 - val_loss: 0.7655\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 1s 596ms/step - loss: 0.0645 - val_loss: 0.7682\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 0.0649 - val_loss: 0.7660\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.0634 - val_loss: 0.7609\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 0.0639 - val_loss: 0.7605\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 0.0622 - val_loss: 0.7653\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 0.0624 - val_loss: 0.7683\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 0.0620 - val_loss: 0.7639\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.0611 - val_loss: 0.7594\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.0608 - val_loss: 0.7605\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 1s 558ms/step - loss: 0.0602 - val_loss: 0.7663\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 0.0601 - val_loss: 0.7712\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.0598 - val_loss: 0.7664\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.0592 - val_loss: 0.7579\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 0.0589 - val_loss: 0.7587\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.0583 - val_loss: 0.7657\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.0580 - val_loss: 0.7663\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 0.0578 - val_loss: 0.7615\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0575 - val_loss: 0.7590\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 1s 969ms/step - loss: 0.0572 - val_loss: 0.7635\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 1s 552ms/step - loss: 0.0568 - val_loss: 0.7693\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 1s 619ms/step - loss: 0.0565 - val_loss: 0.7674\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 1s 609ms/step - loss: 0.0560 - val_loss: 0.7621\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 0.0558 - val_loss: 0.7610\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0555 - val_loss: 0.7647\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 0.0552 - val_loss: 0.7655\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0550 - val_loss: 0.7617\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0549 - val_loss: 0.7597\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0549 - val_loss: 0.7651\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0552 - val_loss: 0.7632\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 1s 608ms/step - loss: 0.0565 - val_loss: 0.7669\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 1s 601ms/step - loss: 0.0564 - val_loss: 0.7627\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 1s 571ms/step - loss: 0.0567 - val_loss: 0.7648\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0538 - val_loss: 0.7665\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0535 - val_loss: 0.7650\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 0.0549 - val_loss: 0.7697\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 0.0539 - val_loss: 0.7676\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 0.0525 - val_loss: 0.7676\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 1s 951ms/step - loss: 0.0530 - val_loss: 0.7742\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 1s 709ms/step - loss: 0.0530 - val_loss: 0.7675\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0521 - val_loss: 0.7657\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 0.0518 - val_loss: 0.7735\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 1s 575ms/step - loss: 0.0522 - val_loss: 0.7700\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0516 - val_loss: 0.7702\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0512 - val_loss: 0.7720\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 0.0513 - val_loss: 0.7691\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.0511 - val_loss: 0.7745\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 0.0508 - val_loss: 0.7750\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.0506 - val_loss: 0.7722\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.0502 - val_loss: 0.7739\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0503 - val_loss: 0.7763\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0504 - val_loss: 0.7737\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0499 - val_loss: 0.7732\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.0504 - val_loss: 0.7767\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 1s 598ms/step - loss: 0.0512 - val_loss: 0.7770\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 0.0516 - val_loss: 0.7707\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 1s 790ms/step - loss: 0.0508 - val_loss: 0.7745\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0497 - val_loss: 0.7771\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 1s 615ms/step - loss: 0.0489 - val_loss: 0.7720\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0500 - val_loss: 0.7753\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0505 - val_loss: 0.7769\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 1s 617ms/step - loss: 0.0487 - val_loss: 0.7735\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 1s 583ms/step - loss: 0.0492 - val_loss: 0.7792\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.0495 - val_loss: 0.7721\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 0.0487 - val_loss: 0.7735\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.0486 - val_loss: 0.7858\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 0.0485 - val_loss: 0.7769\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 0.0484 - val_loss: 0.7767\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 1s 586ms/step - loss: 0.0479 - val_loss: 0.7796\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0475 - val_loss: 0.7780\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0477 - val_loss: 0.7839\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 1s 573ms/step - loss: 0.0471 - val_loss: 0.7777\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 1s 609ms/step - loss: 0.0470 - val_loss: 0.7792\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0468 - val_loss: 0.7838\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0467 - val_loss: 0.7796\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.0462 - val_loss: 0.7787\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 1s 849ms/step - loss: 0.0461 - val_loss: 0.7826\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 1s 928ms/step - loss: 0.0462 - val_loss: 0.7786\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 1s 649ms/step - loss: 0.0455 - val_loss: 0.7756\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.0459 - val_loss: 0.7839\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 1s 638ms/step - loss: 0.0457 - val_loss: 0.7778\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 0.0452 - val_loss: 0.7776\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 1s 607ms/step - loss: 0.0454 - val_loss: 0.7798\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.0450 - val_loss: 0.7816\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 0.0449 - val_loss: 0.7763\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 1s 590ms/step - loss: 0.0449 - val_loss: 0.7816\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 1s 644ms/step - loss: 0.0446 - val_loss: 0.7800\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 1s 609ms/step - loss: 0.0446 - val_loss: 0.7834\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 0.0445 - val_loss: 0.7770\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0447 - val_loss: 0.7886\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0456 - val_loss: 0.7730\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 1s 649ms/step - loss: 0.0473 - val_loss: 0.7840\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.0490 - val_loss: 0.7719\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 0.0469 - val_loss: 0.7713\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.0446 - val_loss: 0.7775\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 1s 987ms/step - loss: 0.0462 - val_loss: 0.7674\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 1s 973ms/step - loss: 0.0461 - val_loss: 0.7729\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 1s 578ms/step - loss: 0.0453 - val_loss: 0.7808\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 1s 563ms/step - loss: 0.0449 - val_loss: 0.7722\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 1s 557ms/step - loss: 0.0444 - val_loss: 0.7759\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 1s 571ms/step - loss: 0.0451 - val_loss: 0.7827\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 0.0435 - val_loss: 0.7808\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 1s 641ms/step - loss: 0.0439 - val_loss: 0.7773\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.0441 - val_loss: 0.7838\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 0.0429 - val_loss: 0.7842\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.0434 - val_loss: 0.7835\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0432 - val_loss: 0.7832\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.0427 - val_loss: 0.7872\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 0.0428 - val_loss: 0.7832\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.0427 - val_loss: 0.7860\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.0425 - val_loss: 0.7857\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0425 - val_loss: 0.7880\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 0.0421 - val_loss: 0.7821\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0424 - val_loss: 0.7887\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 1s 949ms/step - loss: 0.0421 - val_loss: 0.7831\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0419 - val_loss: 0.7872\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0421 - val_loss: 0.7823\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0421 - val_loss: 0.7902\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0421 - val_loss: 0.7799\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0421 - val_loss: 0.7889\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.0418 - val_loss: 0.7830\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0414 - val_loss: 0.7866\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 0.0409 - val_loss: 0.7872\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 1s 566ms/step - loss: 0.0409 - val_loss: 0.7840\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0410 - val_loss: 0.7919\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0410 - val_loss: 0.7841\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.0408 - val_loss: 0.7899\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 0.0406 - val_loss: 0.7866\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 1s 581ms/step - loss: 0.0404 - val_loss: 0.7868\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 1s 561ms/step - loss: 0.0402 - val_loss: 0.7910\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 0.0404 - val_loss: 0.7865\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0404 - val_loss: 0.7902\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0407 - val_loss: 0.7876\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 1s 590ms/step - loss: 0.0411 - val_loss: 0.7876\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0423 - val_loss: 0.7891\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0416 - val_loss: 0.7792\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0415 - val_loss: 0.7878\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0404 - val_loss: 0.7814\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 0.0410 - val_loss: 0.7865\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 0.0416 - val_loss: 0.7839\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.0401 - val_loss: 0.7782\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 1s 592ms/step - loss: 0.0400 - val_loss: 0.7868\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.0410 - val_loss: 0.7803\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.0401 - val_loss: 0.7836\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 1s 617ms/step - loss: 0.0393 - val_loss: 0.7867\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 1s 587ms/step - loss: 0.0400 - val_loss: 0.7779\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.0397 - val_loss: 0.7826\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.0391 - val_loss: 0.7836\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0393 - val_loss: 0.7831\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0393 - val_loss: 0.7818\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 0.0390 - val_loss: 0.7830\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 0.0386 - val_loss: 0.7839\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 0.0389 - val_loss: 0.7848\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 1s 783ms/step - loss: 0.0388 - val_loss: 0.7844\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 1s 922ms/step - loss: 0.0385 - val_loss: 0.7821\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 1s 614ms/step - loss: 0.0386 - val_loss: 0.7881\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.0386 - val_loss: 0.7780\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 0.0385 - val_loss: 0.7873\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0385 - val_loss: 0.7807\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 0.0389 - val_loss: 0.7848\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 0.0390 - val_loss: 0.7745\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.0394 - val_loss: 0.7879\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0399 - val_loss: 0.7733\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0401 - val_loss: 0.7785\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0399 - val_loss: 0.7770\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.0393 - val_loss: 0.7697\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0394 - val_loss: 0.7792\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0392 - val_loss: 0.7678\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.0390 - val_loss: 0.7753\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0381 - val_loss: 0.7788\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0379 - val_loss: 0.7685\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0384 - val_loss: 0.7760\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0382 - val_loss: 0.7745\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.0377 - val_loss: 0.7716\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0374 - val_loss: 0.7788\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0378 - val_loss: 0.7731\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 1s 609ms/step - loss: 0.0378 - val_loss: 0.7742\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0371 - val_loss: 0.7773\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 0.0371 - val_loss: 0.7739\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 1s 589ms/step - loss: 0.0374 - val_loss: 0.7771\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 1s 649ms/step - loss: 0.0371 - val_loss: 0.7765\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 1s 591ms/step - loss: 0.0367 - val_loss: 0.7775\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.0369 - val_loss: 0.7794\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 1s 562ms/step - loss: 0.0369 - val_loss: 0.7784\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0365 - val_loss: 0.7798\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.0365 - val_loss: 0.7800\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 1s 580ms/step - loss: 0.0366 - val_loss: 0.7785\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0364 - val_loss: 0.7804\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 1s 568ms/step - loss: 0.0361 - val_loss: 0.7802\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 1s 597ms/step - loss: 0.0362 - val_loss: 0.7820\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.0363 - val_loss: 0.7802\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.0361 - val_loss: 0.7805\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.0361 - val_loss: 0.7809\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 1s 797ms/step - loss: 0.0365 - val_loss: 0.7835\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0371 - val_loss: 0.7757\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 0.0381 - val_loss: 0.7878\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 1s 584ms/step - loss: 0.0400 - val_loss: 0.7747\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.0388 - val_loss: 0.7763\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.0379 - val_loss: 0.7759\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.0377 - val_loss: 0.7695\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 1s 590ms/step - loss: 0.0390 - val_loss: 0.7739\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 1s 564ms/step - loss: 0.0364 - val_loss: 0.7724\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.0369 - val_loss: 0.7680\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 0.0387 - val_loss: 0.7743\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 0.0363 - val_loss: 0.7751\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0370 - val_loss: 0.7657\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0370 - val_loss: 0.7691\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0360 - val_loss: 0.7752\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 0.0369 - val_loss: 0.7684\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 1s 649ms/step - loss: 0.0356 - val_loss: 0.7667\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.0360 - val_loss: 0.7716\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 1s 599ms/step - loss: 0.0361 - val_loss: 0.7698\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 1s 621ms/step - loss: 0.0355 - val_loss: 0.7696\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 1s 849ms/step - loss: 0.0359 - val_loss: 0.7701\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 1s 805ms/step - loss: 0.0353 - val_loss: 0.7711\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.0359 - val_loss: 0.7697\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.0354 - val_loss: 0.7727\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0356 - val_loss: 0.7719\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 0.0357 - val_loss: 0.7735\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0355 - val_loss: 0.7737\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0357 - val_loss: 0.7759\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0353 - val_loss: 0.7729\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.0354 - val_loss: 0.7750\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.0349 - val_loss: 0.7756\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.0347 - val_loss: 0.7765\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0345 - val_loss: 0.7753\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0344 - val_loss: 0.7752\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.0345 - val_loss: 0.7783\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 0.0344 - val_loss: 0.7779\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 0.0345 - val_loss: 0.7775\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 0.0345 - val_loss: 0.7794\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0346 - val_loss: 0.7785\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0345 - val_loss: 0.7787\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 1s 640ms/step - loss: 0.0343 - val_loss: 0.7789\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 1s 845ms/step - loss: 0.0342 - val_loss: 0.7810\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 1s 811ms/step - loss: 0.0341 - val_loss: 0.7781\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 1s 561ms/step - loss: 0.0340 - val_loss: 0.7820\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0341 - val_loss: 0.7788\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0344 - val_loss: 0.7834\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 1s 589ms/step - loss: 0.0353 - val_loss: 0.7721\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0371 - val_loss: 0.7848\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 0.0380 - val_loss: 0.7684\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0362 - val_loss: 0.7764\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.0341 - val_loss: 0.7812\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0356 - val_loss: 0.7699\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 0.0356 - val_loss: 0.7773\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0340 - val_loss: 0.7794\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.0349 - val_loss: 0.7717\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 1s 592ms/step - loss: 0.0349 - val_loss: 0.7770\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 0.0339 - val_loss: 0.7802\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0345 - val_loss: 0.7733\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0343 - val_loss: 0.7791\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0339 - val_loss: 0.7821\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 1s 610ms/step - loss: 0.0341 - val_loss: 0.7774\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 1s 959ms/step - loss: 0.0337 - val_loss: 0.7785\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 1s 776ms/step - loss: 0.0337 - val_loss: 0.7795\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.0337 - val_loss: 0.7796\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.0334 - val_loss: 0.7795\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.0337 - val_loss: 0.7806\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0337 - val_loss: 0.7778\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0336 - val_loss: 0.7815\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0340 - val_loss: 0.7776\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 0.0338 - val_loss: 0.7788\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 1s 647ms/step - loss: 0.0344 - val_loss: 0.7794\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 1s 587ms/step - loss: 0.0349 - val_loss: 0.7818\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 1s 552ms/step - loss: 0.0350 - val_loss: 0.7790\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 1s 562ms/step - loss: 0.0357 - val_loss: 0.7788\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 1s 964ms/step - loss: 0.0355 - val_loss: 0.7817\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0345 - val_loss: 0.7773\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.0342 - val_loss: 0.7827\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.0342 - val_loss: 0.7820\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 1s 623ms/step - loss: 0.0339 - val_loss: 0.7791\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 1s 904ms/step - loss: 0.0331 - val_loss: 0.7833\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0334 - val_loss: 0.7840\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0337 - val_loss: 0.7835\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.0332 - val_loss: 0.7847\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0328 - val_loss: 0.7848\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 0.0328 - val_loss: 0.7860\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0329 - val_loss: 0.7843\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 1s 563ms/step - loss: 0.0328 - val_loss: 0.7839\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.0327 - val_loss: 0.7867\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0324 - val_loss: 0.7854\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 0.0324 - val_loss: 0.7869\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0324 - val_loss: 0.7869\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.0323 - val_loss: 0.7866\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 1s 605ms/step - loss: 0.0322 - val_loss: 0.7893\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 0.0322 - val_loss: 0.7872\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 1s 608ms/step - loss: 0.0321 - val_loss: 0.7898\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 1s 605ms/step - loss: 0.0320 - val_loss: 0.7894\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.0320 - val_loss: 0.7894\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0320 - val_loss: 0.7907\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0319 - val_loss: 0.7896\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0317 - val_loss: 0.7914\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 1s 930ms/step - loss: 0.0317 - val_loss: 0.7908\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 1s 583ms/step - loss: 0.0317 - val_loss: 0.7914\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 0.0317 - val_loss: 0.7909\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 1s 601ms/step - loss: 0.0316 - val_loss: 0.7925\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 0.0315 - val_loss: 0.7917\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0315 - val_loss: 0.7937\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.0317 - val_loss: 0.7910\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 0.0317 - val_loss: 0.7962\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0319 - val_loss: 0.7888\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 1s 585ms/step - loss: 0.0324 - val_loss: 0.8028\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 1s 563ms/step - loss: 0.0340 - val_loss: 0.7848\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.0344 - val_loss: 0.7986\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.0356 - val_loss: 0.7821\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0333 - val_loss: 0.7789\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 1s 566ms/step - loss: 0.0323 - val_loss: 0.7921\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.0333 - val_loss: 0.7755\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0331 - val_loss: 0.7821\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.0321 - val_loss: 0.7881\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 1s 610ms/step - loss: 0.0321 - val_loss: 0.7752\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 1s 937ms/step - loss: 0.0324 - val_loss: 0.7841\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 1s 838ms/step - loss: 0.0316 - val_loss: 0.7858\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.0315 - val_loss: 0.7797\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 1s 581ms/step - loss: 0.0320 - val_loss: 0.7858\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 1s 568ms/step - loss: 0.0315 - val_loss: 0.7847\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 1s 624ms/step - loss: 0.0310 - val_loss: 0.7811\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 0.0315 - val_loss: 0.7881\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0314 - val_loss: 0.7838\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0310 - val_loss: 0.7835\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 0.0312 - val_loss: 0.7900\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 1s 613ms/step - loss: 0.0312 - val_loss: 0.7841\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.0309 - val_loss: 0.7860\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 1s 637ms/step - loss: 0.0310 - val_loss: 0.7891\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 0.0310 - val_loss: 0.7856\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0308 - val_loss: 0.7883\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0309 - val_loss: 0.7884\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 0.0312 - val_loss: 0.7877\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0313 - val_loss: 0.7874\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 1s 641ms/step - loss: 0.0317 - val_loss: 0.7922\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 1s 816ms/step - loss: 0.0323 - val_loss: 0.7812\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 1s 920ms/step - loss: 0.0329 - val_loss: 0.7918\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 1s 723ms/step - loss: 0.0329 - val_loss: 0.7852\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 1s 585ms/step - loss: 0.0325 - val_loss: 0.7825\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0330 - val_loss: 0.7862\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0327 - val_loss: 0.7848\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0318 - val_loss: 0.7792\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0315 - val_loss: 0.7909\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0323 - val_loss: 0.7853\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0327 - val_loss: 0.7834\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 0.0314 - val_loss: 0.7898\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0314 - val_loss: 0.7854\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.0313 - val_loss: 0.7847\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0314 - val_loss: 0.7942\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 0.0313 - val_loss: 0.7856\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0309 - val_loss: 0.7802\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0311 - val_loss: 0.7928\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 0.0312 - val_loss: 0.7847\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 0.0308 - val_loss: 0.7836\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 1s 564ms/step - loss: 0.0306 - val_loss: 0.7884\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 1s 741ms/step - loss: 0.0308 - val_loss: 0.7901\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 1s 932ms/step - loss: 0.0305 - val_loss: 0.7871\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 0.0302 - val_loss: 0.7909\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0303 - val_loss: 0.7904\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.0304 - val_loss: 0.7881\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 1s 578ms/step - loss: 0.0301 - val_loss: 0.7931\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 0.0300 - val_loss: 0.7895\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 0.0302 - val_loss: 0.7912\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 1s 599ms/step - loss: 0.0299 - val_loss: 0.7909\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 1s 584ms/step - loss: 0.0299 - val_loss: 0.7907\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 1s 602ms/step - loss: 0.0300 - val_loss: 0.7922\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 1s 586ms/step - loss: 0.0300 - val_loss: 0.7924\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 1s 580ms/step - loss: 0.0299 - val_loss: 0.7919\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0301 - val_loss: 0.7949\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 1s 623ms/step - loss: 0.0303 - val_loss: 0.7900\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 0.0307 - val_loss: 0.7978\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0310 - val_loss: 0.7896\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0316 - val_loss: 0.7965\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 0.0313 - val_loss: 0.7919\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0309 - val_loss: 0.7969\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 1s 924ms/step - loss: 0.0299 - val_loss: 0.7927\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 1s 843ms/step - loss: 0.0298 - val_loss: 0.7970\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.0305 - val_loss: 0.7966\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0307 - val_loss: 0.7941\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0311 - val_loss: 0.7973\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 1s 607ms/step - loss: 0.0305 - val_loss: 0.7968\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.0306 - val_loss: 0.7931\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0307 - val_loss: 0.7979\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.0306 - val_loss: 0.7971\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 1s 552ms/step - loss: 0.0303 - val_loss: 0.7915\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0301 - val_loss: 0.8037\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 0.0299 - val_loss: 0.7939\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0301 - val_loss: 0.8004\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.0299 - val_loss: 0.7999\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 0.0295 - val_loss: 0.7969\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 1s 634ms/step - loss: 0.0297 - val_loss: 0.8068\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 1s 585ms/step - loss: 0.0299 - val_loss: 0.7970\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.0295 - val_loss: 0.8041\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0291 - val_loss: 0.8041\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.0292 - val_loss: 0.7980\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 1s 814ms/step - loss: 0.0294 - val_loss: 0.8073\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0293 - val_loss: 0.7998\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 1s 563ms/step - loss: 0.0292 - val_loss: 0.8011\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 0.0292 - val_loss: 0.8040\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 0.0292 - val_loss: 0.8006\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 1s 594ms/step - loss: 0.0293 - val_loss: 0.8039\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 1s 585ms/step - loss: 0.0294 - val_loss: 0.8007\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 0.0297 - val_loss: 0.8033\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 0.0298 - val_loss: 0.8011\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0300 - val_loss: 0.8020\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.0299 - val_loss: 0.8014\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 1s 602ms/step - loss: 0.0298 - val_loss: 0.8014\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 1s 581ms/step - loss: 0.0297 - val_loss: 0.8008\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.0296 - val_loss: 0.8004\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0295 - val_loss: 0.8000\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0292 - val_loss: 0.8035\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.0292 - val_loss: 0.7976\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0297 - val_loss: 0.8062\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.0297 - val_loss: 0.7996\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 1s 584ms/step - loss: 0.0292 - val_loss: 0.8018\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 1s 832ms/step - loss: 0.0290 - val_loss: 0.8014\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0293 - val_loss: 0.8006\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 1s 568ms/step - loss: 0.0297 - val_loss: 0.8018\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 1s 584ms/step - loss: 0.0295 - val_loss: 0.8005\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0293 - val_loss: 0.8028\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0295 - val_loss: 0.8002\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0298 - val_loss: 0.8033\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0295 - val_loss: 0.8037\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 0.0289 - val_loss: 0.8021\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.0287 - val_loss: 0.8039\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0290 - val_loss: 0.8056\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 1s 552ms/step - loss: 0.0291 - val_loss: 0.8017\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 1s 605ms/step - loss: 0.0290 - val_loss: 0.8103\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.0291 - val_loss: 0.7986\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0293 - val_loss: 0.8108\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 0.0295 - val_loss: 0.7989\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0293 - val_loss: 0.8063\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.0291 - val_loss: 0.8018\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.0291 - val_loss: 0.8067\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 1s 617ms/step - loss: 0.0287 - val_loss: 0.8039\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 1s 934ms/step - loss: 0.0284 - val_loss: 0.8035\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 1s 840ms/step - loss: 0.0284 - val_loss: 0.8058\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 1s 601ms/step - loss: 0.0286 - val_loss: 0.8021\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 1s 593ms/step - loss: 0.0289 - val_loss: 0.8088\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 1s 603ms/step - loss: 0.0287 - val_loss: 0.8044\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 1s 627ms/step - loss: 0.0286 - val_loss: 0.8060\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0288 - val_loss: 0.8072\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0291 - val_loss: 0.8027\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0299 - val_loss: 0.8092\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 0.0297 - val_loss: 0.8011\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 0.0299 - val_loss: 0.8076\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 1s 574ms/step - loss: 0.0286 - val_loss: 0.8044\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 0.0282 - val_loss: 0.8058\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 1s 623ms/step - loss: 0.0288 - val_loss: 0.8124\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.0289 - val_loss: 0.8050\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 1s 557ms/step - loss: 0.0287 - val_loss: 0.8137\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0283 - val_loss: 0.8110\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 0.0285 - val_loss: 0.8096\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 0.0291 - val_loss: 0.8146\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0291 - val_loss: 0.8094\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 1s 778ms/step - loss: 0.0291 - val_loss: 0.8075\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0291 - val_loss: 0.8159\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.0295 - val_loss: 0.8029\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 1s 557ms/step - loss: 0.0299 - val_loss: 0.8150\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 0.0291 - val_loss: 0.8120\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0286 - val_loss: 0.8034\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 0.0291 - val_loss: 0.8152\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.0294 - val_loss: 0.8062\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0288 - val_loss: 0.8075\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0280 - val_loss: 0.8137\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 0.0280 - val_loss: 0.8048\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 1s 558ms/step - loss: 0.0283 - val_loss: 0.8099\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0283 - val_loss: 0.8134\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.0282 - val_loss: 0.8081\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0280 - val_loss: 0.8135\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0278 - val_loss: 0.8117\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 0.0281 - val_loss: 0.8131\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.0280 - val_loss: 0.8118\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 1s 562ms/step - loss: 0.0277 - val_loss: 0.8166\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 1s 793ms/step - loss: 0.0277 - val_loss: 0.8086\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 1s 973ms/step - loss: 0.0278 - val_loss: 0.8163\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 0.0279 - val_loss: 0.8116\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0279 - val_loss: 0.8156\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 0.0284 - val_loss: 0.8099\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 0.0292 - val_loss: 0.8212\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 1s 557ms/step - loss: 0.0299 - val_loss: 0.8051\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 1s 608ms/step - loss: 0.0301 - val_loss: 0.8247\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0297 - val_loss: 0.8065\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 1s 606ms/step - loss: 0.0280 - val_loss: 0.8105\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 1s 583ms/step - loss: 0.0277 - val_loss: 0.8196\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0285 - val_loss: 0.8082\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 1s 614ms/step - loss: 0.0285 - val_loss: 0.8179\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.0278 - val_loss: 0.8155\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0275 - val_loss: 0.8084\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 0.0280 - val_loss: 0.8208\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 0.0279 - val_loss: 0.8137\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 1s 586ms/step - loss: 0.0274 - val_loss: 0.8126\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 1s 571ms/step - loss: 0.0276 - val_loss: 0.8198\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 0.0278 - val_loss: 0.8170\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0276 - val_loss: 0.8122\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 1s 792ms/step - loss: 0.0274 - val_loss: 0.8199\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0277 - val_loss: 0.8154\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 1s 590ms/step - loss: 0.0282 - val_loss: 0.8167\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 1s 652ms/step - loss: 0.0286 - val_loss: 0.8243\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 1s 573ms/step - loss: 0.0290 - val_loss: 0.8131\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 1s 573ms/step - loss: 0.0286 - val_loss: 0.8170\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 1s 575ms/step - loss: 0.0280 - val_loss: 0.8216\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 0.0280 - val_loss: 0.8130\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 0.0282 - val_loss: 0.8207\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 1s 590ms/step - loss: 0.0289 - val_loss: 0.8228\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.0287 - val_loss: 0.8121\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 1s 643ms/step - loss: 0.0284 - val_loss: 0.8203\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 1s 593ms/step - loss: 0.0283 - val_loss: 0.8161\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0279 - val_loss: 0.8154\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.0275 - val_loss: 0.8223\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.0273 - val_loss: 0.8180\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 0.0275 - val_loss: 0.8217\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 1s 603ms/step - loss: 0.0276 - val_loss: 0.8186\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0272 - val_loss: 0.8214\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 1s 821ms/step - loss: 0.0270 - val_loss: 0.8209\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0271 - val_loss: 0.8200\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0272 - val_loss: 0.8216\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0272 - val_loss: 0.8217\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 1s 552ms/step - loss: 0.0269 - val_loss: 0.8190\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.0268 - val_loss: 0.8253\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0270 - val_loss: 0.8188\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0270 - val_loss: 0.8291\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 0.0268 - val_loss: 0.8207\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 0.0268 - val_loss: 0.8259\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 1s 601ms/step - loss: 0.0269 - val_loss: 0.8226\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.0274 - val_loss: 0.8301\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 1s 571ms/step - loss: 0.0277 - val_loss: 0.8170\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.0284 - val_loss: 0.8334\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0281 - val_loss: 0.8181\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0277 - val_loss: 0.8238\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.0272 - val_loss: 0.8257\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 1s 587ms/step - loss: 0.0272 - val_loss: 0.8218\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 1s 638ms/step - loss: 0.0275 - val_loss: 0.8271\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 1s 962ms/step - loss: 0.0276 - val_loss: 0.8229\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 1s 777ms/step - loss: 0.0270 - val_loss: 0.8261\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0269 - val_loss: 0.8231\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.0269 - val_loss: 0.8250\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.0267 - val_loss: 0.8277\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.0265 - val_loss: 0.8204\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 1s 594ms/step - loss: 0.0267 - val_loss: 0.8308\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 0.0269 - val_loss: 0.8247\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 0.0271 - val_loss: 0.8285\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 1s 904ms/step - loss: 0.0276 - val_loss: 0.8256\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0280 - val_loss: 0.8276\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 1s 584ms/step - loss: 0.0289 - val_loss: 0.8209\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 0.0295 - val_loss: 0.8242\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 0.0284 - val_loss: 0.8227\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 1s 584ms/step - loss: 0.0270 - val_loss: 0.8211\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 0.0267 - val_loss: 0.8255\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 1s 594ms/step - loss: 0.0276 - val_loss: 0.8215\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 1s 780ms/step - loss: 0.0276 - val_loss: 0.8243\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 0.0266 - val_loss: 0.8272\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 1s 728ms/step - loss: 0.0268 - val_loss: 0.8220\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 1s 567ms/step - loss: 0.0272 - val_loss: 0.8252\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 1s 571ms/step - loss: 0.0268 - val_loss: 0.8262\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 0.0267 - val_loss: 0.8219\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 0.0273 - val_loss: 0.8286\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 0.0274 - val_loss: 0.8210\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0273 - val_loss: 0.8229\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 1s 608ms/step - loss: 0.0277 - val_loss: 0.8267\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.0275 - val_loss: 0.8198\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 0.0265 - val_loss: 0.8234\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 0.0264 - val_loss: 0.8237\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 1s 641ms/step - loss: 0.0269 - val_loss: 0.8277\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 1s 571ms/step - loss: 0.0267 - val_loss: 0.8242\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 1s 585ms/step - loss: 0.0267 - val_loss: 0.8282\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 0.0266 - val_loss: 0.8262\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0264 - val_loss: 0.8277\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 1s 567ms/step - loss: 0.0266 - val_loss: 0.8275\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 1s 585ms/step - loss: 0.0269 - val_loss: 0.8286\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 1s 910ms/step - loss: 0.0267 - val_loss: 0.8243\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 1s 954ms/step - loss: 0.0269 - val_loss: 0.8317\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 0.0269 - val_loss: 0.8206\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0271 - val_loss: 0.8313\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 1s 602ms/step - loss: 0.0269 - val_loss: 0.8277\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 0.0264 - val_loss: 0.8274\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 0.0261 - val_loss: 0.8263\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.0261 - val_loss: 0.8324\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0261 - val_loss: 0.8290\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0260 - val_loss: 0.8267\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0261 - val_loss: 0.8338\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 1s 613ms/step - loss: 0.0262 - val_loss: 0.8257\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 1s 593ms/step - loss: 0.0260 - val_loss: 0.8331\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0259 - val_loss: 0.8296\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 1s 597ms/step - loss: 0.0258 - val_loss: 0.8326\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 1s 589ms/step - loss: 0.0258 - val_loss: 0.8305\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 0.0258 - val_loss: 0.8373\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 1s 623ms/step - loss: 0.0259 - val_loss: 0.8306\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 1s 566ms/step - loss: 0.0260 - val_loss: 0.8354\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0258 - val_loss: 0.8361\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 1s 932ms/step - loss: 0.0257 - val_loss: 0.8346\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 1s 572ms/step - loss: 0.0256 - val_loss: 0.8333\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 1s 627ms/step - loss: 0.0256 - val_loss: 0.8391\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 1s 649ms/step - loss: 0.0258 - val_loss: 0.8325\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 1s 600ms/step - loss: 0.0260 - val_loss: 0.8360\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 1s 636ms/step - loss: 0.0263 - val_loss: 0.8340\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 1s 621ms/step - loss: 0.0270 - val_loss: 0.8329\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 1s 598ms/step - loss: 0.0276 - val_loss: 0.8356\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.0291 - val_loss: 0.8276\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 1s 562ms/step - loss: 0.0286 - val_loss: 0.8383\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 0.0281 - val_loss: 0.8243\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 1s 552ms/step - loss: 0.0277 - val_loss: 0.8350\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0269 - val_loss: 0.8272\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 1s 615ms/step - loss: 0.0269 - val_loss: 0.8293\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.0271 - val_loss: 0.8348\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 1s 602ms/step - loss: 0.0268 - val_loss: 0.8258\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 0.0266 - val_loss: 0.8292\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 1s 725ms/step - loss: 0.0263 - val_loss: 0.8341\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 1s 906ms/step - loss: 0.0263 - val_loss: 0.8308\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 1s 765ms/step - loss: 0.0264 - val_loss: 0.8301\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0261 - val_loss: 0.8317\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0260 - val_loss: 0.8286\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 1s 592ms/step - loss: 0.0258 - val_loss: 0.8335\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 1s 563ms/step - loss: 0.0258 - val_loss: 0.8300\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 1s 615ms/step - loss: 0.0259 - val_loss: 0.8356\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.0257 - val_loss: 0.8317\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 1s 583ms/step - loss: 0.0256 - val_loss: 0.8367\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 1s 609ms/step - loss: 0.0257 - val_loss: 0.8328\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0257 - val_loss: 0.8372\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 1s 615ms/step - loss: 0.0257 - val_loss: 0.8343\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.0262 - val_loss: 0.8429\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 0.0262 - val_loss: 0.8291\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 1s 602ms/step - loss: 0.0264 - val_loss: 0.8425\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 0.0268 - val_loss: 0.8271\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.0265 - val_loss: 0.8400\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0259 - val_loss: 0.8316\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 1s 749ms/step - loss: 0.0257 - val_loss: 0.8329\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 1s 906ms/step - loss: 0.0258 - val_loss: 0.8362\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0260 - val_loss: 0.8325\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.0265 - val_loss: 0.8366\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 0.0263 - val_loss: 0.8417\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0262 - val_loss: 0.8254\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 0.0266 - val_loss: 0.8443\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 1s 581ms/step - loss: 0.0267 - val_loss: 0.8351\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 0.0262 - val_loss: 0.8298\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.0256 - val_loss: 0.8424\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 1s 612ms/step - loss: 0.0254 - val_loss: 0.8323\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 1s 601ms/step - loss: 0.0254 - val_loss: 0.8366\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 1s 583ms/step - loss: 0.0255 - val_loss: 0.8418\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 0.0255 - val_loss: 0.8351\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 1s 573ms/step - loss: 0.0255 - val_loss: 0.8414\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 0.0253 - val_loss: 0.8390\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 0.0251 - val_loss: 0.8392\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 1s 568ms/step - loss: 0.0251 - val_loss: 0.8391\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 0.0251 - val_loss: 0.8402\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 1s 987ms/step - loss: 0.0250 - val_loss: 0.8407\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 1s 810ms/step - loss: 0.0250 - val_loss: 0.8373\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 1s 554ms/step - loss: 0.0250 - val_loss: 0.8433\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0250 - val_loss: 0.8427\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0248 - val_loss: 0.8388\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0248 - val_loss: 0.8453\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.0248 - val_loss: 0.8399\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.0248 - val_loss: 0.8443\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.0248 - val_loss: 0.8421\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0249 - val_loss: 0.8443\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 1s 585ms/step - loss: 0.0252 - val_loss: 0.8402\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 0.0257 - val_loss: 0.8484\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.0265 - val_loss: 0.8372\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0277 - val_loss: 0.8574\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.0288 - val_loss: 0.8303\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 1s 573ms/step - loss: 0.0283 - val_loss: 0.8504\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 1s 594ms/step - loss: 0.0266 - val_loss: 0.8339\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 0.0255 - val_loss: 0.8328\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 1s 585ms/step - loss: 0.0266 - val_loss: 0.8458\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 1s 758ms/step - loss: 0.0273 - val_loss: 0.8351\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 1s 900ms/step - loss: 0.0257 - val_loss: 0.8384\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 1s 724ms/step - loss: 0.0257 - val_loss: 0.8426\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 1s 610ms/step - loss: 0.0264 - val_loss: 0.8351\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 1s 585ms/step - loss: 0.0256 - val_loss: 0.8460\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.0254 - val_loss: 0.8398\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.0259 - val_loss: 0.8400\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0252 - val_loss: 0.8434\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 1s 571ms/step - loss: 0.0253 - val_loss: 0.8394\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 1s 628ms/step - loss: 0.0255 - val_loss: 0.8400\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 1s 593ms/step - loss: 0.0249 - val_loss: 0.8454\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 1s 557ms/step - loss: 0.0248 - val_loss: 0.8464\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.0250 - val_loss: 0.8441\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0246 - val_loss: 0.8447\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.0247 - val_loss: 0.8486\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.0249 - val_loss: 0.8449\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 0.0246 - val_loss: 0.8458\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.0247 - val_loss: 0.8524\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 1s 625ms/step - loss: 0.0250 - val_loss: 0.8421\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 0.0249 - val_loss: 0.8523\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0253 - val_loss: 0.8456\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 0.0261 - val_loss: 0.8483\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 0.0255 - val_loss: 0.8460\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 1s 560ms/step - loss: 0.0252 - val_loss: 0.8392\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 0.0250 - val_loss: 0.8425\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 1s 554ms/step - loss: 0.0247 - val_loss: 0.8437\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0250 - val_loss: 0.8449\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 1s 568ms/step - loss: 0.0250 - val_loss: 0.8419\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0247 - val_loss: 0.8448\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 1s 554ms/step - loss: 0.0250 - val_loss: 0.8430\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 0.0255 - val_loss: 0.8419\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 0.0260 - val_loss: 0.8431\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0269 - val_loss: 0.8396\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.0275 - val_loss: 0.8430\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 1s 609ms/step - loss: 0.0268 - val_loss: 0.8372\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 1s 572ms/step - loss: 0.0256 - val_loss: 0.8389\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 0.0253 - val_loss: 0.8416\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.0258 - val_loss: 0.8365\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 1s 586ms/step - loss: 0.0256 - val_loss: 0.8429\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 0.0248 - val_loss: 0.8434\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 1s 907ms/step - loss: 0.0251 - val_loss: 0.8365\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 1s 827ms/step - loss: 0.0254 - val_loss: 0.8469\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 1s 613ms/step - loss: 0.0246 - val_loss: 0.8409\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0244 - val_loss: 0.8368\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 0.0250 - val_loss: 0.8477\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 0.0248 - val_loss: 0.8398\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0243 - val_loss: 0.8420\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 0.0243 - val_loss: 0.8451\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 0.0247 - val_loss: 0.8411\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 0.0244 - val_loss: 0.8461\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 1s 570ms/step - loss: 0.0241 - val_loss: 0.8462\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 1s 606ms/step - loss: 0.0244 - val_loss: 0.8432\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 0.0243 - val_loss: 0.8476\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.0239 - val_loss: 0.8481\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 0.0240 - val_loss: 0.8464\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.0242 - val_loss: 0.8455\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 0.0241 - val_loss: 0.8508\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 1s 561ms/step - loss: 0.0239 - val_loss: 0.8464\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.0242 - val_loss: 0.8506\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 1s 714ms/step - loss: 0.0246 - val_loss: 0.8482\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0249 - val_loss: 0.8508\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 0.0257 - val_loss: 0.8457\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 0.0267 - val_loss: 0.8490\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 1s 608ms/step - loss: 0.0272 - val_loss: 0.8412\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 1s 612ms/step - loss: 0.0260 - val_loss: 0.8440\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 1s 619ms/step - loss: 0.0243 - val_loss: 0.8435\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 1s 583ms/step - loss: 0.0249 - val_loss: 0.8389\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.0259 - val_loss: 0.8456\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 0.0253 - val_loss: 0.8409\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.0247 - val_loss: 0.8478\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 0.0251 - val_loss: 0.8409\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 1s 566ms/step - loss: 0.0250 - val_loss: 0.8408\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 0.0243 - val_loss: 0.8501\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 0.0244 - val_loss: 0.8371\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.0246 - val_loss: 0.8451\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 0.0240 - val_loss: 0.8454\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 1s 567ms/step - loss: 0.0241 - val_loss: 0.8420\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 0.0245 - val_loss: 0.8445\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 1s 554ms/step - loss: 0.0240 - val_loss: 0.8473\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 1s 780ms/step - loss: 0.0239 - val_loss: 0.8405\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0242 - val_loss: 0.8496\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 1s 640ms/step - loss: 0.0239 - val_loss: 0.8441\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 1s 552ms/step - loss: 0.0236 - val_loss: 0.8443\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.0239 - val_loss: 0.8491\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.0237 - val_loss: 0.8466\n"
          ]
        }
      ],
      "source": [
        "h_gru_5 = model_GRU_5.fit(X_TRAIN,Y_TRAIN,\n",
        "          epochs=1000,\n",
        "          batch_size=X_TRAIN.shape[0],\n",
        "          validation_data = (X_VALID, Y_VALID),\n",
        "          verbose = True\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vXM9b8ticzVq",
        "outputId": "132bd63c-7609-41f3-8fe9-32dc61c39af5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b123499df00>]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h_gru_5.history[\"loss\"])\n",
        "plt.plot(h_gru_5.history[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNbEhlGOhIQf"
      },
      "source": [
        "# 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NOCh4a7FhJQF",
        "outputId": "44ec2aac-60d7-44d8-b78d-0f0b481b5f46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 149, 31)]         0         \n",
            "                                                                 \n",
            " gru_5 (GRU)                 (None, 25)                4350      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 25)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 25)                650       \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 25)                0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 25)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 31)                806       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,806\n",
            "Trainable params: 5,806\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(X_TRAIN.shape[1:]))\n",
        "m = inputs\n",
        "# , kernel_regularizer=regularizers.l1_l2(l1=0.1, l2=0.1)\n",
        "mA = GRU(units=25)(m)\n",
        "mA = Dropout(0.5)(mA)\n",
        "mA = Dense(25)(mA)\n",
        "mA = Dropout(0.5)(mA)\n",
        "m = Flatten()(mA)\n",
        "\n",
        "# A침adir capa Dense de salida\n",
        "out = Dense(Y_TRAIN.shape[1], activation='tanh')(m)\n",
        "\n",
        "model_GRU_6 = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_GRU_6.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse',\n",
        "        metrics=[])\n",
        "\n",
        "model_GRU_6.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u6LlwCAChNFF",
        "outputId": "15b79e4b-dbca-4a52-9517-b63c361e8eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3559 - val_loss: 0.3525\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.3507 - val_loss: 0.3510\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.3468 - val_loss: 0.3522\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.3434 - val_loss: 0.3544\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.3431 - val_loss: 0.3559\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.3416 - val_loss: 0.3571\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.3399 - val_loss: 0.3578\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.3393 - val_loss: 0.3585\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.3389 - val_loss: 0.3592\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.3390 - val_loss: 0.3599\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.3377 - val_loss: 0.3607\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.3381 - val_loss: 0.3613\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.3375 - val_loss: 0.3621\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 0.3374 - val_loss: 0.3628\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 0.3366 - val_loss: 0.3633\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.3367 - val_loss: 0.3652\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.3364 - val_loss: 0.3682\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.3364 - val_loss: 0.3674\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.3355 - val_loss: 0.3651\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.3352 - val_loss: 0.3683\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.3346 - val_loss: 0.3793\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.3353 - val_loss: 0.3732\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.3325 - val_loss: 0.3702\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.3329 - val_loss: 0.3829\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.3309 - val_loss: 0.3891\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.3312 - val_loss: 0.3798\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.3299 - val_loss: 0.3972\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.3279 - val_loss: 0.3983\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.3271 - val_loss: 0.3929\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.3275 - val_loss: 0.4181\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.3276 - val_loss: 0.4000\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.3261 - val_loss: 0.3937\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.3254 - val_loss: 0.4052\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.3229 - val_loss: 0.4182\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.3247 - val_loss: 0.3999\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.3220 - val_loss: 0.3994\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.3227 - val_loss: 0.4293\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.3229 - val_loss: 0.4142\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.3175 - val_loss: 0.4057\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.3169 - val_loss: 0.4175\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.3179 - val_loss: 0.4255\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.3149 - val_loss: 0.4185\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.3153 - val_loss: 0.4137\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.3135 - val_loss: 0.4391\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.3154 - val_loss: 0.4084\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.3125 - val_loss: 0.4333\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.3119 - val_loss: 0.4200\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.3119 - val_loss: 0.4218\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.3108 - val_loss: 0.4357\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.3085 - val_loss: 0.4469\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.3092 - val_loss: 0.4455\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.3076 - val_loss: 0.4327\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 0.3071 - val_loss: 0.4538\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 0.3071 - val_loss: 0.4079\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.3074 - val_loss: 0.4044\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.3035 - val_loss: 0.4095\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.3046 - val_loss: 0.4064\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.3015 - val_loss: 0.3997\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.3044 - val_loss: 0.4366\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.3075 - val_loss: 0.3946\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.3155 - val_loss: 0.4207\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.3046 - val_loss: 0.4230\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.3010 - val_loss: 0.4221\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.3040 - val_loss: 0.4522\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.3059 - val_loss: 0.4197\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.3041 - val_loss: 0.4104\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.3021 - val_loss: 0.4137\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.3009 - val_loss: 0.4146\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.3003 - val_loss: 0.4109\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.2990 - val_loss: 0.4101\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.3016 - val_loss: 0.4101\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.2979 - val_loss: 0.4136\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2995 - val_loss: 0.4259\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2986 - val_loss: 0.4148\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.2984 - val_loss: 0.4171\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2973 - val_loss: 0.4277\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2973 - val_loss: 0.4084\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2968 - val_loss: 0.4274\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2950 - val_loss: 0.4312\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2947 - val_loss: 0.4107\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2935 - val_loss: 0.4185\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.2943 - val_loss: 0.4245\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2936 - val_loss: 0.4336\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.2930 - val_loss: 0.4351\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.2911 - val_loss: 0.4130\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2953 - val_loss: 0.4558\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.2929 - val_loss: 0.4405\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 0.2886 - val_loss: 0.4310\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.2892 - val_loss: 0.4470\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.2867 - val_loss: 0.4430\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.2880 - val_loss: 0.4377\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 0.2858 - val_loss: 0.4709\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2862 - val_loss: 0.4353\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2872 - val_loss: 0.4557\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2844 - val_loss: 0.4480\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2827 - val_loss: 0.4456\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2826 - val_loss: 0.4742\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.2822 - val_loss: 0.4529\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.2845 - val_loss: 0.5399\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2957 - val_loss: 0.4555\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2885 - val_loss: 0.4500\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2882 - val_loss: 0.4556\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2840 - val_loss: 0.4601\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.2886 - val_loss: 0.4513\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2867 - val_loss: 0.4387\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2825 - val_loss: 0.4368\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2858 - val_loss: 0.4431\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2791 - val_loss: 0.4600\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2802 - val_loss: 0.4616\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.2781 - val_loss: 0.4494\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.2791 - val_loss: 0.4449\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2781 - val_loss: 0.4579\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.2774 - val_loss: 0.4713\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.2758 - val_loss: 0.4725\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.2733 - val_loss: 0.4784\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2764 - val_loss: 0.4916\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.2714 - val_loss: 0.4993\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.2709 - val_loss: 0.4953\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.2708 - val_loss: 0.5199\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2687 - val_loss: 0.5362\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2697 - val_loss: 0.5279\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.2697 - val_loss: 0.5547\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.2717 - val_loss: 0.5165\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.2728 - val_loss: 0.5868\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.2809 - val_loss: 0.5065\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.2726 - val_loss: 0.4976\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 0.2758 - val_loss: 0.5105\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 0.2717 - val_loss: 0.5195\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 0.2719 - val_loss: 0.5107\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2681 - val_loss: 0.5017\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2694 - val_loss: 0.5178\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.2692 - val_loss: 0.5356\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2683 - val_loss: 0.5264\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.2687 - val_loss: 0.5012\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2666 - val_loss: 0.5089\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2669 - val_loss: 0.5370\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 0.2672 - val_loss: 0.5342\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2669 - val_loss: 0.5089\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.2664 - val_loss: 0.5072\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 0.2642 - val_loss: 0.5376\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.2611 - val_loss: 0.5879\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.2652 - val_loss: 0.5570\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.2607 - val_loss: 0.5560\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2631 - val_loss: 0.5798\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2611 - val_loss: 0.5751\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2608 - val_loss: 0.5609\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.2628 - val_loss: 0.5688\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 0.2634 - val_loss: 0.5803\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.2603 - val_loss: 0.5936\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.2615 - val_loss: 0.6150\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.2585 - val_loss: 0.5833\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.2595 - val_loss: 0.5841\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2609 - val_loss: 0.6173\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.2579 - val_loss: 0.6222\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.2592 - val_loss: 0.6492\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.2561 - val_loss: 0.6205\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2589 - val_loss: 0.6045\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.2584 - val_loss: 0.6051\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 0.2587 - val_loss: 0.6231\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.2559 - val_loss: 0.6058\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 1s 617ms/step - loss: 0.2557 - val_loss: 0.6083\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 1s 579ms/step - loss: 0.2567 - val_loss: 0.6278\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.2571 - val_loss: 0.6149\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.2551 - val_loss: 0.6608\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 0.2575 - val_loss: 0.6471\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.2558 - val_loss: 0.6104\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2540 - val_loss: 0.6312\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2565 - val_loss: 0.6112\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2538 - val_loss: 0.6727\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2571 - val_loss: 0.5817\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2569 - val_loss: 0.6018\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2544 - val_loss: 0.6163\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2558 - val_loss: 0.5972\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2552 - val_loss: 0.6532\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2550 - val_loss: 0.5789\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2531 - val_loss: 0.5763\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.2526 - val_loss: 0.6290\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2516 - val_loss: 0.6070\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2514 - val_loss: 0.5917\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.2521 - val_loss: 0.6197\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2507 - val_loss: 0.5919\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2526 - val_loss: 0.5971\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2523 - val_loss: 0.6383\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2516 - val_loss: 0.5896\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2517 - val_loss: 0.6091\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2533 - val_loss: 0.6076\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.2510 - val_loss: 0.5875\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.2529 - val_loss: 0.6337\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.2524 - val_loss: 0.6011\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2500 - val_loss: 0.6106\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2493 - val_loss: 0.5840\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2528 - val_loss: 0.6171\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2511 - val_loss: 0.5944\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2515 - val_loss: 0.6023\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2499 - val_loss: 0.6086\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2488 - val_loss: 0.6010\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2502 - val_loss: 0.6189\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2490 - val_loss: 0.5958\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.2483 - val_loss: 0.5828\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 0.2491 - val_loss: 0.6452\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 0.2501 - val_loss: 0.5452\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.2547 - val_loss: 0.5554\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.2524 - val_loss: 0.6062\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2526 - val_loss: 0.5553\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2509 - val_loss: 0.5315\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.2535 - val_loss: 0.5483\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2522 - val_loss: 0.5867\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2504 - val_loss: 0.5483\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.2541 - val_loss: 0.5619\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2498 - val_loss: 0.5446\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2501 - val_loss: 0.5377\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2488 - val_loss: 0.5390\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.2507 - val_loss: 0.5468\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 0.2481 - val_loss: 0.5440\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.2497 - val_loss: 0.5400\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 0.2476 - val_loss: 0.5462\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2487 - val_loss: 0.5377\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2468 - val_loss: 0.5295\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2482 - val_loss: 0.5265\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.2456 - val_loss: 0.5417\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.2477 - val_loss: 0.5423\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.2426 - val_loss: 0.5375\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2456 - val_loss: 0.5477\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2475 - val_loss: 0.5342\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2467 - val_loss: 0.5360\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.2452 - val_loss: 0.5468\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2461 - val_loss: 0.5403\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2447 - val_loss: 0.5301\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2441 - val_loss: 0.5328\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2441 - val_loss: 0.5446\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2473 - val_loss: 0.5338\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.2449 - val_loss: 0.5306\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.2446 - val_loss: 0.5354\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2426 - val_loss: 0.5359\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2451 - val_loss: 0.5385\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 380ms/step - loss: 0.2469 - val_loss: 0.5445\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 0.2441 - val_loss: 0.5380\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 0.2465 - val_loss: 0.5256\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 0.2481 - val_loss: 0.5281\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2445 - val_loss: 0.5414\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.2435 - val_loss: 0.5293\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.2438 - val_loss: 0.5212\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2456 - val_loss: 0.5265\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 0.2445 - val_loss: 0.5328\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.2451 - val_loss: 0.5248\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.2463 - val_loss: 0.5214\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.2433 - val_loss: 0.5203\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2442 - val_loss: 0.5144\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2433 - val_loss: 0.5062\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.2440 - val_loss: 0.5095\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2426 - val_loss: 0.5222\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2442 - val_loss: 0.5036\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2448 - val_loss: 0.5163\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2469 - val_loss: 0.5139\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2449 - val_loss: 0.5091\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2429 - val_loss: 0.5148\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.2432 - val_loss: 0.5174\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2425 - val_loss: 0.5236\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2425 - val_loss: 0.5341\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2422 - val_loss: 0.5367\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2395 - val_loss: 0.5184\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2426 - val_loss: 0.5121\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2423 - val_loss: 0.5083\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2426 - val_loss: 0.5188\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2424 - val_loss: 0.5316\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2442 - val_loss: 0.5284\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2418 - val_loss: 0.5201\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2398 - val_loss: 0.5196\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2395 - val_loss: 0.5230\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2385 - val_loss: 0.5273\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2415 - val_loss: 0.5316\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2393 - val_loss: 0.5442\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 425ms/step - loss: 0.2400 - val_loss: 0.5456\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 0.2388 - val_loss: 0.5294\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 0.2394 - val_loss: 0.5281\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.2386 - val_loss: 0.5306\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2387 - val_loss: 0.5307\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2411 - val_loss: 0.5311\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.2375 - val_loss: 0.5414\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2404 - val_loss: 0.5382\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2383 - val_loss: 0.5318\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2380 - val_loss: 0.5418\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2401 - val_loss: 0.5308\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2375 - val_loss: 0.5285\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2386 - val_loss: 0.5394\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2373 - val_loss: 0.5477\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.2406 - val_loss: 0.5251\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.2387 - val_loss: 0.5336\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.2367 - val_loss: 0.5534\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.2393 - val_loss: 0.5389\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2363 - val_loss: 0.5296\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2382 - val_loss: 0.5224\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.2371 - val_loss: 0.5321\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2389 - val_loss: 0.5445\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.2379 - val_loss: 0.5235\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2411 - val_loss: 0.5197\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2384 - val_loss: 0.5301\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2371 - val_loss: 0.5387\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2403 - val_loss: 0.5282\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2384 - val_loss: 0.5202\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 0.2383 - val_loss: 0.5234\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.2363 - val_loss: 0.5386\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 376ms/step - loss: 0.2364 - val_loss: 0.5567\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.2353 - val_loss: 0.5528\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.2350 - val_loss: 0.5405\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 383ms/step - loss: 0.2346 - val_loss: 0.5418\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.2359 - val_loss: 0.5474\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 0.2356 - val_loss: 0.5521\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.2346 - val_loss: 0.5368\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 0.2323 - val_loss: 0.5264\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.2367 - val_loss: 0.5361\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.2349 - val_loss: 0.5503\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2356 - val_loss: 0.5490\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2352 - val_loss: 0.5394\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2344 - val_loss: 0.5342\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.2351 - val_loss: 0.5344\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.2358 - val_loss: 0.5431\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.2344 - val_loss: 0.5515\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.2374 - val_loss: 0.5514\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.2324 - val_loss: 0.5487\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2369 - val_loss: 0.5479\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.2346 - val_loss: 0.5595\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.2374 - val_loss: 0.5614\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 383ms/step - loss: 0.2351 - val_loss: 0.5493\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.2363 - val_loss: 0.5464\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.2355 - val_loss: 0.5501\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.2339 - val_loss: 0.5467\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2365 - val_loss: 0.5429\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2352 - val_loss: 0.5360\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2360 - val_loss: 0.5412\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2330 - val_loss: 0.5495\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.2364 - val_loss: 0.5489\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2346 - val_loss: 0.5338\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2339 - val_loss: 0.5394\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.2358 - val_loss: 0.5435\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2342 - val_loss: 0.5515\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2355 - val_loss: 0.5596\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2329 - val_loss: 0.5560\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2351 - val_loss: 0.5465\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2340 - val_loss: 0.5472\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2366 - val_loss: 0.5414\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.2322 - val_loss: 0.5400\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 0.2333 - val_loss: 0.5402\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 1s 595ms/step - loss: 0.2360 - val_loss: 0.5531\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.2347 - val_loss: 0.5640\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 0.2332 - val_loss: 0.5632\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 391ms/step - loss: 0.2325 - val_loss: 0.5559\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.2342 - val_loss: 0.5526\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.2331 - val_loss: 0.5514\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.2339 - val_loss: 0.5580\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2353 - val_loss: 0.5560\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.2349 - val_loss: 0.5510\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 335ms/step - loss: 0.2338 - val_loss: 0.5486\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.2337 - val_loss: 0.5507\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.2350 - val_loss: 0.5499\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.2326 - val_loss: 0.5558\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2307 - val_loss: 0.5627\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2339 - val_loss: 0.5649\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.2337 - val_loss: 0.5562\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2332 - val_loss: 0.5417\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2335 - val_loss: 0.5508\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2301 - val_loss: 0.5564\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.2330 - val_loss: 0.5563\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2325 - val_loss: 0.5628\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2336 - val_loss: 0.5519\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2330 - val_loss: 0.5428\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2318 - val_loss: 0.5487\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2331 - val_loss: 0.5805\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2339 - val_loss: 0.5818\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2329 - val_loss: 0.5583\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2350 - val_loss: 0.5525\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2346 - val_loss: 0.5600\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2316 - val_loss: 0.5708\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2325 - val_loss: 0.5843\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2336 - val_loss: 0.5776\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2317 - val_loss: 0.5657\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.2318 - val_loss: 0.5529\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2324 - val_loss: 0.5525\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2345 - val_loss: 0.5540\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 0.2337 - val_loss: 0.5570\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 0.2335 - val_loss: 0.5609\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.2336 - val_loss: 0.5717\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 0.2309 - val_loss: 0.5692\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2339 - val_loss: 0.5646\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2331 - val_loss: 0.5777\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2321 - val_loss: 0.5859\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2297 - val_loss: 0.5751\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2310 - val_loss: 0.5688\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2321 - val_loss: 0.5820\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2312 - val_loss: 0.5873\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2293 - val_loss: 0.5760\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2314 - val_loss: 0.5683\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2314 - val_loss: 0.5904\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2320 - val_loss: 0.5957\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.2297 - val_loss: 0.5835\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2312 - val_loss: 0.5738\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2304 - val_loss: 0.5889\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2331 - val_loss: 0.6098\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2296 - val_loss: 0.6022\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.2297 - val_loss: 0.5912\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2291 - val_loss: 0.5777\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2308 - val_loss: 0.5896\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2300 - val_loss: 0.5974\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2293 - val_loss: 0.5923\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2304 - val_loss: 0.5925\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2302 - val_loss: 0.5869\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2327 - val_loss: 0.5941\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.2305 - val_loss: 0.5949\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2315 - val_loss: 0.5955\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2319 - val_loss: 0.5947\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2316 - val_loss: 0.5850\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2288 - val_loss: 0.5816\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2338 - val_loss: 0.5861\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2330 - val_loss: 0.5681\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2332 - val_loss: 0.5728\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2312 - val_loss: 0.5630\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2325 - val_loss: 0.5553\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.2360 - val_loss: 0.5653\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 0.2323 - val_loss: 0.5848\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 0.2319 - val_loss: 0.5863\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 0.2322 - val_loss: 0.5694\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2295 - val_loss: 0.5728\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2269 - val_loss: 0.5807\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2293 - val_loss: 0.5855\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2296 - val_loss: 0.5718\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2304 - val_loss: 0.5670\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2279 - val_loss: 0.5717\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.2295 - val_loss: 0.5714\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2289 - val_loss: 0.5800\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2283 - val_loss: 0.5664\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2338 - val_loss: 0.5635\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2288 - val_loss: 0.5869\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2286 - val_loss: 0.5877\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2273 - val_loss: 0.5963\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2310 - val_loss: 0.5901\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2303 - val_loss: 0.5935\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2296 - val_loss: 0.6005\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2274 - val_loss: 0.6001\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2293 - val_loss: 0.5932\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2269 - val_loss: 0.5723\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2276 - val_loss: 0.5709\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2300 - val_loss: 0.5962\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2309 - val_loss: 0.5906\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.2283 - val_loss: 0.5964\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2292 - val_loss: 0.6184\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.2311 - val_loss: 0.5865\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2284 - val_loss: 0.5671\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2298 - val_loss: 0.5860\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.2274 - val_loss: 0.6045\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.2264 - val_loss: 0.6053\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 0.2301 - val_loss: 0.6093\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 0.2313 - val_loss: 0.5883\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.2309 - val_loss: 0.5763\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 0.2316 - val_loss: 0.5793\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.2275 - val_loss: 0.5747\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 0.2293 - val_loss: 0.5810\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.2314 - val_loss: 0.6106\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 0.2312 - val_loss: 0.6190\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.2301 - val_loss: 0.5884\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2279 - val_loss: 0.5675\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2296 - val_loss: 0.5794\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2272 - val_loss: 0.6007\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2309 - val_loss: 0.5981\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2275 - val_loss: 0.5744\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2308 - val_loss: 0.5817\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2308 - val_loss: 0.5898\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2266 - val_loss: 0.5845\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2296 - val_loss: 0.5745\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2299 - val_loss: 0.5822\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2292 - val_loss: 0.5933\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2293 - val_loss: 0.5986\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2294 - val_loss: 0.5888\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2285 - val_loss: 0.5889\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2293 - val_loss: 0.5983\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2283 - val_loss: 0.5871\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2294 - val_loss: 0.5624\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.2274 - val_loss: 0.5692\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2247 - val_loss: 0.5877\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2289 - val_loss: 0.6012\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2268 - val_loss: 0.6052\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.2273 - val_loss: 0.5857\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.2306 - val_loss: 0.5512\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.2291 - val_loss: 0.5578\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.2282 - val_loss: 0.5868\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2274 - val_loss: 0.5794\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2268 - val_loss: 0.5600\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2273 - val_loss: 0.5625\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2271 - val_loss: 0.5740\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.2283 - val_loss: 0.5703\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 0.2260 - val_loss: 0.5630\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 0.2281 - val_loss: 0.5696\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 0.2288 - val_loss: 0.5734\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.2276 - val_loss: 0.6023\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2270 - val_loss: 0.5976\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 0.2267 - val_loss: 0.5743\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 0.2284 - val_loss: 0.5683\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.2297 - val_loss: 0.5938\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.2271 - val_loss: 0.6116\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.2268 - val_loss: 0.6158\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 0.2271 - val_loss: 0.5814\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.2259 - val_loss: 0.5442\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.2261 - val_loss: 0.5442\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.2287 - val_loss: 0.5695\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.2271 - val_loss: 0.5971\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 0.2272 - val_loss: 0.6036\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.2296 - val_loss: 0.5999\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.2271 - val_loss: 0.5881\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.2257 - val_loss: 0.5653\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.2286 - val_loss: 0.5595\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2278 - val_loss: 0.5598\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2265 - val_loss: 0.5592\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2280 - val_loss: 0.5626\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2273 - val_loss: 0.5635\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2245 - val_loss: 0.5598\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2240 - val_loss: 0.5548\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.2247 - val_loss: 0.5515\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2290 - val_loss: 0.5668\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2271 - val_loss: 0.5689\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.2264 - val_loss: 0.5600\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2275 - val_loss: 0.5502\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2260 - val_loss: 0.5483\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2260 - val_loss: 0.5514\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2249 - val_loss: 0.5558\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2278 - val_loss: 0.5592\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 0.2277 - val_loss: 0.5531\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 0.2243 - val_loss: 0.5530\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 0.2225 - val_loss: 0.5600\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 0.2252 - val_loss: 0.5623\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.2252 - val_loss: 0.5544\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2260 - val_loss: 0.5592\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2246 - val_loss: 0.5546\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.2261 - val_loss: 0.5421\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2265 - val_loss: 0.5434\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2256 - val_loss: 0.5595\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2262 - val_loss: 0.5740\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2287 - val_loss: 0.5632\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2253 - val_loss: 0.5404\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2280 - val_loss: 0.5313\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2260 - val_loss: 0.5490\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2230 - val_loss: 0.5694\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2270 - val_loss: 0.5542\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.2264 - val_loss: 0.5506\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2265 - val_loss: 0.5351\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2247 - val_loss: 0.5286\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2262 - val_loss: 0.5472\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.2270 - val_loss: 0.5603\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.2273 - val_loss: 0.5518\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2244 - val_loss: 0.5499\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.2236 - val_loss: 0.5399\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2248 - val_loss: 0.5219\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2305 - val_loss: 0.5427\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.2235 - val_loss: 0.5435\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 0.2280 - val_loss: 0.5230\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2242 - val_loss: 0.5352\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.2263 - val_loss: 0.5355\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.2229 - val_loss: 0.5241\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2268 - val_loss: 0.5153\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2254 - val_loss: 0.5423\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2237 - val_loss: 0.5311\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2249 - val_loss: 0.5052\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2257 - val_loss: 0.5016\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.2258 - val_loss: 0.5272\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.2259 - val_loss: 0.5463\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.2265 - val_loss: 0.5396\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.2264 - val_loss: 0.5331\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2248 - val_loss: 0.5375\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.2240 - val_loss: 0.5319\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2247 - val_loss: 0.5147\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2254 - val_loss: 0.5032\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.2268 - val_loss: 0.5117\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2223 - val_loss: 0.5153\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2271 - val_loss: 0.5156\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 442ms/step - loss: 0.2267 - val_loss: 0.5121\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2233 - val_loss: 0.5169\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2255 - val_loss: 0.5295\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2262 - val_loss: 0.5151\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2286 - val_loss: 0.5012\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2247 - val_loss: 0.4998\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2264 - val_loss: 0.5275\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2282 - val_loss: 0.5280\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2257 - val_loss: 0.5066\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2272 - val_loss: 0.5054\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2250 - val_loss: 0.5116\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2245 - val_loss: 0.5098\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2246 - val_loss: 0.4973\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2254 - val_loss: 0.5091\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2249 - val_loss: 0.5321\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2233 - val_loss: 0.5340\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2254 - val_loss: 0.5178\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2232 - val_loss: 0.5105\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2199 - val_loss: 0.5149\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2240 - val_loss: 0.5130\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2240 - val_loss: 0.5082\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.2241 - val_loss: 0.5074\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.2248 - val_loss: 0.5147\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2271 - val_loss: 0.5299\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.2221 - val_loss: 0.5184\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.2243 - val_loss: 0.5125\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 0.2234 - val_loss: 0.5176\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 0.2241 - val_loss: 0.5250\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 0.2240 - val_loss: 0.5142\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.2252 - val_loss: 0.5073\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.2223 - val_loss: 0.5051\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2277 - val_loss: 0.4941\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 0.2247 - val_loss: 0.4995\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2236 - val_loss: 0.5006\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.2240 - val_loss: 0.4932\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.2265 - val_loss: 0.4957\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2251 - val_loss: 0.5181\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.2246 - val_loss: 0.5155\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.2248 - val_loss: 0.5049\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2248 - val_loss: 0.4995\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2235 - val_loss: 0.5223\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2243 - val_loss: 0.5385\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2269 - val_loss: 0.5220\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2229 - val_loss: 0.4943\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2222 - val_loss: 0.4984\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.2237 - val_loss: 0.4996\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.2238 - val_loss: 0.4868\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2223 - val_loss: 0.4761\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.2247 - val_loss: 0.4771\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2242 - val_loss: 0.4773\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2243 - val_loss: 0.4720\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2227 - val_loss: 0.4795\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2251 - val_loss: 0.4984\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2231 - val_loss: 0.5086\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2246 - val_loss: 0.5055\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.2241 - val_loss: 0.5049\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2206 - val_loss: 0.4998\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2242 - val_loss: 0.5004\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2221 - val_loss: 0.5058\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2235 - val_loss: 0.5086\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2237 - val_loss: 0.5219\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.2243 - val_loss: 0.5102\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.2236 - val_loss: 0.4965\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 0.2231 - val_loss: 0.4892\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 0.2274 - val_loss: 0.5086\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.2236 - val_loss: 0.5108\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.2288 - val_loss: 0.4954\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 371ms/step - loss: 0.2219 - val_loss: 0.4867\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.2267 - val_loss: 0.4874\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.2227 - val_loss: 0.5117\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.2237 - val_loss: 0.5250\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2209 - val_loss: 0.5072\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.2236 - val_loss: 0.4998\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2202 - val_loss: 0.5097\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2237 - val_loss: 0.5116\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2236 - val_loss: 0.5242\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2247 - val_loss: 0.5269\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.2235 - val_loss: 0.5140\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2240 - val_loss: 0.5141\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2260 - val_loss: 0.5312\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2223 - val_loss: 0.5400\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2227 - val_loss: 0.5272\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2260 - val_loss: 0.5279\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2261 - val_loss: 0.5275\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2215 - val_loss: 0.5211\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2235 - val_loss: 0.5218\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.2222 - val_loss: 0.5136\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2262 - val_loss: 0.5161\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2234 - val_loss: 0.5168\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2239 - val_loss: 0.5110\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2268 - val_loss: 0.5023\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2224 - val_loss: 0.5052\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2239 - val_loss: 0.5157\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.2222 - val_loss: 0.5180\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2234 - val_loss: 0.5228\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2251 - val_loss: 0.5244\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.2267 - val_loss: 0.5167\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.2246 - val_loss: 0.4984\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 0.2227 - val_loss: 0.4991\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 0.2224 - val_loss: 0.5108\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 0.2230 - val_loss: 0.5226\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.2215 - val_loss: 0.5407\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2232 - val_loss: 0.5305\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2243 - val_loss: 0.5114\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2218 - val_loss: 0.5195\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2203 - val_loss: 0.5312\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2213 - val_loss: 0.5297\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2210 - val_loss: 0.5089\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2248 - val_loss: 0.4994\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2242 - val_loss: 0.4986\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2218 - val_loss: 0.5119\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2216 - val_loss: 0.4990\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2218 - val_loss: 0.4995\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2213 - val_loss: 0.5145\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2228 - val_loss: 0.5151\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2224 - val_loss: 0.5132\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.2199 - val_loss: 0.5072\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2216 - val_loss: 0.5017\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2205 - val_loss: 0.5065\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2211 - val_loss: 0.5094\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2225 - val_loss: 0.5020\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.2234 - val_loss: 0.4987\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2205 - val_loss: 0.5132\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2236 - val_loss: 0.5176\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2219 - val_loss: 0.5102\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2228 - val_loss: 0.5080\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2210 - val_loss: 0.5200\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.2230 - val_loss: 0.5251\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.2244 - val_loss: 0.5116\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.2215 - val_loss: 0.5114\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2223 - val_loss: 0.5205\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.2192 - val_loss: 0.5361\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2232 - val_loss: 0.5184\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2205 - val_loss: 0.5129\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.2234 - val_loss: 0.5255\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.2182 - val_loss: 0.5359\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.2214 - val_loss: 0.5205\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 0.2210 - val_loss: 0.5117\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 0.2199 - val_loss: 0.5304\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.2217 - val_loss: 0.5394\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.2225 - val_loss: 0.5142\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2201 - val_loss: 0.5120\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2242 - val_loss: 0.5248\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2219 - val_loss: 0.5119\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2229 - val_loss: 0.5084\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2230 - val_loss: 0.5005\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 0.2216 - val_loss: 0.4976\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2193 - val_loss: 0.4949\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2203 - val_loss: 0.5117\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2276 - val_loss: 0.5175\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.2216 - val_loss: 0.5238\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.2210 - val_loss: 0.5201\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 371ms/step - loss: 0.2212 - val_loss: 0.5243\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.2209 - val_loss: 0.5261\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.2198 - val_loss: 0.5084\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.2213 - val_loss: 0.5108\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2199 - val_loss: 0.5235\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2210 - val_loss: 0.5292\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2216 - val_loss: 0.5210\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2202 - val_loss: 0.5261\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2208 - val_loss: 0.5277\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2204 - val_loss: 0.5055\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2216 - val_loss: 0.4945\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.2247 - val_loss: 0.4987\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2233 - val_loss: 0.5099\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2204 - val_loss: 0.5071\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2203 - val_loss: 0.5132\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2229 - val_loss: 0.5257\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.2210 - val_loss: 0.5141\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2235 - val_loss: 0.5091\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2207 - val_loss: 0.5160\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.2207 - val_loss: 0.5108\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 0.2201 - val_loss: 0.4948\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.2230 - val_loss: 0.5007\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 0.2213 - val_loss: 0.5080\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.2214 - val_loss: 0.5078\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2196 - val_loss: 0.5056\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2210 - val_loss: 0.5130\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.2223 - val_loss: 0.5298\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2225 - val_loss: 0.5167\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.2196 - val_loss: 0.5212\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.2195 - val_loss: 0.5256\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2222 - val_loss: 0.5096\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2200 - val_loss: 0.5001\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2201 - val_loss: 0.4920\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2210 - val_loss: 0.5003\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.2231 - val_loss: 0.4947\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.2185 - val_loss: 0.4879\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.2177 - val_loss: 0.4967\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2232 - val_loss: 0.5024\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2192 - val_loss: 0.4955\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2201 - val_loss: 0.4868\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2201 - val_loss: 0.5021\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2220 - val_loss: 0.4858\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2204 - val_loss: 0.4946\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2215 - val_loss: 0.5026\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2198 - val_loss: 0.4946\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 0.2214 - val_loss: 0.4931\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.2180 - val_loss: 0.5083\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.2215 - val_loss: 0.5124\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2212 - val_loss: 0.4983\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2204 - val_loss: 0.4947\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2187 - val_loss: 0.4969\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2218 - val_loss: 0.5033\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2211 - val_loss: 0.4996\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2180 - val_loss: 0.4956\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2189 - val_loss: 0.4854\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.2188 - val_loss: 0.4834\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 0.2225 - val_loss: 0.4865\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 0.2207 - val_loss: 0.4944\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 0.2208 - val_loss: 0.5018\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 386ms/step - loss: 0.2219 - val_loss: 0.5026\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.2209 - val_loss: 0.5008\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2191 - val_loss: 0.4912\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2209 - val_loss: 0.4820\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2209 - val_loss: 0.4833\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2216 - val_loss: 0.4873\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2188 - val_loss: 0.4881\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2192 - val_loss: 0.4911\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2197 - val_loss: 0.5044\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2192 - val_loss: 0.5194\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2229 - val_loss: 0.4889\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 335ms/step - loss: 0.2197 - val_loss: 0.4761\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.2179 - val_loss: 0.4763\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.2238 - val_loss: 0.4749\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.2219 - val_loss: 0.4805\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.2182 - val_loss: 0.4932\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2188 - val_loss: 0.4892\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2201 - val_loss: 0.4890\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2198 - val_loss: 0.4943\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2202 - val_loss: 0.4957\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.2190 - val_loss: 0.4925\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2206 - val_loss: 0.4923\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2197 - val_loss: 0.4966\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2193 - val_loss: 0.4993\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2170 - val_loss: 0.4889\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2207 - val_loss: 0.4825\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.2202 - val_loss: 0.4870\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 0.2206 - val_loss: 0.4825\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2191 - val_loss: 0.4848\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2167 - val_loss: 0.4893\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2205 - val_loss: 0.4930\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2207 - val_loss: 0.4866\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2181 - val_loss: 0.4969\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.2201 - val_loss: 0.4921\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 0.2205 - val_loss: 0.4915\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 0.2200 - val_loss: 0.4965\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 0.2207 - val_loss: 0.4930\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2193 - val_loss: 0.4866\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2198 - val_loss: 0.4849\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.2202 - val_loss: 0.4922\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2211 - val_loss: 0.4789\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2220 - val_loss: 0.4702\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.2209 - val_loss: 0.4774\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2206 - val_loss: 0.4852\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2217 - val_loss: 0.4828\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2190 - val_loss: 0.4816\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2211 - val_loss: 0.4829\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.2180 - val_loss: 0.4835\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2198 - val_loss: 0.4978\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2198 - val_loss: 0.4965\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2228 - val_loss: 0.4855\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2201 - val_loss: 0.4902\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2222 - val_loss: 0.5060\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2195 - val_loss: 0.5202\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2193 - val_loss: 0.5063\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.2197 - val_loss: 0.4956\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.2209 - val_loss: 0.4849\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2217 - val_loss: 0.4788\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2190 - val_loss: 0.4888\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.2211 - val_loss: 0.5020\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2230 - val_loss: 0.4920\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.2230 - val_loss: 0.4833\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2190 - val_loss: 0.4814\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2211 - val_loss: 0.4723\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2222 - val_loss: 0.4796\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2213 - val_loss: 0.4939\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.2189 - val_loss: 0.4946\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2191 - val_loss: 0.4856\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2178 - val_loss: 0.4953\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.2181 - val_loss: 0.5026\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 0.2199 - val_loss: 0.5010\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 0.2193 - val_loss: 0.4909\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 0.2190 - val_loss: 0.4879\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 0.2202 - val_loss: 0.4939\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 0.2187 - val_loss: 0.4951\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.2199 - val_loss: 0.4907\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2204 - val_loss: 0.4927\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2194 - val_loss: 0.4958\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2196 - val_loss: 0.4890\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2180 - val_loss: 0.4749\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2198 - val_loss: 0.4806\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2181 - val_loss: 0.4814\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2185 - val_loss: 0.4853\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.2211 - val_loss: 0.4871\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.2203 - val_loss: 0.4873\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2194 - val_loss: 0.4912\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2192 - val_loss: 0.4982\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2199 - val_loss: 0.5004\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2197 - val_loss: 0.4853\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2185 - val_loss: 0.4852\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2221 - val_loss: 0.4962\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2191 - val_loss: 0.5063\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2189 - val_loss: 0.5026\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.2200 - val_loss: 0.4935\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2166 - val_loss: 0.4850\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2189 - val_loss: 0.4918\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.2168 - val_loss: 0.4928\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2181 - val_loss: 0.4924\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2164 - val_loss: 0.4814\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2186 - val_loss: 0.4901\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.2204 - val_loss: 0.5103\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2211 - val_loss: 0.5117\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2173 - val_loss: 0.5090\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 0.2178 - val_loss: 0.5084\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.2175 - val_loss: 0.4949\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.2177 - val_loss: 0.4893\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.2162 - val_loss: 0.4957\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 1s 589ms/step - loss: 0.2199 - val_loss: 0.5034\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 0.2170 - val_loss: 0.5058\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 0.2187 - val_loss: 0.5005\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2169 - val_loss: 0.4946\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2206 - val_loss: 0.4966\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2180 - val_loss: 0.4960\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2216 - val_loss: 0.5017\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2201 - val_loss: 0.5050\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2174 - val_loss: 0.5008\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2181 - val_loss: 0.5023\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2193 - val_loss: 0.5094\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2168 - val_loss: 0.5073\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2186 - val_loss: 0.4807\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2174 - val_loss: 0.4662\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2187 - val_loss: 0.4760\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.2200 - val_loss: 0.4855\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2193 - val_loss: 0.4745\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2184 - val_loss: 0.4853\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2217 - val_loss: 0.5197\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.2188 - val_loss: 0.5215\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.2168 - val_loss: 0.4880\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 0.2200 - val_loss: 0.4854\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.2195 - val_loss: 0.4960\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2202 - val_loss: 0.5002\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2207 - val_loss: 0.4930\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2207 - val_loss: 0.4934\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2192 - val_loss: 0.4985\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2216 - val_loss: 0.5163\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.2189 - val_loss: 0.5199\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.2215 - val_loss: 0.5139\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2169 - val_loss: 0.5109\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.2170 - val_loss: 0.4771\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.2184 - val_loss: 0.4715\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2197 - val_loss: 0.4990\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2202 - val_loss: 0.5086\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 0.2203 - val_loss: 0.5012\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 0.2192 - val_loss: 0.4962\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 0.2191 - val_loss: 0.5017\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.2195 - val_loss: 0.5096\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.2180 - val_loss: 0.5048\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.2172 - val_loss: 0.4974\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.2193 - val_loss: 0.4856\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 0.2195 - val_loss: 0.4977\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2201 - val_loss: 0.5161\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2189 - val_loss: 0.5167\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.2171 - val_loss: 0.4863\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2202 - val_loss: 0.4874\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2189 - val_loss: 0.5021\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2207 - val_loss: 0.4925\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.2209 - val_loss: 0.4891\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.2199 - val_loss: 0.4954\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.2151 - val_loss: 0.5033\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.2200 - val_loss: 0.5010\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.2186 - val_loss: 0.4968\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 0.2196 - val_loss: 0.4971\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.2213 - val_loss: 0.5043\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2176 - val_loss: 0.5088\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2178 - val_loss: 0.5051\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.2216 - val_loss: 0.5005\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.2181 - val_loss: 0.4918\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2193 - val_loss: 0.5008\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2184 - val_loss: 0.5060\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2205 - val_loss: 0.4933\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2197 - val_loss: 0.4840\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2181 - val_loss: 0.5057\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2184 - val_loss: 0.5140\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.2195 - val_loss: 0.4963\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2197 - val_loss: 0.5032\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2237 - val_loss: 0.5060\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2205 - val_loss: 0.5162\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 0.2151 - val_loss: 0.5246\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.2182 - val_loss: 0.5211\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 1s 580ms/step - loss: 0.2207 - val_loss: 0.5119\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.2199 - val_loss: 0.5176\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.2205 - val_loss: 0.5189\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2168 - val_loss: 0.5148\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.2199 - val_loss: 0.4937\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2205 - val_loss: 0.4880\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2175 - val_loss: 0.5152\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2185 - val_loss: 0.5179\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2179 - val_loss: 0.4956\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2179 - val_loss: 0.4901\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2194 - val_loss: 0.4965\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2169 - val_loss: 0.4979\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.2185 - val_loss: 0.4928\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2180 - val_loss: 0.5007\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2194 - val_loss: 0.5004\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2168 - val_loss: 0.5199\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2195 - val_loss: 0.5301\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2194 - val_loss: 0.5139\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2209 - val_loss: 0.4968\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2187 - val_loss: 0.5023\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2190 - val_loss: 0.5036\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.2194 - val_loss: 0.5113\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 335ms/step - loss: 0.2155 - val_loss: 0.4995\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 0.2178 - val_loss: 0.4993\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.2193 - val_loss: 0.5059\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.2184 - val_loss: 0.5079\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2171 - val_loss: 0.5032\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.2174 - val_loss: 0.4955\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.2177 - val_loss: 0.5007\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.2185 - val_loss: 0.5093\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2180 - val_loss: 0.5066\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2200 - val_loss: 0.5041\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2205 - val_loss: 0.4982\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.2180 - val_loss: 0.5073\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.2181 - val_loss: 0.5122\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 0.2189 - val_loss: 0.5000\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 0.2168 - val_loss: 0.4887\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.2159 - val_loss: 0.4910\n"
          ]
        }
      ],
      "source": [
        "h_gru_6 = model_GRU_6.fit(X_TRAIN,Y_TRAIN,\n",
        "          epochs=1000,\n",
        "          batch_size=X_TRAIN.shape[0],\n",
        "          validation_data = (X_VALID, Y_VALID),\n",
        "          verbose = True\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AJ9egZv1hQPa",
        "outputId": "3c6cb932-8b2e-4691-bdaa-2f1b671077e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b123597d480>]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h_gru_6.history[\"loss\"])\n",
        "plt.plot(h_gru_6.history[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j8Ufe6aToKeT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyOYqmzHoK2W"
      },
      "source": [
        "# 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I6YTIZYroLtE",
        "outputId": "8a71c109-72f9-4e56-e34b-608bdce5c938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 149, 31)]         0         \n",
            "                                                                 \n",
            " gru_6 (GRU)                 (None, 1)                 102       \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1)                 0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 2         \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 1)                 0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 31)                62        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 166\n",
            "Trainable params: 166\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(X_TRAIN.shape[1:]))\n",
        "m = inputs\n",
        "\n",
        "mA = GRU(units=1, kernel_regularizer=regularizers.l1_l2(l1=0.1, l2=0.1))(m)\n",
        "\n",
        "mA = Dropout(0.1)(mA)\n",
        "\n",
        "mA = Dense(units=1, activation = 'relu')(mA)\n",
        "\n",
        "m = Flatten()(mA)\n",
        "\n",
        "# A침adir capa Dense de salida\n",
        "out = Dense(Y_TRAIN.shape[1], activation='tanh')(m)\n",
        "\n",
        "model_GRU_7 = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_GRU_7.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse',\n",
        "          metrics=[])\n",
        "\n",
        "model_GRU_7.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X6ppkBV2oc3v",
        "outputId": "10a8fa32-9038-48c8-baba-5edc103e78cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5853 - val_loss: 2.4583\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 2.4563 - val_loss: 2.3384\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 2.3342 - val_loss: 2.2304\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 2.2239 - val_loss: 2.1283\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 2.1195 - val_loss: 2.0256\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 2.0147 - val_loss: 1.9261\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 1.9134 - val_loss: 1.8314\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 1.8171 - val_loss: 1.7357\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 1.7199 - val_loss: 1.6466\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 1.6293 - val_loss: 1.5625\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 1.5439 - val_loss: 1.4814\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.4615 - val_loss: 1.4094\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 1.3885 - val_loss: 1.3401\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.3182 - val_loss: 1.2728\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 1.2502 - val_loss: 1.2111\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 1.1878 - val_loss: 1.1542\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 1.1303 - val_loss: 1.0985\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 1.0742 - val_loss: 1.0450\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 1.0204 - val_loss: 0.9900\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.9652 - val_loss: 0.9357\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.9108 - val_loss: 0.8861\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.8611 - val_loss: 0.8412\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.8162 - val_loss: 0.7984\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.7735 - val_loss: 0.7600\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.7353 - val_loss: 0.7238\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.6993 - val_loss: 0.6895\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.6653 - val_loss: 0.6569\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.6330 - val_loss: 0.6250\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.6013 - val_loss: 0.5958\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.5724 - val_loss: 0.5662\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.5431 - val_loss: 0.5466\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.5236 - val_loss: 0.5350\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.5122 - val_loss: 0.5231\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5005 - val_loss: 0.5122\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.4899 - val_loss: 0.4985\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.4763 - val_loss: 0.4873\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.4653 - val_loss: 0.4793\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.4574 - val_loss: 0.4711\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.4495 - val_loss: 0.4632\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.4417 - val_loss: 0.4573\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.4359 - val_loss: 0.4511\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4299 - val_loss: 0.4423\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.4212 - val_loss: 0.4349\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.4139 - val_loss: 0.4309\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.4100 - val_loss: 0.4269\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.4061 - val_loss: 0.4235\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.4028 - val_loss: 0.4214\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.4006 - val_loss: 0.4199\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3992 - val_loss: 0.4189\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3981 - val_loss: 0.4175\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3967 - val_loss: 0.4160\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3952 - val_loss: 0.4121\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3912 - val_loss: 0.4065\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3855 - val_loss: 0.4012\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3802 - val_loss: 0.3977\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3766 - val_loss: 0.3939\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3727 - val_loss: 0.3907\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.3694 - val_loss: 0.3907\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3694 - val_loss: 0.3904\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3690 - val_loss: 0.3890\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3676 - val_loss: 0.3885\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.3670 - val_loss: 0.3879\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3665 - val_loss: 0.3879\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3664 - val_loss: 0.3871\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.3656 - val_loss: 0.3864\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.3649 - val_loss: 0.3830\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.3615 - val_loss: 0.3818\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.3603 - val_loss: 0.3810\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3596 - val_loss: 0.3791\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.3576 - val_loss: 0.3788\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.3574 - val_loss: 0.3787\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.3573 - val_loss: 0.3781\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.3567 - val_loss: 0.3777\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3563 - val_loss: 0.3759\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3545 - val_loss: 0.3762\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3548 - val_loss: 0.3760\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3546 - val_loss: 0.3758\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3544 - val_loss: 0.3740\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3526 - val_loss: 0.3743\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3528 - val_loss: 0.3745\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.3530 - val_loss: 0.3738\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3523 - val_loss: 0.3748\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3533 - val_loss: 0.3744\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3529 - val_loss: 0.3728\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3513 - val_loss: 0.3722\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3506 - val_loss: 0.3729\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3513 - val_loss: 0.3741\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3526 - val_loss: 0.3732\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3516 - val_loss: 0.3719\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3504 - val_loss: 0.3712\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3497 - val_loss: 0.3729\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.3514 - val_loss: 0.3727\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3512 - val_loss: 0.3718\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3503 - val_loss: 0.3729\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3514 - val_loss: 0.3727\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3513 - val_loss: 0.3714\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3500 - val_loss: 0.3712\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3498 - val_loss: 0.3719\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3505 - val_loss: 0.3722\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3508 - val_loss: 0.3720\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3506 - val_loss: 0.3734\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3520 - val_loss: 0.3722\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3508 - val_loss: 0.3714\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3500 - val_loss: 0.3704\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3490 - val_loss: 0.3703\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3489 - val_loss: 0.3714\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3500 - val_loss: 0.3716\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3503 - val_loss: 0.3697\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3483 - val_loss: 0.3695\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3481 - val_loss: 0.3702\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3488 - val_loss: 0.3704\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3490 - val_loss: 0.3715\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3501 - val_loss: 0.3716\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3502 - val_loss: 0.3713\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3499 - val_loss: 0.3716\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3502 - val_loss: 0.3719\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3505 - val_loss: 0.3716\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3502 - val_loss: 0.3723\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3509 - val_loss: 0.3731\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3517 - val_loss: 0.3711\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3496 - val_loss: 0.3705\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3490 - val_loss: 0.3719\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3504 - val_loss: 0.3716\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3501 - val_loss: 0.3718\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3504 - val_loss: 0.3712\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3497 - val_loss: 0.3699\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.3485 - val_loss: 0.3710\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3495 - val_loss: 0.3712\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3498 - val_loss: 0.3700\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3486 - val_loss: 0.3712\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3497 - val_loss: 0.3717\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3503 - val_loss: 0.3705\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3490 - val_loss: 0.3704\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3489 - val_loss: 0.3718\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3503 - val_loss: 0.3714\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3499 - val_loss: 0.3716\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3502 - val_loss: 0.3727\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3513 - val_loss: 0.3717\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3503 - val_loss: 0.3711\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3497 - val_loss: 0.3703\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3488 - val_loss: 0.3708\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3494 - val_loss: 0.3732\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3517 - val_loss: 0.3728\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3514 - val_loss: 0.3704\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3490 - val_loss: 0.3700\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3485 - val_loss: 0.3717\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3503 - val_loss: 0.3713\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3498 - val_loss: 0.3709\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.3494 - val_loss: 0.3711\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3496 - val_loss: 0.3703\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3488 - val_loss: 0.3712\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.3498 - val_loss: 0.3716\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.3502 - val_loss: 0.3695\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3481 - val_loss: 0.3711\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.3496 - val_loss: 0.3731\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3517 - val_loss: 0.3716\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3502 - val_loss: 0.3707\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3493 - val_loss: 0.3710\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3496 - val_loss: 0.3704\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3490 - val_loss: 0.3708\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3494 - val_loss: 0.3717\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3502 - val_loss: 0.3708\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.3494 - val_loss: 0.3712\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.3498 - val_loss: 0.3708\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.3494 - val_loss: 0.3706\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.3491 - val_loss: 0.3727\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3512 - val_loss: 0.3723\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.3509 - val_loss: 0.3708\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.3494 - val_loss: 0.3701\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.3487 - val_loss: 0.3715\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.3500 - val_loss: 0.3713\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.3499 - val_loss: 0.3715\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3501 - val_loss: 0.3715\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3500 - val_loss: 0.3706\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3492 - val_loss: 0.3713\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3499 - val_loss: 0.3707\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3493 - val_loss: 0.3704\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3489 - val_loss: 0.3721\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3507 - val_loss: 0.3715\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3501 - val_loss: 0.3695\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3480 - val_loss: 0.3696\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3481 - val_loss: 0.3715\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3501 - val_loss: 0.3713\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3499 - val_loss: 0.3710\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3496 - val_loss: 0.3715\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3500 - val_loss: 0.3711\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3496 - val_loss: 0.3714\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3499 - val_loss: 0.3712\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3498 - val_loss: 0.3701\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3487 - val_loss: 0.3715\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3501 - val_loss: 0.3734\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3520 - val_loss: 0.3721\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.3507 - val_loss: 0.3703\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3488 - val_loss: 0.3711\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3496 - val_loss: 0.3711\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3496 - val_loss: 0.3715\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.3501 - val_loss: 0.3720\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3506 - val_loss: 0.3711\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3497 - val_loss: 0.3711\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3497 - val_loss: 0.3712\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3498 - val_loss: 0.3706\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3491 - val_loss: 0.3719\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3505 - val_loss: 0.3726\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3511 - val_loss: 0.3704\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3490 - val_loss: 0.3706\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3492 - val_loss: 0.3727\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3512 - val_loss: 0.3716\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3502 - val_loss: 0.3710\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3496 - val_loss: 0.3714\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3500 - val_loss: 0.3707\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3493 - val_loss: 0.3708\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3494 - val_loss: 0.3712\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.3498 - val_loss: 0.3700\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3486 - val_loss: 0.3717\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3503 - val_loss: 0.3719\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3505 - val_loss: 0.3696\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.3482 - val_loss: 0.3701\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3487 - val_loss: 0.3714\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3500 - val_loss: 0.3714\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3500 - val_loss: 0.3722\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3507 - val_loss: 0.3725\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3511 - val_loss: 0.3713\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3499 - val_loss: 0.3711\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3497 - val_loss: 0.3709\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3495 - val_loss: 0.3702\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3488 - val_loss: 0.3723\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3508 - val_loss: 0.3735\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3520 - val_loss: 0.3714\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3499 - val_loss: 0.3698\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3484 - val_loss: 0.3709\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3495 - val_loss: 0.3716\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3501 - val_loss: 0.3721\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3506 - val_loss: 0.3717\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3503 - val_loss: 0.3706\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3492 - val_loss: 0.3710\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3496 - val_loss: 0.3713\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3499 - val_loss: 0.3700\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3486 - val_loss: 0.3716\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3502 - val_loss: 0.3717\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3503 - val_loss: 0.3709\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3495 - val_loss: 0.3714\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3500 - val_loss: 0.3726\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3511 - val_loss: 0.3712\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3498 - val_loss: 0.3707\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3493 - val_loss: 0.3720\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3506 - val_loss: 0.3715\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3501 - val_loss: 0.3715\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3501 - val_loss: 0.3709\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.3494 - val_loss: 0.3704\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3489 - val_loss: 0.3725\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3511 - val_loss: 0.3723\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3509 - val_loss: 0.3704\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3490 - val_loss: 0.3701\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3486 - val_loss: 0.3716\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.3502 - val_loss: 0.3720\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.3506 - val_loss: 0.3722\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.3508 - val_loss: 0.3720\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.3506 - val_loss: 0.3708\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.3493 - val_loss: 0.3713\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.3499 - val_loss: 0.3719\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.3505 - val_loss: 0.3708\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.3494 - val_loss: 0.3723\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3508 - val_loss: 0.3725\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3511 - val_loss: 0.3707\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3493 - val_loss: 0.3695\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3480 - val_loss: 0.3708\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3494 - val_loss: 0.3713\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3499 - val_loss: 0.3719\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3504 - val_loss: 0.3721\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3507 - val_loss: 0.3707\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3493 - val_loss: 0.3708\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3493 - val_loss: 0.3712\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3498 - val_loss: 0.3705\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3491 - val_loss: 0.3721\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3507 - val_loss: 0.3726\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3512 - val_loss: 0.3711\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3497 - val_loss: 0.3713\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3498 - val_loss: 0.3720\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3506 - val_loss: 0.3709\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3495 - val_loss: 0.3710\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3496 - val_loss: 0.3728\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3514 - val_loss: 0.3721\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3506 - val_loss: 0.3719\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3505 - val_loss: 0.3709\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3494 - val_loss: 0.3700\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3485 - val_loss: 0.3724\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3509 - val_loss: 0.3731\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3516 - val_loss: 0.3706\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3492 - val_loss: 0.3703\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3488 - val_loss: 0.3720\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3506 - val_loss: 0.3716\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3501 - val_loss: 0.3717\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3502 - val_loss: 0.3711\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3497 - val_loss: 0.3701\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3487 - val_loss: 0.3716\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3502 - val_loss: 0.3722\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3508 - val_loss: 0.3709\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3495 - val_loss: 0.3716\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3502 - val_loss: 0.3726\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3512 - val_loss: 0.3708\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3494 - val_loss: 0.3707\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3492 - val_loss: 0.3718\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3503 - val_loss: 0.3713\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3499 - val_loss: 0.3715\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3501 - val_loss: 0.3721\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3507 - val_loss: 0.3708\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3494 - val_loss: 0.3713\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3499 - val_loss: 0.3711\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3497 - val_loss: 0.3706\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3491 - val_loss: 0.3729\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3514 - val_loss: 0.3733\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3519 - val_loss: 0.3717\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3502 - val_loss: 0.3710\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3495 - val_loss: 0.3720\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3506 - val_loss: 0.3714\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3500 - val_loss: 0.3713\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3499 - val_loss: 0.3724\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3510 - val_loss: 0.3713\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3499 - val_loss: 0.3708\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3494 - val_loss: 0.3703\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3489 - val_loss: 0.3701\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3487 - val_loss: 0.3723\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3508 - val_loss: 0.3723\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3508 - val_loss: 0.3705\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3490 - val_loss: 0.3708\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3494 - val_loss: 0.3722\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3507 - val_loss: 0.3717\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3503 - val_loss: 0.3713\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3499 - val_loss: 0.3715\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.3501 - val_loss: 0.3710\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3495 - val_loss: 0.3720\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3506 - val_loss: 0.3723\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3509 - val_loss: 0.3708\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3494 - val_loss: 0.3715\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3500 - val_loss: 0.3726\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3512 - val_loss: 0.3716\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3501 - val_loss: 0.3710\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3496 - val_loss: 0.3718\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3503 - val_loss: 0.3719\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3505 - val_loss: 0.3723\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3509 - val_loss: 0.3718\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3504 - val_loss: 0.3705\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3491 - val_loss: 0.3710\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3496 - val_loss: 0.3711\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3497 - val_loss: 0.3706\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3492 - val_loss: 0.3724\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3510 - val_loss: 0.3728\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3513 - val_loss: 0.3708\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3493 - val_loss: 0.3702\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3488 - val_loss: 0.3718\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.3504 - val_loss: 0.3717\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.3502 - val_loss: 0.3718\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.3504 - val_loss: 0.3725\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.3511 - val_loss: 0.3711\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.3497 - val_loss: 0.3709\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.3495 - val_loss: 0.3704\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.3490 - val_loss: 0.3706\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.3491 - val_loss: 0.3724\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.3509 - val_loss: 0.3724\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.3510 - val_loss: 0.3706\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3491 - val_loss: 0.3711\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3497 - val_loss: 0.3725\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3511 - val_loss: 0.3713\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3499 - val_loss: 0.3716\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3501 - val_loss: 0.3723\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3508 - val_loss: 0.3718\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3503 - val_loss: 0.3726\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3512 - val_loss: 0.3722\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3507 - val_loss: 0.3699\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3485 - val_loss: 0.3715\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3500 - val_loss: 0.3732\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3517 - val_loss: 0.3717\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3502 - val_loss: 0.3707\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3493 - val_loss: 0.3713\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3498 - val_loss: 0.3712\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3497 - val_loss: 0.3720\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3506 - val_loss: 0.3719\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3504 - val_loss: 0.3700\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3486 - val_loss: 0.3706\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3491 - val_loss: 0.3715\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3500 - val_loss: 0.3710\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3496 - val_loss: 0.3726\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3511 - val_loss: 0.3722\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3507 - val_loss: 0.3700\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3486 - val_loss: 0.3709\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3494 - val_loss: 0.3725\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3511 - val_loss: 0.3722\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3508 - val_loss: 0.3716\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3502 - val_loss: 0.3724\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3510 - val_loss: 0.3716\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3501 - val_loss: 0.3714\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3500 - val_loss: 0.3710\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3495 - val_loss: 0.3705\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3491 - val_loss: 0.3727\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3513 - val_loss: 0.3730\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3516 - val_loss: 0.3709\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3495 - val_loss: 0.3706\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3491 - val_loss: 0.3716\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3501 - val_loss: 0.3717\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3503 - val_loss: 0.3721\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3507 - val_loss: 0.3722\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3507 - val_loss: 0.3715\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3501 - val_loss: 0.3718\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3504 - val_loss: 0.3715\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3501 - val_loss: 0.3699\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3485 - val_loss: 0.3716\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3501 - val_loss: 0.3729\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3514 - val_loss: 0.3714\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3500 - val_loss: 0.3705\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3491 - val_loss: 0.3711\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3497 - val_loss: 0.3713\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3499 - val_loss: 0.3717\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3503 - val_loss: 0.3719\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3505 - val_loss: 0.3706\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3492 - val_loss: 0.3714\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3500 - val_loss: 0.3719\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3505 - val_loss: 0.3715\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3501 - val_loss: 0.3726\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3512 - val_loss: 0.3719\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3505 - val_loss: 0.3708\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3493 - val_loss: 0.3714\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3499 - val_loss: 0.3731\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3517 - val_loss: 0.3720\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3506 - val_loss: 0.3712\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3498 - val_loss: 0.3722\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3508 - val_loss: 0.3717\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3502 - val_loss: 0.3718\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3504 - val_loss: 0.3708\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.3494 - val_loss: 0.3703\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3489 - val_loss: 0.3727\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3512 - val_loss: 0.3726\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.3512 - val_loss: 0.3706\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3492 - val_loss: 0.3698\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3484 - val_loss: 0.3712\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3498 - val_loss: 0.3714\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3500 - val_loss: 0.3721\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3507 - val_loss: 0.3725\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3510 - val_loss: 0.3709\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3494 - val_loss: 0.3715\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3500 - val_loss: 0.3715\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3501 - val_loss: 0.3707\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.3493 - val_loss: 0.3724\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.3510 - val_loss: 0.3733\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.3519 - val_loss: 0.3714\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.3500 - val_loss: 0.3708\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.3494 - val_loss: 0.3716\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.3501 - val_loss: 0.3713\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.3499 - val_loss: 0.3717\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.3503 - val_loss: 0.3719\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.3505 - val_loss: 0.3709\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.3494 - val_loss: 0.3722\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.3507 - val_loss: 0.3723\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3508 - val_loss: 0.3705\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3491 - val_loss: 0.3721\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3507 - val_loss: 0.3728\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3514 - val_loss: 0.3713\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3498 - val_loss: 0.3713\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.3499 - val_loss: 0.3724\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3510 - val_loss: 0.3711\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3497 - val_loss: 0.3711\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3496 - val_loss: 0.3720\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3506 - val_loss: 0.3712\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3498 - val_loss: 0.3711\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3496 - val_loss: 0.3705\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3491 - val_loss: 0.3707\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3493 - val_loss: 0.3729\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.3515 - val_loss: 0.3727\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3512 - val_loss: 0.3701\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.3487 - val_loss: 0.3704\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3490 - val_loss: 0.3722\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3507 - val_loss: 0.3721\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3507 - val_loss: 0.3724\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3510 - val_loss: 0.3721\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3507 - val_loss: 0.3708\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3493 - val_loss: 0.3715\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3500 - val_loss: 0.3718\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3504 - val_loss: 0.3712\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3498 - val_loss: 0.3725\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3511 - val_loss: 0.3733\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3519 - val_loss: 0.3718\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3503 - val_loss: 0.3708\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3493 - val_loss: 0.3714\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3500 - val_loss: 0.3715\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3500 - val_loss: 0.3718\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3504 - val_loss: 0.3719\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3504 - val_loss: 0.3708\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3494 - val_loss: 0.3714\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3500 - val_loss: 0.3711\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.3496 - val_loss: 0.3700\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3486 - val_loss: 0.3720\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3505 - val_loss: 0.3727\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3512 - val_loss: 0.3715\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3501 - val_loss: 0.3712\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3498 - val_loss: 0.3722\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3508 - val_loss: 0.3716\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3501 - val_loss: 0.3712\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3498 - val_loss: 0.3726\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3512 - val_loss: 0.3715\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3501 - val_loss: 0.3712\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3498 - val_loss: 0.3708\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3494 - val_loss: 0.3709\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3494 - val_loss: 0.3729\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3514 - val_loss: 0.3724\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3510 - val_loss: 0.3703\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3488 - val_loss: 0.3707\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3492 - val_loss: 0.3727\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3513 - val_loss: 0.3729\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3514 - val_loss: 0.3723\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3509 - val_loss: 0.3715\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3500 - val_loss: 0.3708\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3493 - val_loss: 0.3720\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3506 - val_loss: 0.3722\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3507 - val_loss: 0.3705\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3491 - val_loss: 0.3717\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3502 - val_loss: 0.3729\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3515 - val_loss: 0.3713\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3499 - val_loss: 0.3705\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3490 - val_loss: 0.3709\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3495 - val_loss: 0.3709\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3495 - val_loss: 0.3722\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3508 - val_loss: 0.3727\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3512 - val_loss: 0.3710\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3495 - val_loss: 0.3709\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3495 - val_loss: 0.3708\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3493 - val_loss: 0.3706\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3492 - val_loss: 0.3728\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3514 - val_loss: 0.3731\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3516 - val_loss: 0.3709\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3495 - val_loss: 0.3712\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3498 - val_loss: 0.3726\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3512 - val_loss: 0.3716\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3501 - val_loss: 0.3716\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.3502 - val_loss: 0.3728\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.3513 - val_loss: 0.3718\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.3503 - val_loss: 0.3719\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.3504 - val_loss: 0.3713\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.3498 - val_loss: 0.3704\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.3490 - val_loss: 0.3723\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.3508 - val_loss: 0.3725\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.3510 - val_loss: 0.3704\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.3490 - val_loss: 0.3707\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3493 - val_loss: 0.3723\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3509 - val_loss: 0.3718\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3503 - val_loss: 0.3717\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3503 - val_loss: 0.3714\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3500 - val_loss: 0.3708\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3494 - val_loss: 0.3721\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3507 - val_loss: 0.3720\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3506 - val_loss: 0.3705\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.3491 - val_loss: 0.3719\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3505 - val_loss: 0.3729\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3515 - val_loss: 0.3712\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3498 - val_loss: 0.3704\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3490 - val_loss: 0.3710\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3496 - val_loss: 0.3718\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3503 - val_loss: 0.3726\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3512 - val_loss: 0.3727\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3512 - val_loss: 0.3707\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.3492 - val_loss: 0.3709\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3495 - val_loss: 0.3714\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3499 - val_loss: 0.3712\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3497 - val_loss: 0.3734\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3519 - val_loss: 0.3730\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3516 - val_loss: 0.3709\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3495 - val_loss: 0.3708\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3493 - val_loss: 0.3723\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3508 - val_loss: 0.3721\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.3506 - val_loss: 0.3713\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3499 - val_loss: 0.3723\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3509 - val_loss: 0.3717\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3503 - val_loss: 0.3715\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3500 - val_loss: 0.3709\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3495 - val_loss: 0.3701\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3486 - val_loss: 0.3720\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3506 - val_loss: 0.3723\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3509 - val_loss: 0.3708\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3494 - val_loss: 0.3709\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3495 - val_loss: 0.3719\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3504 - val_loss: 0.3713\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3499 - val_loss: 0.3714\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3500 - val_loss: 0.3723\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3509 - val_loss: 0.3716\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3502 - val_loss: 0.3720\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3506 - val_loss: 0.3719\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3504 - val_loss: 0.3711\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3496 - val_loss: 0.3723\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3509 - val_loss: 0.3730\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3516 - val_loss: 0.3712\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3498 - val_loss: 0.3704\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3490 - val_loss: 0.3717\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3502 - val_loss: 0.3722\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3508 - val_loss: 0.3726\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3511 - val_loss: 0.3718\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3504 - val_loss: 0.3700\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3486 - val_loss: 0.3715\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3501 - val_loss: 0.3719\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3504 - val_loss: 0.3712\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3497 - val_loss: 0.3727\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3513 - val_loss: 0.3725\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3510 - val_loss: 0.3708\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3494 - val_loss: 0.3707\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3492 - val_loss: 0.3723\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3508 - val_loss: 0.3712\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3498 - val_loss: 0.3712\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3498 - val_loss: 0.3726\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3511 - val_loss: 0.3716\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.3501 - val_loss: 0.3716\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.3502 - val_loss: 0.3704\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3490 - val_loss: 0.3702\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3488 - val_loss: 0.3728\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3514 - val_loss: 0.3732\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3518 - val_loss: 0.3711\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3496 - val_loss: 0.3707\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3493 - val_loss: 0.3719\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.3504 - val_loss: 0.3714\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3500 - val_loss: 0.3720\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3506 - val_loss: 0.3725\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3511 - val_loss: 0.3713\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3499 - val_loss: 0.3722\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3507 - val_loss: 0.3721\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3506 - val_loss: 0.3707\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3493 - val_loss: 0.3722\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.3508 - val_loss: 0.3731\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.3517 - val_loss: 0.3713\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.3499 - val_loss: 0.3708\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.3494 - val_loss: 0.3717\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3503 - val_loss: 0.3716\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.3502 - val_loss: 0.3718\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.3504 - val_loss: 0.3712\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.3497 - val_loss: 0.3700\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.3486 - val_loss: 0.3712\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.3498 - val_loss: 0.3718\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.3503 - val_loss: 0.3711\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3497 - val_loss: 0.3725\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3511 - val_loss: 0.3724\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3510 - val_loss: 0.3708\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3494 - val_loss: 0.3715\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3501 - val_loss: 0.3726\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3511 - val_loss: 0.3715\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3501 - val_loss: 0.3716\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.3501 - val_loss: 0.3728\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3514 - val_loss: 0.3718\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.3504 - val_loss: 0.3710\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3496 - val_loss: 0.3702\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3488 - val_loss: 0.3706\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3492 - val_loss: 0.3732\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3518 - val_loss: 0.3737\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3522 - val_loss: 0.3709\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3495 - val_loss: 0.3700\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3486 - val_loss: 0.3719\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3504 - val_loss: 0.3724\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3510 - val_loss: 0.3725\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3511 - val_loss: 0.3719\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3505 - val_loss: 0.3709\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3494 - val_loss: 0.3716\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3502 - val_loss: 0.3718\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3504 - val_loss: 0.3704\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3489 - val_loss: 0.3713\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3499 - val_loss: 0.3727\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3513 - val_loss: 0.3717\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3502 - val_loss: 0.3710\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.3496 - val_loss: 0.3717\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3502 - val_loss: 0.3714\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3500 - val_loss: 0.3715\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3500 - val_loss: 0.3719\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3505 - val_loss: 0.3709\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3495 - val_loss: 0.3717\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.3502 - val_loss: 0.3715\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.3501 - val_loss: 0.3708\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.3494 - val_loss: 0.3725\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3511 - val_loss: 0.3726\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3511 - val_loss: 0.3710\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3496 - val_loss: 0.3713\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.3499 - val_loss: 0.3728\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.3514 - val_loss: 0.3722\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.3508 - val_loss: 0.3720\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.3506 - val_loss: 0.3728\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.3513 - val_loss: 0.3713\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.3499 - val_loss: 0.3712\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.3498 - val_loss: 0.3708\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.3493 - val_loss: 0.3706\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.3492 - val_loss: 0.3730\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3516 - val_loss: 0.3725\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3510 - val_loss: 0.3699\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3485 - val_loss: 0.3701\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3487 - val_loss: 0.3719\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3505 - val_loss: 0.3719\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3505 - val_loss: 0.3722\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3507 - val_loss: 0.3722\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.3507 - val_loss: 0.3711\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3497 - val_loss: 0.3718\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.3504 - val_loss: 0.3718\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3503 - val_loss: 0.3704\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3490 - val_loss: 0.3718\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3504 - val_loss: 0.3732\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3517 - val_loss: 0.3717\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3502 - val_loss: 0.3712\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3498 - val_loss: 0.3713\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3498 - val_loss: 0.3711\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3496 - val_loss: 0.3721\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3506 - val_loss: 0.3725\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3510 - val_loss: 0.3715\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3500 - val_loss: 0.3720\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3506 - val_loss: 0.3717\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.3502 - val_loss: 0.3705\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.3491 - val_loss: 0.3726\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.3511 - val_loss: 0.3730\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.3516 - val_loss: 0.3709\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.3495 - val_loss: 0.3708\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.3494 - val_loss: 0.3724\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.3509 - val_loss: 0.3719\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.3505 - val_loss: 0.3717\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.3503 - val_loss: 0.3719\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3505 - val_loss: 0.3709\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3494 - val_loss: 0.3713\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3499 - val_loss: 0.3711\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3497 - val_loss: 0.3710\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3495 - val_loss: 0.3726\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3512 - val_loss: 0.3721\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.3507 - val_loss: 0.3700\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3485 - val_loss: 0.3706\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3492 - val_loss: 0.3723\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3509 - val_loss: 0.3720\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3506 - val_loss: 0.3719\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.3504 - val_loss: 0.3724\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3510 - val_loss: 0.3715\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.3501 - val_loss: 0.3719\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3505 - val_loss: 0.3718\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3504 - val_loss: 0.3708\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3494 - val_loss: 0.3725\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3511 - val_loss: 0.3738\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3524 - val_loss: 0.3721\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3506 - val_loss: 0.3702\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3488 - val_loss: 0.3708\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3493 - val_loss: 0.3716\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3501 - val_loss: 0.3722\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3508 - val_loss: 0.3726\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3511 - val_loss: 0.3708\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3494 - val_loss: 0.3710\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3495 - val_loss: 0.3713\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3499 - val_loss: 0.3707\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.3493 - val_loss: 0.3725\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3511 - val_loss: 0.3723\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3509 - val_loss: 0.3708\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3494 - val_loss: 0.3711\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3497 - val_loss: 0.3726\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3511 - val_loss: 0.3717\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3503 - val_loss: 0.3708\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3494 - val_loss: 0.3722\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3507 - val_loss: 0.3715\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3501 - val_loss: 0.3719\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3505 - val_loss: 0.3714\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3500 - val_loss: 0.3707\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3493 - val_loss: 0.3726\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.3512 - val_loss: 0.3726\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3512 - val_loss: 0.3707\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3493 - val_loss: 0.3710\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.3495 - val_loss: 0.3724\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3510 - val_loss: 0.3721\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3506 - val_loss: 0.3723\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3508 - val_loss: 0.3720\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3506 - val_loss: 0.3710\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3496 - val_loss: 0.3721\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3507 - val_loss: 0.3719\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3504 - val_loss: 0.3707\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3492 - val_loss: 0.3724\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3509 - val_loss: 0.3735\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3520 - val_loss: 0.3713\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3498 - val_loss: 0.3698\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3484 - val_loss: 0.3708\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3494 - val_loss: 0.3713\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3499 - val_loss: 0.3725\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3510 - val_loss: 0.3722\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3508 - val_loss: 0.3702\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3488 - val_loss: 0.3711\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3496 - val_loss: 0.3712\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3497 - val_loss: 0.3712\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3498 - val_loss: 0.3728\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3514 - val_loss: 0.3726\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3512 - val_loss: 0.3712\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3498 - val_loss: 0.3718\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3504 - val_loss: 0.3730\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3515 - val_loss: 0.3713\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3498 - val_loss: 0.3710\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.3496 - val_loss: 0.3725\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3511 - val_loss: 0.3721\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3507 - val_loss: 0.3723\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3508 - val_loss: 0.3709\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.3495 - val_loss: 0.3702\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.3487 - val_loss: 0.3726\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.3512 - val_loss: 0.3731\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.3516 - val_loss: 0.3710\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3495 - val_loss: 0.3707\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.3492 - val_loss: 0.3720\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.3505 - val_loss: 0.3717\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.3502 - val_loss: 0.3721\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.3506 - val_loss: 0.3717\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.3503 - val_loss: 0.3705\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3490 - val_loss: 0.3713\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3499 - val_loss: 0.3719\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3504 - val_loss: 0.3711\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3497 - val_loss: 0.3722\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.3508 - val_loss: 0.3729\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.3515 - val_loss: 0.3709\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3495 - val_loss: 0.3705\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3491 - val_loss: 0.3717\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.3503 - val_loss: 0.3722\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.3508 - val_loss: 0.3723\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3509 - val_loss: 0.3721\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3506 - val_loss: 0.3707\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3492 - val_loss: 0.3711\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3497 - val_loss: 0.3716\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3502 - val_loss: 0.3710\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3495 - val_loss: 0.3727\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3513 - val_loss: 0.3734\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3519 - val_loss: 0.3717\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3502 - val_loss: 0.3712\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3498 - val_loss: 0.3722\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3508 - val_loss: 0.3717\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3503 - val_loss: 0.3716\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3501 - val_loss: 0.3727\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3512 - val_loss: 0.3720\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3505 - val_loss: 0.3711\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3497 - val_loss: 0.3703\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3488 - val_loss: 0.3699\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3484 - val_loss: 0.3723\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3508 - val_loss: 0.3728\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3514 - val_loss: 0.3705\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3491 - val_loss: 0.3706\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3492 - val_loss: 0.3721\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.3507 - val_loss: 0.3718\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3504 - val_loss: 0.3720\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3506 - val_loss: 0.3722\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3507 - val_loss: 0.3710\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3496 - val_loss: 0.3720\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3505 - val_loss: 0.3723\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3508 - val_loss: 0.3712\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3497 - val_loss: 0.3720\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3505 - val_loss: 0.3725\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3511 - val_loss: 0.3711\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3497 - val_loss: 0.3712\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3497 - val_loss: 0.3723\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3508 - val_loss: 0.3721\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3506 - val_loss: 0.3722\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3508 - val_loss: 0.3720\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3506 - val_loss: 0.3707\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3492 - val_loss: 0.3717\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3503 - val_loss: 0.3715\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3501 - val_loss: 0.3705\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3491 - val_loss: 0.3726\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3512 - val_loss: 0.3727\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.3513 - val_loss: 0.3710\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3495 - val_loss: 0.3705\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3491 - val_loss: 0.3716\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3502 - val_loss: 0.3715\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3501 - val_loss: 0.3719\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3505 - val_loss: 0.3730\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3516 - val_loss: 0.3716\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3501 - val_loss: 0.3712\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3497 - val_loss: 0.3704\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3490 - val_loss: 0.3706\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3492 - val_loss: 0.3731\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3517 - val_loss: 0.3727\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3513 - val_loss: 0.3704\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3489 - val_loss: 0.3707\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3493 - val_loss: 0.3722\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3508 - val_loss: 0.3721\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3506 - val_loss: 0.3720\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3506 - val_loss: 0.3720\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3506 - val_loss: 0.3716\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3501 - val_loss: 0.3728\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3514 - val_loss: 0.3726\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3512 - val_loss: 0.3704\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3489 - val_loss: 0.3716\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3501 - val_loss: 0.3731\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3517 - val_loss: 0.3716\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3502 - val_loss: 0.3709\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.3495 - val_loss: 0.3712\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.3498 - val_loss: 0.3712\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.3497 - val_loss: 0.3718\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.3504 - val_loss: 0.3719\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.3504 - val_loss: 0.3706\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.3492 - val_loss: 0.3710\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.3496 - val_loss: 0.3715\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.3500 - val_loss: 0.3713\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.3498 - val_loss: 0.3729\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.3515 - val_loss: 0.3727\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.3513 - val_loss: 0.3705\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.3491 - val_loss: 0.3707\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.3493 - val_loss: 0.3725\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.3510 - val_loss: 0.3720\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.3506 - val_loss: 0.3718\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.3504 - val_loss: 0.3729\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.3514 - val_loss: 0.3716\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.3501 - val_loss: 0.3711\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.3497 - val_loss: 0.3709\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3495 - val_loss: 0.3712\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3498 - val_loss: 0.3732\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3517 - val_loss: 0.3730\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3516 - val_loss: 0.3708\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3494 - val_loss: 0.3704\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3490 - val_loss: 0.3721\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.3506 - val_loss: 0.3719\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3505 - val_loss: 0.3717\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3503 - val_loss: 0.3721\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3506 - val_loss: 0.3713\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3499 - val_loss: 0.3722\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3508 - val_loss: 0.3718\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3504 - val_loss: 0.3699\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3484 - val_loss: 0.3716\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3502 - val_loss: 0.3733\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3518 - val_loss: 0.3720\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3505 - val_loss: 0.3707\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3493 - val_loss: 0.3711\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.3496 - val_loss: 0.3712\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.3497 - val_loss: 0.3718\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3504 - val_loss: 0.3723\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3509 - val_loss: 0.3706\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3491 - val_loss: 0.3714\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3500 - val_loss: 0.3718\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3504 - val_loss: 0.3714\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3499 - val_loss: 0.3731\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3517 - val_loss: 0.3725\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3511 - val_loss: 0.3707\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3492 - val_loss: 0.3712\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3498 - val_loss: 0.3731\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3517 - val_loss: 0.3725\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.3510 - val_loss: 0.3717\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3502 - val_loss: 0.3720\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.3505 - val_loss: 0.3712\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3497 - val_loss: 0.3718\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3503 - val_loss: 0.3711\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3496 - val_loss: 0.3706\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.3491 - val_loss: 0.3727\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.3512 - val_loss: 0.3726\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.3511 - val_loss: 0.3706\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.3492 - val_loss: 0.3705\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3491 - val_loss: 0.3716\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3502 - val_loss: 0.3713\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3499 - val_loss: 0.3721\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3507 - val_loss: 0.3724\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3509 - val_loss: 0.3713\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3499 - val_loss: 0.3716\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3501 - val_loss: 0.3711\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3497 - val_loss: 0.3707\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.3493 - val_loss: 0.3725\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.3511 - val_loss: 0.3736\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.3522 - val_loss: 0.3716\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3502 - val_loss: 0.3709\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3494 - val_loss: 0.3715\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3500 - val_loss: 0.3714\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3500 - val_loss: 0.3723\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3509 - val_loss: 0.3721\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3507 - val_loss: 0.3708\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3493 - val_loss: 0.3718\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.3504 - val_loss: 0.3719\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.3505 - val_loss: 0.3710\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.3495 - val_loss: 0.3723\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3508 - val_loss: 0.3727\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3512 - val_loss: 0.3712\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3497 - val_loss: 0.3713\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3499 - val_loss: 0.3730\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3515 - val_loss: 0.3717\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.3503 - val_loss: 0.3711\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.3497 - val_loss: 0.3719\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.3504 - val_loss: 0.3713\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.3498 - val_loss: 0.3714\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.3500 - val_loss: 0.3708\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.3493 - val_loss: 0.3704\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.3490 - val_loss: 0.3725\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3511 - val_loss: 0.3728\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.3514 - val_loss: 0.3706\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.3492 - val_loss: 0.3705\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.3490 - val_loss: 0.3720\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.3506 - val_loss: 0.3722\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3508 - val_loss: 0.3725\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.3511 - val_loss: 0.3726\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3512 - val_loss: 0.3711\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3497 - val_loss: 0.3712\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3498 - val_loss: 0.3717\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3503 - val_loss: 0.3710\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3496 - val_loss: 0.3727\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.3513 - val_loss: 0.3736\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3522 - val_loss: 0.3714\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3500 - val_loss: 0.3705\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.3491 - val_loss: 0.3715\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3500 - val_loss: 0.3720\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3505 - val_loss: 0.3722\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3508 - val_loss: 0.3721\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.3506 - val_loss: 0.3708\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3494 - val_loss: 0.3714\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3499 - val_loss: 0.3716\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3502 - val_loss: 0.3702\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3487 - val_loss: 0.3718\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3504 - val_loss: 0.3724\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3509 - val_loss: 0.3712\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3497 - val_loss: 0.3718\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.3503 - val_loss: 0.3726\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.3512 - val_loss: 0.3712\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3498 - val_loss: 0.3711\n"
          ]
        }
      ],
      "source": [
        "h_gru_7 = model_GRU_7.fit(X_TRAIN,Y_TRAIN,\n",
        "          epochs=1000,\n",
        "          batch_size=X_TRAIN.shape[0],\n",
        "          validation_data = (X_VALID, Y_VALID),\n",
        "          verbose = True\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MT3eJAawoiDp",
        "outputId": "1c859b1e-75a0-4b7b-a368-758f21a980a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b12348b9330>]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h_gru_7.history[\"loss\"])\n",
        "plt.plot(h_gru_7.history[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxaJSS_hpnly"
      },
      "source": [
        "# NUEVOS INPUTS PARA USAR DENSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf-oIr86p0UZ"
      },
      "source": [
        "#DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MkmVRfe-AU9w"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XJO6qMJj_0eF",
        "outputId": "5dbdbd6f-be09-4a50-996d-08b3c038f38d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "def descargar_datos_yahoo(ticker, start_date, end_date):\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    return data[\"Adj Close\"]\n",
        "\n",
        "# Ticker del IBEX 35 en Yahoo Finance\n",
        "ticker = \"^IBEX\"\n",
        "\n",
        "# Fechas de inicio y fin\n",
        "start_date = \"2016-01-01\"\n",
        "end_date = \"2023-01-01\"\n",
        "\n",
        "# Descarga de datos\n",
        "bench = descargar_datos_yahoo(ticker, start_date, end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HGUW397W_4UX",
        "outputId": "b86affa4-f425-47f8-fa62-b4bb3ac7209d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bench.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a4me8Fw-_9sE"
      },
      "outputs": [],
      "source": [
        "bench.name = \"Bench\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "czcUAoQnHMMZ",
        "outputId": "8ce38844-b710-459e-fc3b-d7f5907bc6a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "2016-01-04    9313.190430\n",
              "2016-01-05    9335.190430\n",
              "2016-01-06    9197.390625\n",
              "2016-01-07    9059.291016\n",
              "2016-01-08    8909.191406\n",
              "                 ...     \n",
              "2022-12-23    8269.099609\n",
              "2022-12-27    8270.099609\n",
              "2022-12-28    8258.500000\n",
              "2022-12-29    8318.299805\n",
              "2022-12-30    8229.099609\n",
              "Name: Bench, Length: 1791, dtype: float64"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z9Nt1yUj5DFK"
      },
      "outputs": [],
      "source": [
        "start_date = \"2016-01-01\"\n",
        "end_date = \"2023-01-01\"\n",
        "\n",
        "tickers = ['BKT', 'IBE', 'REP', 'TEF', 'ACS', 'FER', 'IDR', 'BBVA', 'ITX', 'SAN',\n",
        "       'ENG', 'SAB', 'MAP', 'GRF', 'MTS', 'AMS', 'IAG', 'CABK', 'SCYR', 'ELE',\n",
        "       'ACX', 'AENA', 'ANA', 'MRL', 'CLNX', 'MEL', 'COL', 'NTGY', 'SLR', 'FDR',\n",
        "       'ROVI', 'RED', 'ANE', 'LOG', 'UNI']\n",
        "\n",
        "tickers = [ticker + \".MC\" for ticker in tickers]\n",
        "#tickers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gpdoTCjL5J5P",
        "outputId": "ad4ae6a4-57d5-4921-c58b-7eebafee11ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  35 of 35 completed\n"
          ]
        }
      ],
      "source": [
        "data = yf.download(tickers, start=start_date, end=end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mHOi2kVl5Keh",
        "outputId": "a5e3ecb6-1daa-4356-b3bd-08f378653d16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ACS.MC        0\n",
              "ACX.MC        0\n",
              "AENA.MC       0\n",
              "AMS.MC        0\n",
              "ANA.MC        0\n",
              "ANE.MC     1406\n",
              "BBVA.MC       0\n",
              "BKT.MC        0\n",
              "CABK.MC       0\n",
              "CLNX.MC       0\n",
              "COL.MC        0\n",
              "ELE.MC        0\n",
              "ENG.MC        0\n",
              "FDR.MC        0\n",
              "FER.MC        0\n",
              "GRF.MC        0\n",
              "IAG.MC        0\n",
              "IBE.MC        0\n",
              "IDR.MC        0\n",
              "ITX.MC        0\n",
              "LOG.MC        0\n",
              "MAP.MC        0\n",
              "MEL.MC        0\n",
              "MRL.MC        0\n",
              "MTS.MC        0\n",
              "NTGY.MC       0\n",
              "RED.MC        0\n",
              "REP.MC        0\n",
              "ROVI.MC       0\n",
              "SAB.MC        0\n",
              "SAN.MC        0\n",
              "SCYR.MC       0\n",
              "SLR.MC        0\n",
              "TEF.MC        0\n",
              "UNI.MC      383\n",
              "dtype: int64"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"Adj Close\"].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RWlHyQV85yVH"
      },
      "outputs": [],
      "source": [
        "data = data[\"Adj Close\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LCks4u9x5Y7_"
      },
      "outputs": [],
      "source": [
        "data.drop([\"ANE.MC\"],axis = 1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f0eFHFbK53bp",
        "outputId": "14292742-d3d1-4fd2-9338-ae7bd8fb6c2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "383"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "itGKxeY_5-G0"
      },
      "outputs": [],
      "source": [
        "data = data.fillna(method=\"ffill\")\n",
        "data = data.fillna(method=\"bfill\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TlYzxcJL6UrF",
        "outputId": "81418a1e-1128-45c6-8ff0-8cb289e13370"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zSgqQhbwpr9x"
      },
      "outputs": [],
      "source": [
        "ibex_data = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SxK2Va-gAZ_l",
        "outputId": "fc84564c-1b5b-4c90-f6f9-526f1a46adef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-3e1469b3-c40e-4bc4-bdec-56708cdbf810\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ACS.MC</th>\n",
              "      <th>ACX.MC</th>\n",
              "      <th>AENA.MC</th>\n",
              "      <th>AMS.MC</th>\n",
              "      <th>ANA.MC</th>\n",
              "      <th>BBVA.MC</th>\n",
              "      <th>BKT.MC</th>\n",
              "      <th>CABK.MC</th>\n",
              "      <th>CLNX.MC</th>\n",
              "      <th>COL.MC</th>\n",
              "      <th>...</th>\n",
              "      <th>NTGY.MC</th>\n",
              "      <th>RED.MC</th>\n",
              "      <th>REP.MC</th>\n",
              "      <th>ROVI.MC</th>\n",
              "      <th>SAB.MC</th>\n",
              "      <th>SAN.MC</th>\n",
              "      <th>SCYR.MC</th>\n",
              "      <th>SLR.MC</th>\n",
              "      <th>TEF.MC</th>\n",
              "      <th>UNI.MC</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-01-04</th>\n",
              "      <td>15.980104</td>\n",
              "      <td>5.751740</td>\n",
              "      <td>79.277679</td>\n",
              "      <td>36.199356</td>\n",
              "      <td>60.231125</td>\n",
              "      <td>4.407446</td>\n",
              "      <td>4.751968</td>\n",
              "      <td>2.298497</td>\n",
              "      <td>14.772966</td>\n",
              "      <td>5.183412</td>\n",
              "      <td>...</td>\n",
              "      <td>10.938681</td>\n",
              "      <td>11.855685</td>\n",
              "      <td>6.404625</td>\n",
              "      <td>12.837639</td>\n",
              "      <td>1.201982</td>\n",
              "      <td>3.033155</td>\n",
              "      <td>1.313086</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>5.248940</td>\n",
              "      <td>1.019846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-05</th>\n",
              "      <td>16.011051</td>\n",
              "      <td>5.890941</td>\n",
              "      <td>79.734390</td>\n",
              "      <td>36.367851</td>\n",
              "      <td>60.508472</td>\n",
              "      <td>4.436829</td>\n",
              "      <td>4.754172</td>\n",
              "      <td>2.279122</td>\n",
              "      <td>14.747093</td>\n",
              "      <td>5.223532</td>\n",
              "      <td>...</td>\n",
              "      <td>11.072264</td>\n",
              "      <td>11.923389</td>\n",
              "      <td>6.347962</td>\n",
              "      <td>12.837639</td>\n",
              "      <td>1.198304</td>\n",
              "      <td>3.007060</td>\n",
              "      <td>1.333093</td>\n",
              "      <td>0.745000</td>\n",
              "      <td>5.275911</td>\n",
              "      <td>1.019846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-06</th>\n",
              "      <td>15.887266</td>\n",
              "      <td>5.630723</td>\n",
              "      <td>79.315742</td>\n",
              "      <td>36.458935</td>\n",
              "      <td>60.346691</td>\n",
              "      <td>4.342002</td>\n",
              "      <td>4.649078</td>\n",
              "      <td>2.198032</td>\n",
              "      <td>14.574611</td>\n",
              "      <td>5.055030</td>\n",
              "      <td>...</td>\n",
              "      <td>10.989146</td>\n",
              "      <td>11.899774</td>\n",
              "      <td>6.166520</td>\n",
              "      <td>12.846617</td>\n",
              "      <td>1.186534</td>\n",
              "      <td>2.910236</td>\n",
              "      <td>1.284186</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>5.237833</td>\n",
              "      <td>1.019846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-07</th>\n",
              "      <td>15.447847</td>\n",
              "      <td>5.364235</td>\n",
              "      <td>77.907539</td>\n",
              "      <td>35.552696</td>\n",
              "      <td>58.127884</td>\n",
              "      <td>4.286575</td>\n",
              "      <td>4.581467</td>\n",
              "      <td>2.142059</td>\n",
              "      <td>14.600484</td>\n",
              "      <td>4.894553</td>\n",
              "      <td>...</td>\n",
              "      <td>10.891187</td>\n",
              "      <td>11.828919</td>\n",
              "      <td>5.922048</td>\n",
              "      <td>12.765821</td>\n",
              "      <td>1.158581</td>\n",
              "      <td>2.850492</td>\n",
              "      <td>1.210084</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>5.197640</td>\n",
              "      <td>1.019846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-08</th>\n",
              "      <td>15.206474</td>\n",
              "      <td>5.406247</td>\n",
              "      <td>78.630669</td>\n",
              "      <td>35.038105</td>\n",
              "      <td>58.089371</td>\n",
              "      <td>4.189745</td>\n",
              "      <td>4.524878</td>\n",
              "      <td>2.115508</td>\n",
              "      <td>14.238276</td>\n",
              "      <td>4.886529</td>\n",
              "      <td>...</td>\n",
              "      <td>10.768416</td>\n",
              "      <td>11.776964</td>\n",
              "      <td>5.684580</td>\n",
              "      <td>13.017186</td>\n",
              "      <td>1.120330</td>\n",
              "      <td>2.773582</td>\n",
              "      <td>1.202674</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>5.072830</td>\n",
              "      <td>1.019846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-23</th>\n",
              "      <td>25.243809</td>\n",
              "      <td>8.771631</td>\n",
              "      <td>116.368965</td>\n",
              "      <td>48.669140</td>\n",
              "      <td>167.100845</td>\n",
              "      <td>5.398695</td>\n",
              "      <td>6.106119</td>\n",
              "      <td>3.428859</td>\n",
              "      <td>31.025721</td>\n",
              "      <td>5.710853</td>\n",
              "      <td>...</td>\n",
              "      <td>24.128513</td>\n",
              "      <td>15.721425</td>\n",
              "      <td>14.106544</td>\n",
              "      <td>35.113213</td>\n",
              "      <td>0.873044</td>\n",
              "      <td>2.744514</td>\n",
              "      <td>2.508429</td>\n",
              "      <td>17.295000</td>\n",
              "      <td>3.251023</td>\n",
              "      <td>0.995289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-27</th>\n",
              "      <td>25.262548</td>\n",
              "      <td>8.845105</td>\n",
              "      <td>116.175179</td>\n",
              "      <td>48.204399</td>\n",
              "      <td>167.780121</td>\n",
              "      <td>5.382497</td>\n",
              "      <td>6.063608</td>\n",
              "      <td>3.412019</td>\n",
              "      <td>30.815815</td>\n",
              "      <td>5.706074</td>\n",
              "      <td>...</td>\n",
              "      <td>24.012835</td>\n",
              "      <td>15.542825</td>\n",
              "      <td>14.325471</td>\n",
              "      <td>35.113213</td>\n",
              "      <td>0.872469</td>\n",
              "      <td>2.764152</td>\n",
              "      <td>2.517959</td>\n",
              "      <td>17.150000</td>\n",
              "      <td>3.243335</td>\n",
              "      <td>0.994336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-28</th>\n",
              "      <td>25.187584</td>\n",
              "      <td>8.775399</td>\n",
              "      <td>115.254692</td>\n",
              "      <td>48.708691</td>\n",
              "      <td>167.391968</td>\n",
              "      <td>5.388214</td>\n",
              "      <td>6.075202</td>\n",
              "      <td>3.417632</td>\n",
              "      <td>30.955750</td>\n",
              "      <td>5.749085</td>\n",
              "      <td>...</td>\n",
              "      <td>23.897158</td>\n",
              "      <td>15.524024</td>\n",
              "      <td>14.230286</td>\n",
              "      <td>34.570324</td>\n",
              "      <td>0.869726</td>\n",
              "      <td>2.747459</td>\n",
              "      <td>2.498898</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>3.255828</td>\n",
              "      <td>1.011496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-29</th>\n",
              "      <td>25.290659</td>\n",
              "      <td>8.781051</td>\n",
              "      <td>115.448479</td>\n",
              "      <td>48.748245</td>\n",
              "      <td>168.750504</td>\n",
              "      <td>5.432997</td>\n",
              "      <td>6.111916</td>\n",
              "      <td>3.422310</td>\n",
              "      <td>31.485508</td>\n",
              "      <td>5.825548</td>\n",
              "      <td>...</td>\n",
              "      <td>24.003193</td>\n",
              "      <td>15.622725</td>\n",
              "      <td>14.263600</td>\n",
              "      <td>35.365265</td>\n",
              "      <td>0.873644</td>\n",
              "      <td>2.767098</td>\n",
              "      <td>2.512241</td>\n",
              "      <td>17.325001</td>\n",
              "      <td>3.311566</td>\n",
              "      <td>1.008636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-30</th>\n",
              "      <td>25.084511</td>\n",
              "      <td>8.705692</td>\n",
              "      <td>113.655952</td>\n",
              "      <td>48.006638</td>\n",
              "      <td>166.809738</td>\n",
              "      <td>5.368205</td>\n",
              "      <td>6.055879</td>\n",
              "      <td>3.435408</td>\n",
              "      <td>30.905775</td>\n",
              "      <td>5.744306</td>\n",
              "      <td>...</td>\n",
              "      <td>23.434443</td>\n",
              "      <td>15.284327</td>\n",
              "      <td>14.135099</td>\n",
              "      <td>34.958103</td>\n",
              "      <td>0.862868</td>\n",
              "      <td>2.751878</td>\n",
              "      <td>2.477931</td>\n",
              "      <td>17.120001</td>\n",
              "      <td>3.252945</td>\n",
              "      <td>0.982896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1792 rows 칑 34 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e1469b3-c40e-4bc4-bdec-56708cdbf810')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-70ff86ef-cd75-4e76-ad8d-b17d1a4ae748\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-70ff86ef-cd75-4e76-ad8d-b17d1a4ae748')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-70ff86ef-cd75-4e76-ad8d-b17d1a4ae748 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e1469b3-c40e-4bc4-bdec-56708cdbf810 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e1469b3-c40e-4bc4-bdec-56708cdbf810');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "               ACS.MC    ACX.MC     AENA.MC     AMS.MC      ANA.MC   BBVA.MC  \\\n",
              "Date                                                                           \n",
              "2016-01-04  15.980104  5.751740   79.277679  36.199356   60.231125  4.407446   \n",
              "2016-01-05  16.011051  5.890941   79.734390  36.367851   60.508472  4.436829   \n",
              "2016-01-06  15.887266  5.630723   79.315742  36.458935   60.346691  4.342002   \n",
              "2016-01-07  15.447847  5.364235   77.907539  35.552696   58.127884  4.286575   \n",
              "2016-01-08  15.206474  5.406247   78.630669  35.038105   58.089371  4.189745   \n",
              "...               ...       ...         ...        ...         ...       ...   \n",
              "2022-12-23  25.243809  8.771631  116.368965  48.669140  167.100845  5.398695   \n",
              "2022-12-27  25.262548  8.845105  116.175179  48.204399  167.780121  5.382497   \n",
              "2022-12-28  25.187584  8.775399  115.254692  48.708691  167.391968  5.388214   \n",
              "2022-12-29  25.290659  8.781051  115.448479  48.748245  168.750504  5.432997   \n",
              "2022-12-30  25.084511  8.705692  113.655952  48.006638  166.809738  5.368205   \n",
              "\n",
              "              BKT.MC   CABK.MC    CLNX.MC    COL.MC  ...    NTGY.MC  \\\n",
              "Date                                                 ...              \n",
              "2016-01-04  4.751968  2.298497  14.772966  5.183412  ...  10.938681   \n",
              "2016-01-05  4.754172  2.279122  14.747093  5.223532  ...  11.072264   \n",
              "2016-01-06  4.649078  2.198032  14.574611  5.055030  ...  10.989146   \n",
              "2016-01-07  4.581467  2.142059  14.600484  4.894553  ...  10.891187   \n",
              "2016-01-08  4.524878  2.115508  14.238276  4.886529  ...  10.768416   \n",
              "...              ...       ...        ...       ...  ...        ...   \n",
              "2022-12-23  6.106119  3.428859  31.025721  5.710853  ...  24.128513   \n",
              "2022-12-27  6.063608  3.412019  30.815815  5.706074  ...  24.012835   \n",
              "2022-12-28  6.075202  3.417632  30.955750  5.749085  ...  23.897158   \n",
              "2022-12-29  6.111916  3.422310  31.485508  5.825548  ...  24.003193   \n",
              "2022-12-30  6.055879  3.435408  30.905775  5.744306  ...  23.434443   \n",
              "\n",
              "               RED.MC     REP.MC    ROVI.MC    SAB.MC    SAN.MC   SCYR.MC  \\\n",
              "Date                                                                        \n",
              "2016-01-04  11.855685   6.404625  12.837639  1.201982  3.033155  1.313086   \n",
              "2016-01-05  11.923389   6.347962  12.837639  1.198304  3.007060  1.333093   \n",
              "2016-01-06  11.899774   6.166520  12.846617  1.186534  2.910236  1.284186   \n",
              "2016-01-07  11.828919   5.922048  12.765821  1.158581  2.850492  1.210084   \n",
              "2016-01-08  11.776964   5.684580  13.017186  1.120330  2.773582  1.202674   \n",
              "...               ...        ...        ...       ...       ...       ...   \n",
              "2022-12-23  15.721425  14.106544  35.113213  0.873044  2.744514  2.508429   \n",
              "2022-12-27  15.542825  14.325471  35.113213  0.872469  2.764152  2.517959   \n",
              "2022-12-28  15.524024  14.230286  34.570324  0.869726  2.747459  2.498898   \n",
              "2022-12-29  15.622725  14.263600  35.365265  0.873644  2.767098  2.512241   \n",
              "2022-12-30  15.284327  14.135099  34.958103  0.862868  2.751878  2.477931   \n",
              "\n",
              "               SLR.MC    TEF.MC    UNI.MC  \n",
              "Date                                       \n",
              "2016-01-04   0.700000  5.248940  1.019846  \n",
              "2016-01-05   0.745000  5.275911  1.019846  \n",
              "2016-01-06   0.720000  5.237833  1.019846  \n",
              "2016-01-07   0.700000  5.197640  1.019846  \n",
              "2016-01-08   0.700000  5.072830  1.019846  \n",
              "...               ...       ...       ...  \n",
              "2022-12-23  17.295000  3.251023  0.995289  \n",
              "2022-12-27  17.150000  3.243335  0.994336  \n",
              "2022-12-28  17.000000  3.255828  1.011496  \n",
              "2022-12-29  17.325001  3.311566  1.008636  \n",
              "2022-12-30  17.120001  3.252945  0.982896  \n",
              "\n",
              "[1792 rows x 34 columns]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ibex_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1LJ1mvicEcwI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pZPvPdYvAb7f",
        "outputId": "dcc82355-9572-4fe1-cff5-0e64ac2da5d8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(ibex_data / ibex_data.iloc[0,:]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IqHGyu-pNHE_",
        "outputId": "179a13f2-7997-479f-d1d9-099283986536"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ibex_data.isna().sum().sum(), bench.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mqqI2LUnRv9-"
      },
      "outputs": [],
      "source": [
        "ibex_data[\"Bench\"] = bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VzF9nG48R6sZ",
        "outputId": "726e1326-d146-43e3-9474-1966176af415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ibex_data.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RP-DjkIgSdQX"
      },
      "outputs": [],
      "source": [
        "ibex_data.fillna(method='ffill', inplace=True)  # forward fill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SI47gms1Sgf_",
        "outputId": "86b9632d-d563-45ce-cc7b-8fc9b0a87ad3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ibex_data.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mtnz6A5CSiG5",
        "outputId": "940094e7-f399-4dd8-9602-409b21811b31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "2016-01-04    9313.190430\n",
              "2016-01-05    9335.190430\n",
              "2016-01-06    9197.390625\n",
              "2016-01-07    9059.291016\n",
              "2016-01-08    8909.191406\n",
              "                 ...     \n",
              "2022-12-23    8269.099609\n",
              "2022-12-27    8270.099609\n",
              "2022-12-28    8258.500000\n",
              "2022-12-29    8318.299805\n",
              "2022-12-30    8229.099609\n",
              "Name: Bench, Length: 1792, dtype: float64"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ibex_data.Bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5uus0u3F60_8",
        "outputId": "4d3763b5-a26a-4762-de73-a6b9f5bfc0f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-a6bc1e1b-f4e7-4b11-9ea1-00e1e159140e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ACS.MC</th>\n",
              "      <th>ACX.MC</th>\n",
              "      <th>AENA.MC</th>\n",
              "      <th>AMS.MC</th>\n",
              "      <th>ANA.MC</th>\n",
              "      <th>BBVA.MC</th>\n",
              "      <th>BKT.MC</th>\n",
              "      <th>CABK.MC</th>\n",
              "      <th>CLNX.MC</th>\n",
              "      <th>COL.MC</th>\n",
              "      <th>...</th>\n",
              "      <th>RED.MC</th>\n",
              "      <th>REP.MC</th>\n",
              "      <th>ROVI.MC</th>\n",
              "      <th>SAB.MC</th>\n",
              "      <th>SAN.MC</th>\n",
              "      <th>SCYR.MC</th>\n",
              "      <th>SLR.MC</th>\n",
              "      <th>TEF.MC</th>\n",
              "      <th>UNI.MC</th>\n",
              "      <th>Bench</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-01-04</th>\n",
              "      <td>15.980104</td>\n",
              "      <td>5.751740</td>\n",
              "      <td>79.277679</td>\n",
              "      <td>36.199356</td>\n",
              "      <td>60.231125</td>\n",
              "      <td>4.407446</td>\n",
              "      <td>4.751968</td>\n",
              "      <td>2.298497</td>\n",
              "      <td>14.772966</td>\n",
              "      <td>5.183412</td>\n",
              "      <td>...</td>\n",
              "      <td>11.855685</td>\n",
              "      <td>6.404625</td>\n",
              "      <td>12.837639</td>\n",
              "      <td>1.201982</td>\n",
              "      <td>3.033155</td>\n",
              "      <td>1.313086</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>5.248940</td>\n",
              "      <td>1.019846</td>\n",
              "      <td>9313.190430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-05</th>\n",
              "      <td>16.011051</td>\n",
              "      <td>5.890941</td>\n",
              "      <td>79.734390</td>\n",
              "      <td>36.367851</td>\n",
              "      <td>60.508472</td>\n",
              "      <td>4.436829</td>\n",
              "      <td>4.754172</td>\n",
              "      <td>2.279122</td>\n",
              "      <td>14.747093</td>\n",
              "      <td>5.223532</td>\n",
              "      <td>...</td>\n",
              "      <td>11.923389</td>\n",
              "      <td>6.347962</td>\n",
              "      <td>12.837639</td>\n",
              "      <td>1.198304</td>\n",
              "      <td>3.007060</td>\n",
              "      <td>1.333093</td>\n",
              "      <td>0.745000</td>\n",
              "      <td>5.275911</td>\n",
              "      <td>1.019846</td>\n",
              "      <td>9335.190430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-06</th>\n",
              "      <td>15.887266</td>\n",
              "      <td>5.630723</td>\n",
              "      <td>79.315742</td>\n",
              "      <td>36.458935</td>\n",
              "      <td>60.346691</td>\n",
              "      <td>4.342002</td>\n",
              "      <td>4.649078</td>\n",
              "      <td>2.198032</td>\n",
              "      <td>14.574611</td>\n",
              "      <td>5.055030</td>\n",
              "      <td>...</td>\n",
              "      <td>11.899774</td>\n",
              "      <td>6.166520</td>\n",
              "      <td>12.846617</td>\n",
              "      <td>1.186534</td>\n",
              "      <td>2.910236</td>\n",
              "      <td>1.284186</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>5.237833</td>\n",
              "      <td>1.019846</td>\n",
              "      <td>9197.390625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-07</th>\n",
              "      <td>15.447847</td>\n",
              "      <td>5.364235</td>\n",
              "      <td>77.907539</td>\n",
              "      <td>35.552696</td>\n",
              "      <td>58.127884</td>\n",
              "      <td>4.286575</td>\n",
              "      <td>4.581467</td>\n",
              "      <td>2.142059</td>\n",
              "      <td>14.600484</td>\n",
              "      <td>4.894553</td>\n",
              "      <td>...</td>\n",
              "      <td>11.828919</td>\n",
              "      <td>5.922048</td>\n",
              "      <td>12.765821</td>\n",
              "      <td>1.158581</td>\n",
              "      <td>2.850492</td>\n",
              "      <td>1.210084</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>5.197640</td>\n",
              "      <td>1.019846</td>\n",
              "      <td>9059.291016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-08</th>\n",
              "      <td>15.206474</td>\n",
              "      <td>5.406247</td>\n",
              "      <td>78.630669</td>\n",
              "      <td>35.038105</td>\n",
              "      <td>58.089371</td>\n",
              "      <td>4.189745</td>\n",
              "      <td>4.524878</td>\n",
              "      <td>2.115508</td>\n",
              "      <td>14.238276</td>\n",
              "      <td>4.886529</td>\n",
              "      <td>...</td>\n",
              "      <td>11.776964</td>\n",
              "      <td>5.684580</td>\n",
              "      <td>13.017186</td>\n",
              "      <td>1.120330</td>\n",
              "      <td>2.773582</td>\n",
              "      <td>1.202674</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>5.072830</td>\n",
              "      <td>1.019846</td>\n",
              "      <td>8909.191406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-23</th>\n",
              "      <td>25.243809</td>\n",
              "      <td>8.771631</td>\n",
              "      <td>116.368965</td>\n",
              "      <td>48.669140</td>\n",
              "      <td>167.100845</td>\n",
              "      <td>5.398695</td>\n",
              "      <td>6.106119</td>\n",
              "      <td>3.428859</td>\n",
              "      <td>31.025721</td>\n",
              "      <td>5.710853</td>\n",
              "      <td>...</td>\n",
              "      <td>15.721425</td>\n",
              "      <td>14.106544</td>\n",
              "      <td>35.113213</td>\n",
              "      <td>0.873044</td>\n",
              "      <td>2.744514</td>\n",
              "      <td>2.508429</td>\n",
              "      <td>17.295000</td>\n",
              "      <td>3.251023</td>\n",
              "      <td>0.995289</td>\n",
              "      <td>8269.099609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-27</th>\n",
              "      <td>25.262548</td>\n",
              "      <td>8.845105</td>\n",
              "      <td>116.175179</td>\n",
              "      <td>48.204399</td>\n",
              "      <td>167.780121</td>\n",
              "      <td>5.382497</td>\n",
              "      <td>6.063608</td>\n",
              "      <td>3.412019</td>\n",
              "      <td>30.815815</td>\n",
              "      <td>5.706074</td>\n",
              "      <td>...</td>\n",
              "      <td>15.542825</td>\n",
              "      <td>14.325471</td>\n",
              "      <td>35.113213</td>\n",
              "      <td>0.872469</td>\n",
              "      <td>2.764152</td>\n",
              "      <td>2.517959</td>\n",
              "      <td>17.150000</td>\n",
              "      <td>3.243335</td>\n",
              "      <td>0.994336</td>\n",
              "      <td>8270.099609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-28</th>\n",
              "      <td>25.187584</td>\n",
              "      <td>8.775399</td>\n",
              "      <td>115.254692</td>\n",
              "      <td>48.708691</td>\n",
              "      <td>167.391968</td>\n",
              "      <td>5.388214</td>\n",
              "      <td>6.075202</td>\n",
              "      <td>3.417632</td>\n",
              "      <td>30.955750</td>\n",
              "      <td>5.749085</td>\n",
              "      <td>...</td>\n",
              "      <td>15.524024</td>\n",
              "      <td>14.230286</td>\n",
              "      <td>34.570324</td>\n",
              "      <td>0.869726</td>\n",
              "      <td>2.747459</td>\n",
              "      <td>2.498898</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>3.255828</td>\n",
              "      <td>1.011496</td>\n",
              "      <td>8258.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-29</th>\n",
              "      <td>25.290659</td>\n",
              "      <td>8.781051</td>\n",
              "      <td>115.448479</td>\n",
              "      <td>48.748245</td>\n",
              "      <td>168.750504</td>\n",
              "      <td>5.432997</td>\n",
              "      <td>6.111916</td>\n",
              "      <td>3.422310</td>\n",
              "      <td>31.485508</td>\n",
              "      <td>5.825548</td>\n",
              "      <td>...</td>\n",
              "      <td>15.622725</td>\n",
              "      <td>14.263600</td>\n",
              "      <td>35.365265</td>\n",
              "      <td>0.873644</td>\n",
              "      <td>2.767098</td>\n",
              "      <td>2.512241</td>\n",
              "      <td>17.325001</td>\n",
              "      <td>3.311566</td>\n",
              "      <td>1.008636</td>\n",
              "      <td>8318.299805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-30</th>\n",
              "      <td>25.084511</td>\n",
              "      <td>8.705692</td>\n",
              "      <td>113.655952</td>\n",
              "      <td>48.006638</td>\n",
              "      <td>166.809738</td>\n",
              "      <td>5.368205</td>\n",
              "      <td>6.055879</td>\n",
              "      <td>3.435408</td>\n",
              "      <td>30.905775</td>\n",
              "      <td>5.744306</td>\n",
              "      <td>...</td>\n",
              "      <td>15.284327</td>\n",
              "      <td>14.135099</td>\n",
              "      <td>34.958103</td>\n",
              "      <td>0.862868</td>\n",
              "      <td>2.751878</td>\n",
              "      <td>2.477931</td>\n",
              "      <td>17.120001</td>\n",
              "      <td>3.252945</td>\n",
              "      <td>0.982896</td>\n",
              "      <td>8229.099609</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1792 rows 칑 35 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6bc1e1b-f4e7-4b11-9ea1-00e1e159140e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-d1cb3b57-fca9-4327-9aac-2c7136047e07\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d1cb3b57-fca9-4327-9aac-2c7136047e07')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-d1cb3b57-fca9-4327-9aac-2c7136047e07 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a6bc1e1b-f4e7-4b11-9ea1-00e1e159140e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a6bc1e1b-f4e7-4b11-9ea1-00e1e159140e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "               ACS.MC    ACX.MC     AENA.MC     AMS.MC      ANA.MC   BBVA.MC  \\\n",
              "Date                                                                           \n",
              "2016-01-04  15.980104  5.751740   79.277679  36.199356   60.231125  4.407446   \n",
              "2016-01-05  16.011051  5.890941   79.734390  36.367851   60.508472  4.436829   \n",
              "2016-01-06  15.887266  5.630723   79.315742  36.458935   60.346691  4.342002   \n",
              "2016-01-07  15.447847  5.364235   77.907539  35.552696   58.127884  4.286575   \n",
              "2016-01-08  15.206474  5.406247   78.630669  35.038105   58.089371  4.189745   \n",
              "...               ...       ...         ...        ...         ...       ...   \n",
              "2022-12-23  25.243809  8.771631  116.368965  48.669140  167.100845  5.398695   \n",
              "2022-12-27  25.262548  8.845105  116.175179  48.204399  167.780121  5.382497   \n",
              "2022-12-28  25.187584  8.775399  115.254692  48.708691  167.391968  5.388214   \n",
              "2022-12-29  25.290659  8.781051  115.448479  48.748245  168.750504  5.432997   \n",
              "2022-12-30  25.084511  8.705692  113.655952  48.006638  166.809738  5.368205   \n",
              "\n",
              "              BKT.MC   CABK.MC    CLNX.MC    COL.MC  ...     RED.MC  \\\n",
              "Date                                                 ...              \n",
              "2016-01-04  4.751968  2.298497  14.772966  5.183412  ...  11.855685   \n",
              "2016-01-05  4.754172  2.279122  14.747093  5.223532  ...  11.923389   \n",
              "2016-01-06  4.649078  2.198032  14.574611  5.055030  ...  11.899774   \n",
              "2016-01-07  4.581467  2.142059  14.600484  4.894553  ...  11.828919   \n",
              "2016-01-08  4.524878  2.115508  14.238276  4.886529  ...  11.776964   \n",
              "...              ...       ...        ...       ...  ...        ...   \n",
              "2022-12-23  6.106119  3.428859  31.025721  5.710853  ...  15.721425   \n",
              "2022-12-27  6.063608  3.412019  30.815815  5.706074  ...  15.542825   \n",
              "2022-12-28  6.075202  3.417632  30.955750  5.749085  ...  15.524024   \n",
              "2022-12-29  6.111916  3.422310  31.485508  5.825548  ...  15.622725   \n",
              "2022-12-30  6.055879  3.435408  30.905775  5.744306  ...  15.284327   \n",
              "\n",
              "               REP.MC    ROVI.MC    SAB.MC    SAN.MC   SCYR.MC     SLR.MC  \\\n",
              "Date                                                                        \n",
              "2016-01-04   6.404625  12.837639  1.201982  3.033155  1.313086   0.700000   \n",
              "2016-01-05   6.347962  12.837639  1.198304  3.007060  1.333093   0.745000   \n",
              "2016-01-06   6.166520  12.846617  1.186534  2.910236  1.284186   0.720000   \n",
              "2016-01-07   5.922048  12.765821  1.158581  2.850492  1.210084   0.700000   \n",
              "2016-01-08   5.684580  13.017186  1.120330  2.773582  1.202674   0.700000   \n",
              "...               ...        ...       ...       ...       ...        ...   \n",
              "2022-12-23  14.106544  35.113213  0.873044  2.744514  2.508429  17.295000   \n",
              "2022-12-27  14.325471  35.113213  0.872469  2.764152  2.517959  17.150000   \n",
              "2022-12-28  14.230286  34.570324  0.869726  2.747459  2.498898  17.000000   \n",
              "2022-12-29  14.263600  35.365265  0.873644  2.767098  2.512241  17.325001   \n",
              "2022-12-30  14.135099  34.958103  0.862868  2.751878  2.477931  17.120001   \n",
              "\n",
              "              TEF.MC    UNI.MC        Bench  \n",
              "Date                                         \n",
              "2016-01-04  5.248940  1.019846  9313.190430  \n",
              "2016-01-05  5.275911  1.019846  9335.190430  \n",
              "2016-01-06  5.237833  1.019846  9197.390625  \n",
              "2016-01-07  5.197640  1.019846  9059.291016  \n",
              "2016-01-08  5.072830  1.019846  8909.191406  \n",
              "...              ...       ...          ...  \n",
              "2022-12-23  3.251023  0.995289  8269.099609  \n",
              "2022-12-27  3.243335  0.994336  8270.099609  \n",
              "2022-12-28  3.255828  1.011496  8258.500000  \n",
              "2022-12-29  3.311566  1.008636  8318.299805  \n",
              "2022-12-30  3.252945  0.982896  8229.099609  \n",
              "\n",
              "[1792 rows x 35 columns]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ibex_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F5HjThspATQ1",
        "outputId": "94f1c7d0-a32a-49c0-b824-326013dbc545"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "2016-01-04    9313.190430\n",
              "2016-01-05    9335.190430\n",
              "2016-01-06    9197.390625\n",
              "2016-01-07    9059.291016\n",
              "2016-01-08    8909.191406\n",
              "                 ...     \n",
              "2022-12-23    8269.099609\n",
              "2022-12-27    8270.099609\n",
              "2022-12-28    8258.500000\n",
              "2022-12-29    8318.299805\n",
              "2022-12-30    8229.099609\n",
              "Name: Bench, Length: 1791, dtype: float64"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NPlqWJ2yAVBB",
        "outputId": "40c57224-f5ac-4147-ff56-d1daef281252"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "2016-01-04    9313.190430\n",
              "2016-01-05    9335.190430\n",
              "2016-01-06    9197.390625\n",
              "2016-01-07    9059.291016\n",
              "2016-01-08    8909.191406\n",
              "                 ...     \n",
              "2022-12-23    8269.099609\n",
              "2022-12-27    8270.099609\n",
              "2022-12-28    8258.500000\n",
              "2022-12-29    8318.299805\n",
              "2022-12-30    8229.099609\n",
              "Name: Bench, Length: 1792, dtype: float64"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ibex_data.Bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D5td8YJop6bV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_dataset(ibex_data, create_suffle_data = True):\n",
        "  days_steps = 1\n",
        "  days_backward = 28 * 3\n",
        "  days_forward = 28\n",
        "\n",
        "  datos_analisis = ibex_data\n",
        "  datos_analisis_bench = ibex_data.Bench\n",
        "  X_DATA = []\n",
        "  Y_DATA = []\n",
        "  X_DATA_POS = []\n",
        "\n",
        "  for i in range(days_backward, len(datos_analisis), days_steps):\n",
        "\n",
        "      X_forward = datos_analisis[i:i+days_forward]\n",
        "\n",
        "      X_data = datos_analisis[i-days_backward:i]\n",
        "      # Norm todos empiezan en 1\n",
        "      X_data = X_data/ X_data.iloc[0,:]\n",
        "\n",
        "      X_data_bench = datos_analisis_bench[i-days_backward:i]\n",
        "      # Norm todos empiezan en 1\n",
        "      X_data_bench = X_data_bench/ X_data_bench.iloc[0]\n",
        "\n",
        "      ## RETONORNOS\n",
        "      last_day = X_data.iloc[X_data.shape[0]-1,:]\n",
        "      returns_n = 2 * (last_day - last_day.min()) / (last_day.max() - last_day.min()) - 1\n",
        "\n",
        "      ## MAXIMOS\n",
        "      x_max =  X_data.max()\n",
        "      max_n = 2 * (x_max - x_max.min()) / (x_max.max() - x_max.min()) - 1\n",
        "\n",
        "      ## MINIMOS\n",
        "      x_min = X_data.min()\n",
        "      min_n = 2 * (x_min - x_min.min()) / (x_min.max() - x_min.min()) - 1\n",
        "\n",
        "      ## STD\n",
        "      x_std = X_data.std()\n",
        "      std_n = 2 * (x_std - x_std.min()) / (x_std.max() - x_std.min()) - 1\n",
        "\n",
        "      ## MEDIA\n",
        "      x_mean = X_data.mean()\n",
        "      mean_n = 2 * (x_mean - x_mean.min()) / (x_mean.max() - x_mean.min()) - 1\n",
        "\n",
        "      ## SHARPE\n",
        "      x_sharpe = x_mean / (x_std + 0.000001)\n",
        "      sharpe_n = 2 * (x_sharpe - x_sharpe.min()) / (x_sharpe.max() - x_sharpe.min()) - 1\n",
        "\n",
        "\n",
        "      ## ALPHA Y BETA\n",
        "\n",
        "      betas = {}\n",
        "      alphas = {}\n",
        "\n",
        "      market_returns = X_data_bench.diff().dropna()\n",
        "      df = X_data.diff().dropna()\n",
        "\n",
        "      # Para cada activo, calcular beta y alpha\n",
        "      for column in df.columns:\n",
        "          asset_returns = df[column]\n",
        "\n",
        "          # Calcula el beta\n",
        "          cov_matrix = np.cov(asset_returns, market_returns)\n",
        "          beta = cov_matrix[0, 1] / cov_matrix[1, 1]\n",
        "          betas[column] = beta\n",
        "\n",
        "          # Calcula el alpha\n",
        "          alpha = asset_returns.mean() - beta * market_returns.mean()\n",
        "          alphas[column] = alpha\n",
        "\n",
        "      betas_series = pd.Series(betas)\n",
        "      alphas_series = pd.Series(alphas)\n",
        "\n",
        "      beta_n = 2 * (betas_series - betas_series.min()) / (betas_series.max() - betas_series.min()) - 1\n",
        "      alpha_n = 2 * (alphas_series - alphas_series.min()) / (alphas_series.max() - alphas_series.min()) - 1\n",
        "\n",
        "      rs = calculate_sharpe_ratio(np.log(X_forward).diff().dropna(), 0)\n",
        "      y = np.argsort(np.argsort(-rs))\n",
        "      y_n = 2 * (y - y.min()) / (y.max() - y.min()) - 1\n",
        "      #print(y)\n",
        "      #print(y_n)\n",
        "      # TODO: Unir returns_n, max_n, min_n, std_n, mean_n, sharpe_n, beta_n, alpha_n , en un solo dataframe\n",
        "\n",
        "      # Unir todas las series en un DataFrame\n",
        "      features_df = pd.concat([returns_n, max_n, min_n, std_n, mean_n, sharpe_n, beta_n, alpha_n], axis=1)\n",
        "\n",
        "      # Nombrar las columnas\n",
        "      features_df.columns = ['returns', 'max', 'min', 'std', 'mean', 'sharpe', 'beta', 'alpha']\n",
        "\n",
        "\n",
        "      if create_suffle_data == True :\n",
        "        for _ in range(10):\n",
        "          t = features_df.reset_index(drop=True)\n",
        "          t = pd.concat([t, y_n.reset_index(drop=True) ], axis=1).sample(frac=1)\n",
        "\n",
        "          p = t.iloc[:,-1]\n",
        "          t = t.iloc[:,:-1]\n",
        "\n",
        "          X_DATA.append(t)\n",
        "          Y_DATA.append(p)\n",
        "          X_DATA_POS.append(t.index)\n",
        "      else:\n",
        "          X_DATA.append(features_df)\n",
        "          Y_DATA.append(y_n)\n",
        "          X_DATA_POS.append(features_df.index)\n",
        "\n",
        "\n",
        "\n",
        "      #print(features_df)\n",
        "      #print(y)\n",
        "      #print(rs)\n",
        "\n",
        "  return X_DATA , Y_DATA ,X_DATA_POS\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk67dlm2Wiv2"
      },
      "source": [
        "# X e y para TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uRNZC2N0eBvd",
        "outputId": "a1ae5f6e-5dc0-4bee-d0cb-0dd71b47d019"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# las fechas estan dentro del dasaset\n",
        "X_DATA_TRAIN , Y_DATA_TRAIN , X_DATA_POS_TRAIN = create_dataset(ibex_data.loc[\"2016-01-04\":\"2020-08-03\"], create_suffle_data = False)\n",
        "X_DATA_VALID , Y_DATA_VALID , X_DATA_POS_VALID = create_dataset(ibex_data.loc[\"2020-08-04\":\"2021-12-29\"], create_suffle_data = False)\n",
        "#X_DATA_TEST , Y_DATA_TEST , X_DATA_POS_TEST = create_dataset(ibex_data.loc[\"2021-12-30\":])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j1VmGvFNiNXG",
        "outputId": "77a29330-d852-435b-c3a1-319ec15a92e7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-03f22c2a-77ed-48ee-b87b-b6436f0b82e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>returns</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>sharpe</th>\n",
              "      <th>beta</th>\n",
              "      <th>alpha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ACS.MC</th>\n",
              "      <td>-0.400864</td>\n",
              "      <td>-0.708283</td>\n",
              "      <td>-0.601875</td>\n",
              "      <td>-0.508974</td>\n",
              "      <td>-0.375391</td>\n",
              "      <td>-0.999987</td>\n",
              "      <td>0.109078</td>\n",
              "      <td>-0.410349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACX.MC</th>\n",
              "      <td>-0.471878</td>\n",
              "      <td>-0.586413</td>\n",
              "      <td>-0.484873</td>\n",
              "      <td>-0.472507</td>\n",
              "      <td>-0.075451</td>\n",
              "      <td>-0.999988</td>\n",
              "      <td>0.113675</td>\n",
              "      <td>-0.479220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AENA.MC</th>\n",
              "      <td>-0.314281</td>\n",
              "      <td>-0.633475</td>\n",
              "      <td>0.347717</td>\n",
              "      <td>-0.623052</td>\n",
              "      <td>-0.064336</td>\n",
              "      <td>-0.999980</td>\n",
              "      <td>-0.377487</td>\n",
              "      <td>-0.351748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AMS.MC</th>\n",
              "      <td>-0.645111</td>\n",
              "      <td>-0.967160</td>\n",
              "      <td>-0.100319</td>\n",
              "      <td>-0.811158</td>\n",
              "      <td>-0.480446</td>\n",
              "      <td>-0.999959</td>\n",
              "      <td>-0.352714</td>\n",
              "      <td>-0.672414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ANA.MC</th>\n",
              "      <td>-0.838502</td>\n",
              "      <td>-0.991669</td>\n",
              "      <td>-0.395678</td>\n",
              "      <td>-0.813592</td>\n",
              "      <td>-0.797002</td>\n",
              "      <td>-0.999961</td>\n",
              "      <td>-0.385706</td>\n",
              "      <td>-0.862368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BBVA.MC</th>\n",
              "      <td>-0.792129</td>\n",
              "      <td>-0.889941</td>\n",
              "      <td>-0.318499</td>\n",
              "      <td>-0.734159</td>\n",
              "      <td>-0.639887</td>\n",
              "      <td>-0.999973</td>\n",
              "      <td>0.558763</td>\n",
              "      <td>-0.767420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BKT.MC</th>\n",
              "      <td>-0.602804</td>\n",
              "      <td>-0.889893</td>\n",
              "      <td>0.216178</td>\n",
              "      <td>-0.808992</td>\n",
              "      <td>-0.387512</td>\n",
              "      <td>-0.999958</td>\n",
              "      <td>0.084716</td>\n",
              "      <td>-0.608168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CABK.MC</th>\n",
              "      <td>-0.989466</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.731717</td>\n",
              "      <td>-0.755208</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.999974</td>\n",
              "      <td>0.376730</td>\n",
              "      <td>-0.969075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CLNX.MC</th>\n",
              "      <td>-0.946454</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.455723</td>\n",
              "      <td>-0.740033</td>\n",
              "      <td>-0.892297</td>\n",
              "      <td>-0.999975</td>\n",
              "      <td>-0.439242</td>\n",
              "      <td>-0.970253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>COL.MC</th>\n",
              "      <td>-0.575336</td>\n",
              "      <td>-0.904780</td>\n",
              "      <td>-0.210206</td>\n",
              "      <td>-0.715553</td>\n",
              "      <td>-0.447088</td>\n",
              "      <td>-0.999974</td>\n",
              "      <td>-0.442252</td>\n",
              "      <td>-0.609229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ELE.MC</th>\n",
              "      <td>-0.589381</td>\n",
              "      <td>-0.918816</td>\n",
              "      <td>0.284120</td>\n",
              "      <td>-0.820726</td>\n",
              "      <td>-0.363097</td>\n",
              "      <td>-0.999955</td>\n",
              "      <td>-0.531789</td>\n",
              "      <td>-0.627621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ENG.MC</th>\n",
              "      <td>-0.610144</td>\n",
              "      <td>-0.887212</td>\n",
              "      <td>0.740833</td>\n",
              "      <td>-0.886408</td>\n",
              "      <td>-0.174237</td>\n",
              "      <td>-0.999922</td>\n",
              "      <td>-0.627536</td>\n",
              "      <td>-0.652879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FDR.MC</th>\n",
              "      <td>-0.240140</td>\n",
              "      <td>-0.546234</td>\n",
              "      <td>0.846327</td>\n",
              "      <td>-0.545269</td>\n",
              "      <td>0.234008</td>\n",
              "      <td>-0.999983</td>\n",
              "      <td>-0.849758</td>\n",
              "      <td>-0.304502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FER.MC</th>\n",
              "      <td>-0.824040</td>\n",
              "      <td>-0.995182</td>\n",
              "      <td>-0.093352</td>\n",
              "      <td>-0.825695</td>\n",
              "      <td>-0.662069</td>\n",
              "      <td>-0.999957</td>\n",
              "      <td>-0.370263</td>\n",
              "      <td>-0.847478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GRF.MC</th>\n",
              "      <td>-0.795579</td>\n",
              "      <td>-0.970436</td>\n",
              "      <td>0.178096</td>\n",
              "      <td>-0.869793</td>\n",
              "      <td>-0.478677</td>\n",
              "      <td>-0.999937</td>\n",
              "      <td>-0.461612</td>\n",
              "      <td>-0.824597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IAG.MC</th>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.945474</td>\n",
              "      <td>-0.676576</td>\n",
              "      <td>-0.748436</td>\n",
              "      <td>-0.902819</td>\n",
              "      <td>-0.999974</td>\n",
              "      <td>-0.015215</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IBE.MC</th>\n",
              "      <td>-0.678698</td>\n",
              "      <td>-0.932969</td>\n",
              "      <td>0.425354</td>\n",
              "      <td>-0.858264</td>\n",
              "      <td>-0.401251</td>\n",
              "      <td>-0.999942</td>\n",
              "      <td>-0.588604</td>\n",
              "      <td>-0.717544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IDR.MC</th>\n",
              "      <td>-0.372982</td>\n",
              "      <td>-0.620789</td>\n",
              "      <td>0.105955</td>\n",
              "      <td>-0.500579</td>\n",
              "      <td>0.018119</td>\n",
              "      <td>-0.999986</td>\n",
              "      <td>-0.017556</td>\n",
              "      <td>-0.389893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ITX.MC</th>\n",
              "      <td>-0.776819</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.156998</td>\n",
              "      <td>-0.869311</td>\n",
              "      <td>-0.478120</td>\n",
              "      <td>-0.999938</td>\n",
              "      <td>-0.243374</td>\n",
              "      <td>-0.794829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOG.MC</th>\n",
              "      <td>-0.605038</td>\n",
              "      <td>-0.828605</td>\n",
              "      <td>0.329587</td>\n",
              "      <td>-0.781867</td>\n",
              "      <td>-0.138433</td>\n",
              "      <td>-0.999962</td>\n",
              "      <td>-0.436195</td>\n",
              "      <td>-0.637817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAP.MC</th>\n",
              "      <td>-0.650368</td>\n",
              "      <td>-0.902863</td>\n",
              "      <td>-0.673591</td>\n",
              "      <td>-0.644965</td>\n",
              "      <td>-0.783474</td>\n",
              "      <td>-0.999982</td>\n",
              "      <td>0.115513</td>\n",
              "      <td>-0.652834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MEL.MC</th>\n",
              "      <td>-0.791207</td>\n",
              "      <td>-0.983778</td>\n",
              "      <td>-0.893535</td>\n",
              "      <td>-0.683781</td>\n",
              "      <td>-0.880988</td>\n",
              "      <td>-0.999980</td>\n",
              "      <td>-0.184915</td>\n",
              "      <td>-0.805748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MRL.MC</th>\n",
              "      <td>-0.826533</td>\n",
              "      <td>-0.990478</td>\n",
              "      <td>-0.430687</td>\n",
              "      <td>-0.797359</td>\n",
              "      <td>-0.798519</td>\n",
              "      <td>-0.999965</td>\n",
              "      <td>-0.219513</td>\n",
              "      <td>-0.841953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MTS.MC</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NTGY.MC</th>\n",
              "      <td>-0.656089</td>\n",
              "      <td>-0.946315</td>\n",
              "      <td>-0.075698</td>\n",
              "      <td>-0.775850</td>\n",
              "      <td>-0.465831</td>\n",
              "      <td>-0.999966</td>\n",
              "      <td>-0.274859</td>\n",
              "      <td>-0.678992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RED.MC</th>\n",
              "      <td>-0.593684</td>\n",
              "      <td>-0.917590</td>\n",
              "      <td>0.513101</td>\n",
              "      <td>-0.855768</td>\n",
              "      <td>-0.268808</td>\n",
              "      <td>-0.999941</td>\n",
              "      <td>-0.652158</td>\n",
              "      <td>-0.638158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>REP.MC</th>\n",
              "      <td>-0.438474</td>\n",
              "      <td>-0.694276</td>\n",
              "      <td>-0.364672</td>\n",
              "      <td>-0.564427</td>\n",
              "      <td>-0.404725</td>\n",
              "      <td>-0.999985</td>\n",
              "      <td>0.511229</td>\n",
              "      <td>-0.425741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ROVI.MC</th>\n",
              "      <td>-0.576935</td>\n",
              "      <td>-0.863363</td>\n",
              "      <td>-0.065139</td>\n",
              "      <td>-0.737791</td>\n",
              "      <td>-0.291513</td>\n",
              "      <td>-0.999970</td>\n",
              "      <td>-0.884252</td>\n",
              "      <td>-0.634099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAB.MC</th>\n",
              "      <td>-0.575399</td>\n",
              "      <td>-0.798469</td>\n",
              "      <td>-0.002281</td>\n",
              "      <td>-0.672701</td>\n",
              "      <td>-0.417941</td>\n",
              "      <td>-0.999978</td>\n",
              "      <td>0.543888</td>\n",
              "      <td>-0.557277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SAN.MC</th>\n",
              "      <td>-0.646042</td>\n",
              "      <td>-0.868486</td>\n",
              "      <td>-0.624096</td>\n",
              "      <td>-0.660147</td>\n",
              "      <td>-0.695500</td>\n",
              "      <td>-0.999981</td>\n",
              "      <td>0.638335</td>\n",
              "      <td>-0.621048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SCYR.MC</th>\n",
              "      <td>-0.619536</td>\n",
              "      <td>-0.849917</td>\n",
              "      <td>-0.818019</td>\n",
              "      <td>-0.547680</td>\n",
              "      <td>-0.623259</td>\n",
              "      <td>-0.999987</td>\n",
              "      <td>0.480426</td>\n",
              "      <td>-0.603580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SLR.MC</th>\n",
              "      <td>-0.786999</td>\n",
              "      <td>-0.883696</td>\n",
              "      <td>-0.588620</td>\n",
              "      <td>-0.640438</td>\n",
              "      <td>-0.825120</td>\n",
              "      <td>-0.999983</td>\n",
              "      <td>-0.228687</td>\n",
              "      <td>-0.803962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TEF.MC</th>\n",
              "      <td>-0.746681</td>\n",
              "      <td>-0.954429</td>\n",
              "      <td>0.021486</td>\n",
              "      <td>-0.824710</td>\n",
              "      <td>-0.437563</td>\n",
              "      <td>-0.999955</td>\n",
              "      <td>0.232452</td>\n",
              "      <td>-0.740400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UNI.MC</th>\n",
              "      <td>-0.670618</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.241012</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.731380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bench</th>\n",
              "      <td>-0.727205</td>\n",
              "      <td>-0.995726</td>\n",
              "      <td>-0.133905</td>\n",
              "      <td>-0.817041</td>\n",
              "      <td>-0.566952</td>\n",
              "      <td>-0.999958</td>\n",
              "      <td>0.044105</td>\n",
              "      <td>-0.731380</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03f22c2a-77ed-48ee-b87b-b6436f0b82e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-ac13e563-d915-401a-a98f-8994b0292a8a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac13e563-d915-401a-a98f-8994b0292a8a')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-ac13e563-d915-401a-a98f-8994b0292a8a button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03f22c2a-77ed-48ee-b87b-b6436f0b82e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03f22c2a-77ed-48ee-b87b-b6436f0b82e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "          returns       max       min       std      mean    sharpe      beta  \\\n",
              "ACS.MC  -0.400864 -0.708283 -0.601875 -0.508974 -0.375391 -0.999987  0.109078   \n",
              "ACX.MC  -0.471878 -0.586413 -0.484873 -0.472507 -0.075451 -0.999988  0.113675   \n",
              "AENA.MC -0.314281 -0.633475  0.347717 -0.623052 -0.064336 -0.999980 -0.377487   \n",
              "AMS.MC  -0.645111 -0.967160 -0.100319 -0.811158 -0.480446 -0.999959 -0.352714   \n",
              "ANA.MC  -0.838502 -0.991669 -0.395678 -0.813592 -0.797002 -0.999961 -0.385706   \n",
              "BBVA.MC -0.792129 -0.889941 -0.318499 -0.734159 -0.639887 -0.999973  0.558763   \n",
              "BKT.MC  -0.602804 -0.889893  0.216178 -0.808992 -0.387512 -0.999958  0.084716   \n",
              "CABK.MC -0.989466 -1.000000 -0.731717 -0.755208 -1.000000 -0.999974  0.376730   \n",
              "CLNX.MC -0.946454 -1.000000 -0.455723 -0.740033 -0.892297 -0.999975 -0.439242   \n",
              "COL.MC  -0.575336 -0.904780 -0.210206 -0.715553 -0.447088 -0.999974 -0.442252   \n",
              "ELE.MC  -0.589381 -0.918816  0.284120 -0.820726 -0.363097 -0.999955 -0.531789   \n",
              "ENG.MC  -0.610144 -0.887212  0.740833 -0.886408 -0.174237 -0.999922 -0.627536   \n",
              "FDR.MC  -0.240140 -0.546234  0.846327 -0.545269  0.234008 -0.999983 -0.849758   \n",
              "FER.MC  -0.824040 -0.995182 -0.093352 -0.825695 -0.662069 -0.999957 -0.370263   \n",
              "GRF.MC  -0.795579 -0.970436  0.178096 -0.869793 -0.478677 -0.999937 -0.461612   \n",
              "IAG.MC  -1.000000 -0.945474 -0.676576 -0.748436 -0.902819 -0.999974 -0.015215   \n",
              "IBE.MC  -0.678698 -0.932969  0.425354 -0.858264 -0.401251 -0.999942 -0.588604   \n",
              "IDR.MC  -0.372982 -0.620789  0.105955 -0.500579  0.018119 -0.999986 -0.017556   \n",
              "ITX.MC  -0.776819 -1.000000  0.156998 -0.869311 -0.478120 -0.999938 -0.243374   \n",
              "LOG.MC  -0.605038 -0.828605  0.329587 -0.781867 -0.138433 -0.999962 -0.436195   \n",
              "MAP.MC  -0.650368 -0.902863 -0.673591 -0.644965 -0.783474 -0.999982  0.115513   \n",
              "MEL.MC  -0.791207 -0.983778 -0.893535 -0.683781 -0.880988 -0.999980 -0.184915   \n",
              "MRL.MC  -0.826533 -0.990478 -0.430687 -0.797359 -0.798519 -0.999965 -0.219513   \n",
              "MTS.MC   1.000000  1.000000 -1.000000  1.000000  1.000000 -1.000000  1.000000   \n",
              "NTGY.MC -0.656089 -0.946315 -0.075698 -0.775850 -0.465831 -0.999966 -0.274859   \n",
              "RED.MC  -0.593684 -0.917590  0.513101 -0.855768 -0.268808 -0.999941 -0.652158   \n",
              "REP.MC  -0.438474 -0.694276 -0.364672 -0.564427 -0.404725 -0.999985  0.511229   \n",
              "ROVI.MC -0.576935 -0.863363 -0.065139 -0.737791 -0.291513 -0.999970 -0.884252   \n",
              "SAB.MC  -0.575399 -0.798469 -0.002281 -0.672701 -0.417941 -0.999978  0.543888   \n",
              "SAN.MC  -0.646042 -0.868486 -0.624096 -0.660147 -0.695500 -0.999981  0.638335   \n",
              "SCYR.MC -0.619536 -0.849917 -0.818019 -0.547680 -0.623259 -0.999987  0.480426   \n",
              "SLR.MC  -0.786999 -0.883696 -0.588620 -0.640438 -0.825120 -0.999983 -0.228687   \n",
              "TEF.MC  -0.746681 -0.954429  0.021486 -0.824710 -0.437563 -0.999955  0.232452   \n",
              "UNI.MC  -0.670618 -1.000000  1.000000 -1.000000 -0.241012  1.000000 -1.000000   \n",
              "Bench   -0.727205 -0.995726 -0.133905 -0.817041 -0.566952 -0.999958  0.044105   \n",
              "\n",
              "            alpha  \n",
              "ACS.MC  -0.410349  \n",
              "ACX.MC  -0.479220  \n",
              "AENA.MC -0.351748  \n",
              "AMS.MC  -0.672414  \n",
              "ANA.MC  -0.862368  \n",
              "BBVA.MC -0.767420  \n",
              "BKT.MC  -0.608168  \n",
              "CABK.MC -0.969075  \n",
              "CLNX.MC -0.970253  \n",
              "COL.MC  -0.609229  \n",
              "ELE.MC  -0.627621  \n",
              "ENG.MC  -0.652879  \n",
              "FDR.MC  -0.304502  \n",
              "FER.MC  -0.847478  \n",
              "GRF.MC  -0.824597  \n",
              "IAG.MC  -1.000000  \n",
              "IBE.MC  -0.717544  \n",
              "IDR.MC  -0.389893  \n",
              "ITX.MC  -0.794829  \n",
              "LOG.MC  -0.637817  \n",
              "MAP.MC  -0.652834  \n",
              "MEL.MC  -0.805748  \n",
              "MRL.MC  -0.841953  \n",
              "MTS.MC   1.000000  \n",
              "NTGY.MC -0.678992  \n",
              "RED.MC  -0.638158  \n",
              "REP.MC  -0.425741  \n",
              "ROVI.MC -0.634099  \n",
              "SAB.MC  -0.557277  \n",
              "SAN.MC  -0.621048  \n",
              "SCYR.MC -0.603580  \n",
              "SLR.MC  -0.803962  \n",
              "TEF.MC  -0.740400  \n",
              "UNI.MC  -0.731380  \n",
              "Bench   -0.731380  "
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_DATA_TRAIN[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BhlDIX9nbvAO"
      },
      "outputs": [],
      "source": [
        "X_DATA_TRAIN = np.array(X_DATA_TRAIN)\n",
        "Y_DATA_TRAIN = np.array(Y_DATA_TRAIN)\n",
        "\n",
        "X_DATA_VALID = np.array(X_DATA_VALID)\n",
        "Y_DATA_VALID = np.array(Y_DATA_VALID)\n",
        "\n",
        "#X_DATA_TEST = np.array(X_DATA_TEST)\n",
        "#Y_DATA_TEST = np.array(Y_DATA_TEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DyRB_k6PfkMX",
        "outputId": "6e87894a-4cdd-4a58-c6af-d9fd416855df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1088, 35, 8), (1088, 35), (278, 35, 8), (278, 35))"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_DATA_TRAIN.shape, Y_DATA_TRAIN.shape, X_DATA_VALID.shape, Y_DATA_VALID.shape,  #X_DATA_TEST.shape, Y_DATA_TEST.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aZpAzdeclGAz",
        "outputId": "ef617ffa-1c6f-4524-f6f0-da75be03d94e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "X_DATA_TEST_OK , Y_DATA_TEST_OK , X_DATA_POS_TEST = create_dataset(ibex_data.loc[\"2021-12-30\":], create_suffle_data = False)\n",
        "\n",
        "X_DATA_TEST_OK = np.array(X_DATA_TEST_OK)\n",
        "Y_DATA_TEST_OK = np.array(Y_DATA_TEST_OK)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mu_6a8wUlenf",
        "outputId": "0d74704b-0987-4fdc-d675-62627d82df6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((174, 35, 8), (174, 35))"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_DATA_TEST_OK.shape , Y_DATA_TEST_OK.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VSTW8j2qq5Xf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cmy8IAmXWH9"
      },
      "source": [
        "# 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SL71gGUIXe8d"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from tensorflow.keras.layers import Dense, GRU, Dropout, Flatten, Conv1D\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HoYOu_hcXZU9",
        "outputId": "55762fb1-4c64-4605-bd4e-029dd93c2705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 35, 8)]           0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 26, 1)             81        \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 26)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 35)                945       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,026\n",
            "Trainable params: 1,026\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(X_DATA_TRAIN.shape[1:]))\n",
        "m = inputs\n",
        "\n",
        "m = Conv1D(1, 10 , activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=0.1, l2=0.1))(m)\n",
        "A = Dropout(0.2)(m)\n",
        "m = Flatten()(m)\n",
        "\n",
        "# A침adir capa Dense de salida\n",
        "out = Dense(Y_DATA_TRAIN.shape[1], activation='tanh')(m)\n",
        "\n",
        "model_GRU_8 = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_GRU_8.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse',\n",
        "        metrics=[])\n",
        "\n",
        "model_GRU_8.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oPe1nA1PHZVa"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UJaLILxtXrrJ",
        "outputId": "b853299c-05a5-432a-f3f6-7858b7afbbc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 612ms/step - loss: 1.8384 - val_loss: 1.7263\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7027 - val_loss: 1.6192\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.5762 - val_loss: 1.5161\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.4570 - val_loss: 1.4196\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.3473 - val_loss: 1.3283\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.2462 - val_loss: 1.2426\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 1.1532 - val_loss: 1.1624\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.0681 - val_loss: 1.0885\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.9915 - val_loss: 1.0227\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.9248 - val_loss: 0.9602\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.8630 - val_loss: 0.8978\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.8027 - val_loss: 0.8380\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7464 - val_loss: 0.7824\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.6953 - val_loss: 0.7295\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6480 - val_loss: 0.6845\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6091 - val_loss: 0.6454\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5764 - val_loss: 0.6076\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.5448 - val_loss: 0.5742\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.5174 - val_loss: 0.5454\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4943 - val_loss: 0.5224\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4761 - val_loss: 0.5019\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4596 - val_loss: 0.4817\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4421 - val_loss: 0.4660\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4288 - val_loss: 0.4562\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4218 - val_loss: 0.4511\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4195 - val_loss: 0.4467\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4178 - val_loss: 0.4429\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4167 - val_loss: 0.4406\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4162 - val_loss: 0.4400\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4164 - val_loss: 0.4383\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4137 - val_loss: 0.4362\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4094 - val_loss: 0.4312\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4019 - val_loss: 0.4307\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3985 - val_loss: 0.4288\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3936 - val_loss: 0.4255\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3873 - val_loss: 0.4205\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3789 - val_loss: 0.4195\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3740 - val_loss: 0.4212\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.3720 - val_loss: 0.4219\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.3706 - val_loss: 0.4217\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3697 - val_loss: 0.4217\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3698 - val_loss: 0.4200\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3678 - val_loss: 0.4186\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.3654 - val_loss: 0.4174\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3638 - val_loss: 0.4144\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3618 - val_loss: 0.4114\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3598 - val_loss: 0.4097\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3589 - val_loss: 0.4065\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.3568 - val_loss: 0.4026\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3541 - val_loss: 0.3990\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3514 - val_loss: 0.3987\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3512 - val_loss: 0.3995\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3521 - val_loss: 0.3973\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3511 - val_loss: 0.3942\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3489 - val_loss: 0.3914\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3463 - val_loss: 0.3910\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3460 - val_loss: 0.3902\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3451 - val_loss: 0.3905\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3452 - val_loss: 0.3890\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3442 - val_loss: 0.3893\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3444 - val_loss: 0.3895\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3435 - val_loss: 0.3891\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3434 - val_loss: 0.3872\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3427 - val_loss: 0.3866\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3414 - val_loss: 0.3889\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.3414 - val_loss: 0.3906\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3416 - val_loss: 0.3900\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3413 - val_loss: 0.3907\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3409 - val_loss: 0.3950\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3401 - val_loss: 0.3979\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3410 - val_loss: 0.3948\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3406 - val_loss: 0.3923\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3404 - val_loss: 0.3930\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3391 - val_loss: 0.3954\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.3383 - val_loss: 0.3942\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3378 - val_loss: 0.3907\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3376 - val_loss: 0.3898\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3377 - val_loss: 0.3928\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3362 - val_loss: 0.3944\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3359 - val_loss: 0.3913\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.3360 - val_loss: 0.3906\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.3350 - val_loss: 0.3949\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3350 - val_loss: 0.3951\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.3354 - val_loss: 0.3911\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.3349 - val_loss: 0.3909\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3334 - val_loss: 0.3944\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3330 - val_loss: 0.3950\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3328 - val_loss: 0.3913\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.3322 - val_loss: 0.3921\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.3315 - val_loss: 0.3972\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.3320 - val_loss: 0.3965\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.3317 - val_loss: 0.3946\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.3309 - val_loss: 0.3974\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.3310 - val_loss: 0.3982\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.3299 - val_loss: 0.3964\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3294 - val_loss: 0.3976\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.3297 - val_loss: 0.4001\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.3294 - val_loss: 0.3984\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3289 - val_loss: 0.3975\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.3298 - val_loss: 0.3995\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.3285 - val_loss: 0.4001\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3278 - val_loss: 0.3992\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3275 - val_loss: 0.3998\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3271 - val_loss: 0.4018\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.3265 - val_loss: 0.4033\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3266 - val_loss: 0.4018\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.3263 - val_loss: 0.4017\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.3255 - val_loss: 0.4056\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.3256 - val_loss: 0.4040\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.3246 - val_loss: 0.3996\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.3237 - val_loss: 0.4029\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.3235 - val_loss: 0.4079\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3245 - val_loss: 0.4046\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3231Restoring model weights from the end of the best epoch: 64.\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.3231 - val_loss: 0.4073\n",
            "Epoch 114: early stopping\n"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience = 50, verbose=1, restore_best_weights=True)\n",
        "\n",
        "h_gru_8 = model_GRU_8.fit(\n",
        "          X_DATA_TRAIN,\n",
        "          Y_DATA_TRAIN,\n",
        "          epochs=200,\n",
        "          batch_size= X_DATA_TRAIN.shape[0] ,\n",
        "          validation_data = (X_DATA_VALID, Y_DATA_VALID),\n",
        "          verbose = True,\n",
        "          callbacks=[early_stopping]\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0BlooqElynWx",
        "outputId": "bd93350a-9865-48d7-c58f-a09350b9706a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 35), dtype=float32, numpy=\n",
              "array([[ 0.45884717,  0.05296531,  0.02859236, -0.13872175, -0.09214912,\n",
              "         0.02011112,  0.01668498,  0.01011404, -0.14132789, -0.12000493,\n",
              "         0.05954735, -0.12190435, -0.02493406, -0.03161812,  0.07936045,\n",
              "        -0.10175996,  0.08543486, -0.03595542,  0.05399216,  0.0663781 ,\n",
              "         0.08317406, -0.0074438 , -0.00720908, -0.06352046, -0.03167765,\n",
              "        -0.12222033,  0.0702366 , -0.0701272 ,  0.04310995,  0.13637428,\n",
              "        -0.06139449, -0.2057361 ,  0.06510431, -0.02893617, -0.05796077]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_GRU_8(X_DATA_TRAIN[0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o-Fv51Saz-aQ",
        "outputId": "0ea068ee-e5ee-42b6-efbf-9a47af5e8f54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 35), dtype=float32, numpy=\n",
              "array([[ 0.44656616,  0.0620284 ,  0.01551911, -0.13388574, -0.10146765,\n",
              "         0.03007102,  0.02326925,  0.02111371, -0.14574313, -0.12162048,\n",
              "         0.04680298, -0.1235458 , -0.02954776, -0.04068292,  0.07116564,\n",
              "        -0.0929661 ,  0.07254351, -0.02341052,  0.05553114,  0.07210605,\n",
              "         0.08619311, -0.00049019, -0.00681764, -0.05954761, -0.04347677,\n",
              "        -0.12289847,  0.07442003, -0.0784722 ,  0.05122385,  0.13752593,\n",
              "        -0.06638834, -0.21546988,  0.07245187, -0.02058756, -0.05425608]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_GRU_8(X_DATA_TRAIN[1:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BUn2R0sukjIU",
        "outputId": "9b48a247-de66-4ceb-e6f0-45d4d2cd2f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 3ms/step - loss: 0.4328\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.43282952904701233"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eva = model_GRU_8.evaluate(X_DATA_TEST_OK, Y_DATA_TEST_OK)\n",
        "eva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KOwMTmdX1yeA",
        "outputId": "47cd0d73-2a82-4cd1-fd97-82b789e55e52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.43283"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "round(eva, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ZAqvuCaZWgI",
        "outputId": "c1973b9b-ce2b-4b7a-9829-d0d5e7631725"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b12350e0640>]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(h_gru_8.history[\"loss\"])\n",
        "plt.plot(h_gru_8.history[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJHwOzX_ZmL3"
      },
      "source": [
        "# 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cxRYuQd_Zm-3",
        "outputId": "e43cb8d0-430e-408a-fd6e-c8bf999dcd9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 35, 8)]           0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 35, 100)           900       \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 3500)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 35)                122535    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 123,435\n",
            "Trainable params: 123,435\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(X_DATA_TRAIN.shape[1:]))\n",
        "m = inputs\n",
        "\n",
        "mA = Dense(units=100, activation='tanh')(m)\n",
        "\n",
        "m = Flatten()(mA)\n",
        "\n",
        "# A침adir capa Dense de salida\n",
        "out = Dense(Y_DATA_TRAIN.shape[1], activation='tanh')(m)\n",
        "\n",
        "model_GRU_9 = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_GRU_9.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse',\n",
        "        metrics=[])\n",
        "\n",
        "model_GRU_9.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-fdXLM2muR0"
      },
      "source": [
        "# ---- VER PREDICCIONES ---- > > >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWMCcLCamz9x"
      },
      "source": [
        "# 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHd7BGhSp5J5"
      },
      "source": [
        "## vemos una prediccion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WhTqXHHCmwcO"
      },
      "outputs": [],
      "source": [
        "x_eje = X_DATA_TEST_OK[35:36]\n",
        "y_eje = Y_DATA_TEST_OK[35:36]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z9LEB2jKmti3",
        "outputId": "4f8240e2-d205-4d28-b489-d57ac52bf97d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 35), dtype=float32, numpy=\n",
              "array([[ 0.30467546,  0.00934902,  0.0318145 , -0.10692504, -0.168754  ,\n",
              "         0.12327385,  0.0534845 ,  0.05469586, -0.23827766, -0.07155599,\n",
              "        -0.11246741, -0.10270008,  0.01281512, -0.20399679,  0.03016939,\n",
              "         0.0354912 , -0.15591517,  0.11243656,  0.05209925, -0.04551917,\n",
              "         0.05457422,  0.11521752, -0.02438232,  0.00159059, -0.05852231,\n",
              "        -0.14591996,  0.15926868, -0.1391583 ,  0.22985   ,  0.15637974,\n",
              "        -0.09190354, -0.19819763,  0.12627198,  0.06848525,  0.03834023]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = model_GRU_8(x_eje)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5fQdyairo7cg",
        "outputId": "06ff0ede-1984-41ff-b32d-3806bd09f472"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.305,  0.009,  0.032, -0.107, -0.169,  0.123,  0.053,  0.055,\n",
              "       -0.238, -0.072, -0.112, -0.103,  0.013, -0.204,  0.03 ,  0.035,\n",
              "       -0.156,  0.112,  0.052, -0.046,  0.055,  0.115, -0.024,  0.002,\n",
              "       -0.059, -0.146,  0.159, -0.139,  0.23 ,  0.156, -0.092, -0.198,\n",
              "        0.126,  0.068,  0.038], dtype=float32)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred[0].numpy().round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "df3pPeQ8nIV5",
        "outputId": "8fd57b65-189a-43db-b242-7f73257447d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0, 18, 15, 26, 31,  5, 11,  9, 34, 23, 27, 25, 17, 33, 16, 14,\n",
              "        30,  7, 12, 21, 10,  6, 20, 19, 22, 29,  2, 28,  1,  3, 24, 32,\n",
              "         4,  8, 13]])"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ranking_pred = np.argsort(np.argsort(-y_pred))\n",
        "ranking_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KhTy7F6qntoC",
        "outputId": "df207b7a-126b-4222-b6bd-48cc1da78d47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.23529412,  0.58823529,  1.        , -0.70588235, -0.76470588,\n",
              "         0.05882353,  0.82352941,  0.70588235, -0.88235294, -0.17647059,\n",
              "        -0.11764706,  0.64705882,  0.17647059, -0.94117647,  0.47058824,\n",
              "        -0.29411765, -0.58823529,  0.35294118, -0.82352941, -1.        ,\n",
              "         0.94117647,  0.52941176, -0.35294118,  0.29411765, -0.47058824,\n",
              "        -0.64705882,  0.41176471, -0.52941176,  0.88235294,  0.76470588,\n",
              "         0.        , -0.41176471,  0.11764706,  0.23529412, -0.05882353]])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_eje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0Do4_o9-nvfe",
        "outputId": "65bc0c95-f439-4b40-c427-9388eef8f5d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[21,  7,  0, 29, 30, 16,  3,  5, 32, 20, 19,  6, 14, 33,  9, 22,\n",
              "        27, 11, 31, 34,  1,  8, 23, 12, 25, 28, 10, 26,  2,  4, 17, 24,\n",
              "        15, 13, 18]])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ranking = np.argsort(np.argsort(-y_eje))\n",
        "ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3HGYSzA0p-cb",
        "outputId": "664ba642-7f81-4cf4-aec1-b248f9ff228a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 21, -11, -15,   3,  -1,  11,  -8,  -4,  -2,  -3,  -8, -19,  -3,\n",
              "          0,  -7,   8,  -3,   4,  19,  13,  -9,   2,   3,  -7,   3,  -1,\n",
              "          8,  -2,   1,   1,  -7,  -8,  11,   5,   5]])"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ranking - ranking_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OWbCn6KNq1BA",
        "outputId": "9adebe9f-db31-421b-938b-25e883caf6f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0,  5,  7, 17, 21, 26, 28, 29, 32, 33])"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.where(ranking_pred < 10)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qFFTx53nrs8s",
        "outputId": "1ee6b425-2d6e-450f-a190-fed96a4753af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1,  2,  6,  7, 11, 14, 20, 21, 28, 29])"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.where(ranking < 10)[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vvwaJALth5h"
      },
      "source": [
        "cuantos activos del ranking real estan dentro del ranking predicho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0EPXCqIztNLN",
        "outputId": "ae066e18-d99c-483d-b426-437d7e50ba15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len (\n",
        "    set(np.where(ranking_pred < 10)[1]) & set(np.where(ranking < 10)[1])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbVTOHNtu1_"
      },
      "source": [
        "como de lejos estan los activos del top 10 de su posicion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SJgeVm_2tqA4",
        "outputId": "40d0f85c-b177-4b48-e25f-748508973fa5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 5, 9, 7, 6, 2, 1, 3, 4, 8])"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "puntos_ranking_pred =  ranking_pred[0][np.where(ranking_pred < 10)[1]]\n",
        "puntos_ranking_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5uRKPdoJt8-N",
        "outputId": "a7b6afef-26df-462a-f1b3-850de807f698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([21, 16,  5, 11,  8, 10,  2,  4, 15, 13])"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "puntos_ranking_real = ranking[0][np.where(ranking_pred < 10)[1]]\n",
        "puntos_ranking_real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eeXm9pmVuKEd",
        "outputId": "a4d3ba76-0e9e-4bfc-ff9a-e278a21c4792"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6.8"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abs(puntos_ranking_pred - puntos_ranking_real).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM8xshQ5uxIE"
      },
      "source": [
        "## ahora con todo el dataset de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uNnbNNjgu4eG",
        "outputId": "96626ae8-7e84-49a0-a69d-e0ede7af7123"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(174, 35, 8)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_DATA_TEST_OK.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IcNVHloYupoU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def ver_comportamiento(MODELO,X_ANALSIS, Y_ANALISIS  ):\n",
        "  top_activos = 10\n",
        "\n",
        "  t_activos_in_top = []\n",
        "  t_evaluacion = []\n",
        "  t_distancia_media_de_top = []\n",
        "\n",
        "  MODELO = model_GRU_8\n",
        "  X_ANALSIS = X_DATA_TEST_OK\n",
        "  Y_ANALISIS = Y_DATA_TEST_OK\n",
        "\n",
        "  for i in range(X_ANALSIS.shape[0]):\n",
        "    print(X_ANALSIS[i:i+1].shape)\n",
        "\n",
        "    y_pred = MODELO(X_ANALSIS[i:i+1])\n",
        "    ranking_pred = np.argsort(np.argsort(-y_pred))\n",
        "\n",
        "    ranking = np.argsort(np.argsort(-Y_ANALISIS[i:i+1]))\n",
        "\n",
        "    # cuantos activos del ranking real estan dentro del ranking predicho\n",
        "\n",
        "    activos_in_top = len ( set(np.where(ranking_pred < top_activos)[1]) & set(np.where(ranking < top_activos)[1]) )\n",
        "\n",
        "    evaluacion = MODELO.evaluate(X_ANALSIS[i:i+1], Y_ANALISIS[i:i+1] )\n",
        "\n",
        "    # como de lejos estan los activos del top 10 de su posicion\n",
        "\n",
        "    puntos_ranking_pred =  ranking_pred[0][np.where(ranking_pred < top_activos)[1]]\n",
        "    puntos_ranking_real = ranking[0][np.where(ranking_pred < top_activos)[1]]\n",
        "\n",
        "    puntos = abs(puntos_ranking_pred - puntos_ranking_real).mean()\n",
        "\n",
        "\n",
        "    t_activos_in_top.append(activos_in_top)\n",
        "    t_evaluacion.append(evaluacion)\n",
        "    t_distancia_media_de_top.append(puntos)\n",
        "\n",
        "  return t_activos_in_top, t_evaluacion, t_distancia_media_de_top\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG8Nd_7KCeqS"
      },
      "outputs": [],
      "source": [
        "MODELO = model_GRU_8\n",
        "X_ANALSIS = X_DATA_TEST_OK\n",
        "Y_ANALISIS = Y_DATA_TEST_OK\n",
        "\n",
        "t_activos_in_top, t_evaluacion, t_distancia_media_de_top = ver_comportamiento(MODELO,X_ANALSIS, Y_ANALISIS  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tydWrPC6xTyK",
        "outputId": "960ce2c8-729c-461d-e958-6a599270c7da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b12347e0d90>]"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABomklEQVR4nO29eZgc9XX++1avs/Vo3wYtiN0skkEYWcbGCzKgYBsvlzgYBxkTHNsiXpT4R+TnepGTWMTcOIkTgu38MPheh8XkGvBybcKOMQKEQLYBI0tCQoAkhASa6emZ6bXuH93f6m9VV3UtXdVd1f1+nocHTW/1nZ5aTp3znvcoqqqqIIQQQgjxgVinF0AIIYSQ7oGBBSGEEEJ8g4EFIYQQQnyDgQUhhBBCfIOBBSGEEEJ8g4EFIYQQQnyDgQUhhBBCfIOBBSGEEEJ8I9HuDVYqFezbtw+ZTAaKorR784QQQgjxgKqqyGazGBkZQSxmnZdoe2Cxb98+LFq0qN2bJYQQQogPvPTSS1i4cKHl820PLDKZDIDqwoaHh9u9eUIIIYR4YGxsDIsWLdKu41a0PbAQ5Y/h4WEGFoQQQkjEsJMxULxJCCGEEN9gYEEIIYQQ32BgQQghhBDfYGBBCCGEEN9gYEEIIYQQ32BgQQghhBDfYGBBCCGEEN9gYEEIIYQQ32BgQQghhBDfcBVYHH300VAUpeG/devWBbU+QgghhEQIV5beW7ZsQblc1n5+5pln8N73vhcXX3yx7wsjhBBCSPRwFVjMmTNH9/M111yDY489Fu985zt9XRQhhBBCoonnIWSFQgE/+tGPsH79+qYDSfL5PPL5vPbz2NiY102SiKGqKn746B68efEMvHnR9Kav/e1LR/DT3+5DRVWhQMGfnDYfZx49sz0LJYQQ4hueA4s777wTR44cwSc+8Ymmr9u0aRM2btzodTMkwvz25VF8/WfPYdnCafjpVW9v+tqv3PUMfvfyqPbzr3e8hnvWMxNGCCFRw3NXyA033IA1a9ZgZGSk6es2bNiA0dFR7b+XXnrJ6yZJxDgyUQAAjE0WbV97KFvNap138rzqz+P5Zi8nhBASUjxlLF588UXce++9+MlPfmL72nQ6jXQ67WUzJOIUShUAQKmi2r52PF8CAHzibUfjf557Fbl82eYdhBBCwoinjMWNN96IuXPn4sILL/R7PaSLKJRrgUW5eWChqqoWWMyf1qe9N19icEEIIVHDdWBRqVRw4403Yu3atUgkPEs0SA9Qz1hUmr5usliGSGrMG+7THh+fKgW2NkIIIcHgOrC49957sXfvXnzyk58MYj2ki3BaChEBRDymYCAVx1C6GrCKLAYhhJDo4DrlcN5550FV7WvmhDgthWRrAcRQOgFFUTCUTmA8X0KWGQtCCIkcnBVCAsNpKURkLESmYqiPGQtCCIkqDCxIYORLzjIW43l9YDEoSiHMWBBCSORgYEECQ9ZYNCufiZKHyFRkqLEghJDIwsCCBIbQWABAuYmA05ixEP/PMrAghJDIwcCCBIbIWADNO0PGp6rOnCJjoWksWAohhJDIwcCCBIYcWBTL1gJOkbHIGDIWOWYsCCEkcjCwIIEhBxbNSyFVh00RUGTYFUIIIZGFgQUJDFljUWzSGTKeN5RChMaCpRBCCIkcDCxIYDjOWFj6WNhPRSWEEBIuGFiQwNBnLBxoLAwZC5ZCCCEkejCwIIHhtCtE87FIJwFIGguWQgghJHIwsCCBoS+F2GcsBtPx6v9T9LEghJCowsCCBIZz8aahFMKMBSGERBYGFiQwdKWQZoGFsRRS+z81FoQQEj0YWJDA0GssrEsh2th0Q8ZiolBu2k1CCCEkfDCwIIEhl0KsxJv5UlkLQOrTTePa87kCsxaEEBIlGFiQwHBSCsnVXDeBemCRTsSRSlR3TeosCCEkWjCwIIGRd1AKEfNABlJxxGOK9jhHpxNCSDRhYEECo1CqZyOsMhZZg+umQOgsaOtNCCHRgoEFCQwnGotxg3BTQPdNQgiJJgwsSGDoNRbmpRAxDyRjzFik6WVBCCFRhIEFCYRSuQI5SVG0yFiIUsegVWDBQWSEEBIpGFiQQCgYMhRWlt5aKYQaC0II6QoYWJBAkMsggLWlt+a6SY0FIYR0BQwsSCAYAwsrB01tTohFxoIaC0IIiRYMLEgg5A2BhZV4M2uRsRCBBp03CSEkWjCwIIFg1FhYlkLy+gFkAlEKocaCEEKiBQMLEgiOSyFWGos+TjglhJAowsCCBEKDeNPK0rtgobGgjwUhhEQSBhYkEIplo8bCnaV3po9dIYQQEkUYWJBAMGYsvFp6U2NBCCHRgoEFCYR8Q8bCwiDLZggZMxaEEBItGFiQQHDrY9EQWEgGWapq/l5CCCHhg4EFCQQnzpuVimpbCilXVEwVzbMdhBBCwgcDCxIIjRqLxuBANr8yZiwGUnEoSvXfWQ4iI4SQyMDAggSC0SDLTLwpshXJuIJ0Qr8rKoqiBRu5fDmgVRJCCPEbBhYkEBoyFibiTVm4qYj0hESGXhaEEBI5XAcWr7zyCj7+8Y9j1qxZ6O/vx2mnnYYnn3wyiLWRCNMYWDRmLLIW+gqBNjqdpRBCCIkM5md0C9544w2cffbZePe7341f/vKXmDNnDnbs2IEZM2YEtT4SUZyUQnIWc0IEdN8khJDo4Sqw+Md//EcsWrQIN954o/bY0qVLfV8UiT4N001NxJsiYDDaeQs4L4QQQqKHq8Dipz/9Kc4//3xcfPHFeOihh3DUUUfhs5/9LK688krL9+TzeeTzee3nsbEx76slDeRLZdzwyG68lq1+x3MyafzF249BKtEe+czPf7cP6UQc7z15nu5xUQrpT8YxWSx7KoVk0jTJIoSQqOEqsHjhhRdw/fXXY/369fjyl7+MLVu24HOf+xxSqRTWrl1r+p5NmzZh48aNviyWNPLwHw/hW7/arnvs2DlDOP+U+YFvO5cv4fO3bkMipuCZjecjGa8HMyKwGEzXAguzrhAL100Bbb0JISR6uLqtrVQqOOOMM/DNb34Tp59+Oj71qU/hyiuvxHe/+13L92zYsAGjo6Pafy+99FLLiyZ1jkwUAABLZg3gmDmDAIBD4/lmb/GNXL6EckVFvlTR9BKCQrnaIjqQqgYHxqFkQD0TMWgRWAwyY0EIIZHDVWCxYMECnHzyybrH3vSmN2Hv3r2W70mn0xgeHtb9R/xDiCRPnJfBmxdOB4CGi3xQyDoK48VfZCwGUnEA5pbe4j0Zm64QijcJISQ6uAoszj77bGzfrk+7//GPf8SSJUt8XRRxjriApxKxtl+I5c4Pu8DCVGNhUwqhxoIQQqKHq8Dii1/8Ih577DF885vfxM6dO3HzzTfj+9//PtatWxfU+ogNusBCaBLadCGWvSqMwYwIOrRSiFlXiMUAMgEnnBJCSPRwFVi85S1vwR133IFbbrkFp556Kv7u7/4O//Iv/4JLL700qPURG8TFPd2JjIUUWBiDGUelkKmq8ZWlQRZ9LAghJHK46goBgPe973143/veF8RaiAdEZiAVj7W9dKArhRgu/nlDYGE23VTTWDBjQQghXQNnhUQcU41FJ0ohhm2KLpD+lBh/3lgK0TQW9LEghJCugYFFxMnrNBZVp8p2+T401VgIH4sm4k0xNt1OY0EfC0IIiQ4MLCKOKEck4zEMpqsX8Xbd4eebaSyEeDPdRLw5ZdNuqmUsOISMEEKiAgOLiCOXQjK1jEVH2k0tMhaaeNOQsVBVVeoKMR9CJn6fqWLF1GCLEEJI+GBgEXG0wCJe11i0yyBLr7Eomj4nSiFFQ1dIvlTRBJ0i02JEfrxdvxMhhJDWYGARcXTtpqJ0UCihYtLeGdS2gWYGWdU1lQwZB/n1gynzUkgiHkNfsrqLUmdBCCHRgIFFxNHaTRMxTaugqsBEsRz8tkv1bYzn9durG2TVxJuGQEceQBaLKZbbEGUSdoYQQkg0YGARcWSNRToRQ6J2kW6HzkKvsdCXQjQfi7TIWBgCCxvXTUGmzeUdQgghrcHAIuLUNRZxKIoieVkE30nhrBRi7rxp52EhaLdNOSGEkNZgYBFx5FIIIF2I25GxsPCxUFW1oRRibDd1mrGgrTchhEQLBhYRRy6FALL3Q/AX4nzZ3MeiVFGh1hIUQpipqvqshcioWHlYCGjrTQgh0YKBRcSRZ4UA9Qt1WzQWhlKIWosm5MdFxgIASpXGDIetxoIZC0IIiRQMLCKOVcaiHZoEOYBQVWCiUG54fEAKHGQBp+gisS2F9FFjQQghUYKBRcSRfSwAYDDdvi4KOYAA6uUKkUWJKfV1AcbAovnIdAE1FoQQEi0YWEQco3iznaUQo822EIzKWZSE5FHhpRQyyHkhhBASKRhYRBzZ0htor3izYOGmmZfWpCgK4rXgQjbJyrr0saB4kxBCogEDi4jTqLGojU7vRCmkIWNRFW4mTAKLcbc+FiyFEEJIJGBgEWFkvwgtsGhjKSRvo7EQ+opkLZsizwtx62NB501CCIkGDCwijFyK0DQW7SyFWAUWhiyKKIUUdeLN6mvpY0EIId0FA4sII1/YNY1FO30sDNkSMS+kaPDWSMargUXZrBRSK91YkRFDyFgKIYSQSMDAIsKYBhYd8LGYNZgC4CRj0ejUSR8LQgjpLhhYRBiRMUjEFG30eCeGkM2sBRZZY1dILbBIxGoaC5OMhW0pRCrtCGdPQggh4YWBRYQxZgaAusYiV3O2DHT7ZX1goXWFWJZCqo+XyhVMFp05b4rAQ3b2JIQQEl4YWEQYs8CirRoL16WQasYhJwUIgzaBRVoy2aKAkxBCwg8DiwiTN5hjAfULdaFcQb4U7B1+vRSSBmDmY6FvNxXiTREgpBIxXVBkhqIo2u9ELwtCCAk/DCwijLErA6iPKQeCz1poGYshvcaiUAtoNI1FXC/e1PQVNtkKQTvdRAkhhLQGA4sIY1YKiccUDNZGlQd9Ic7baCzScVEKEQZZImPhbACZoJ3zTwghhLQGA4sIY5wTItBaNAO8EKuqqm1/xoC5xkKUQJIGS++swwFkAmYsCCEkOjCwiDDGkemCdlyIZRdNkbHIWYg3RSlETDd1auctoPsmIYREBwYWEaZoorEAgKG+4N0qZTvxBh+LsoWPhSiFOPSwEGiB0hRHpxNCSNhhYBFhzMSbQHvmhciun6LdtFCqdqJYZyz0XSFOMxYcnU4IIdGBgUWEMWs3BdpTChHBQzymYLi/Pu8jly83aD+0sem1QCjrcGS6oJ025YQQQlqDgUWEMesKAdqjSZCDh3hMwYDoRJkqNWYsDJbe9YxF8wFkgiEOIiOEkMjAwCLC1C/gcd3jdU1CkBoLvVdFPatQrLebGkshRh8LpxkLlkIIISQyMLCIMMaZHIJ2aBKMg8ZkK/HGjIVBY1Gorkv4bdgxlK5nQwghhIQbBhYRxrIU0gYLbKOOQhaMNmgs4oZSiKaxcFcKocaCEELCDwOLCGPlYzGoXeSDa880bntQDiwM3SpJYynEq48FMxaEEBJ6XAUWX//616Eoiu6/k046Kai1ERss203bId40bFvOkhjLJMbppl59LHIFBhaEEBJ2nJ3ZJU455RTce++99Q9IuP4I4hOWlt5tEG8azblkgWVju6n5dFPXPhbMWBBCSOhxHRUkEgnMnz8/iLUQlxgzA4J2+D5YaiymSg1BhxBvFivCx8LdEDL6WBBCSHRwHVjs2LEDIyMj6Ovrw6pVq7Bp0yYsXrzY8vX5fB75fF77eWxszNtKSQN2Phavjk5h48+exUAqjrVvOxpzM32OPveNXAE3PrpHCwCOnTOEj791ie41Vl0hD2w/iFeOTOqeE+LNclmFqqpaxsLx2PTaZwtnz3SisZvk5sf34pg5g3jrMbMcfSYhfnNgdAp3PP0K/uwtizCj5kYbNI/uOoR7nnvV0Wv7k3H8+aolWDCtP7D1jOdL+NFjL+JPTl2AxbMGAtsOCTeuAouVK1fipptuwoknnoj9+/dj48aNeMc73oFnnnkGmUzG9D2bNm3Cxo0bfVks0SN0DklDKWTOUBoAkCuUceNv9gAAVBX4Xxc408PcuuUlfOe+HbrHVi6diePn1f/GxqBGBC3P7qsHjmKGSFKy9J4sllGriDjOWAym6q/L5RsDi50Hx/HlO36PJbMG8NCX3u3oMwnxmxseeQH/+evdiCnAX77z2LZs869//FvsH51y/PpCqYL/830nB7aen27bh2t++Tx2HRzHtRcvD2w7JNy4CizWrFmj/XvZsmVYuXIllixZgh//+Me44oorTN+zYcMGrF+/Xvt5bGwMixYt8rhcIlMo6U2qBHOH+3Ddx87Ac/tH8dSLR7D5hcM4NJ43+whTxGvPWDwdLx6ewOFcAa+N5/WBhcFD4yMrFiJfKmN0sprlWDJrECfWXl8Xb1Y0nURMqd5BOSEeUzCYiiNXKGN8qqQFLMb1im0T0gmOTFT3v3buh2Lf/8TbjsZg2vp4EueBoNcm1hNkqzsJPy0pL6dPn44TTjgBO3futHxNOp1GOp1uZTPEAq3lM97Y3HPhsgW4cNkC/PDRPdj8wmHk8mXHnyvGn7/npLm49w8HcThXaHi/MWMxlE7gU+eY36WJjEq5omo6iaF0AoqiOF7TUF8CuUIZWZMWWuO4dkI6gQi227Uf5ktlrdNq/XknYLiJL8z//vUL2PzCYd1U4iAQx6LwrCG9SUs+FuPj49i1axcWLFjg13qIC6zaTWW8CB/li3+9dVV/QbfSd5iRkNpNNXMsh/oKwWCTTpdxBhYkBIj9L+iLt0A+FuRyoRnCbyboYySrBRY8FnsZV4HF3/zN3+Chhx7Cnj178Oijj+JDH/oQ4vE4LrnkkqDWR5rg5OJeN5dyngKVnTGtWletWl3NiGuW3hXtjsapvkLQbBS8SLuWKioqvFMiHUILLNoU4IpjYTAV144xK1JtCizEeaJU5nHYy7g6u7/88su45JJLcPjwYcyZMwdvf/vb8dhjj2HOnDlBrY80wcnFvdkF2QrZZ8Iq4+EkWyJISpbeWZceFoJmg8jkxwrlCvpizrQbhPhJu0sh2SnnQboWWAScTRlnxoLAZWBx6623BrUO4gErHwsZL3bYsjOm1fvdZCzk6aZu54QIhpoESPLa8qUK+hyKQgnxE3E85ttVCnERpKfi1WMiz4wFaQOcFRJhjEZUZnjRWMgnLKuMh5OgRiA0FuWKew8LgRhE1kxjAVBnQTpH20shLoL0dpVCshRvEjCwiDSOxJtSCUFVnR3ssjOmlWjSTSlEWHoXy6prO29Bs/kncmtbu4RzhBjplMbCSZDeNo1FTeTNUkhvw8AiwjgpR4gLuKoCEwX7llPZGXMoXS+FNGgs3GQs4nXxppu6sEyzUfByxwozFqRTtF1jIcSbTfwrBOIcEbjGgqUQAgYWkcZqbLpMfzIOIRh3IuDUOWNK4s2WNBa1jEWprGpBQGDiTQYWpEN0qt1UlAmb0b6MBUshhIFFpHGSNVAUpendvhFxslIUYCAV10oQxpHlToIaQUKy9HY7Ml3QbGLruGTexcCCdIr2l0KqQbqTY0nLWAS4Ntmwq8SSZE/DwCLCONU5ZGriLicZC7kMUg1KzEWT7jQWUldIABoL2aOjUHbuMEqIn7S7FCLccB11hbSh3VQ+RxRZCulpGFhElEpF1Q5eu3KEOPHkXAQWQhBm6WPhSmMh+VhMibqwS+fNlHV3ixxsBN1OR4gV7S6FePKxCPD4kI/DMkshPQ0Di4gin7zsLu6aANNFKUS8J2PrY2EvHEtqGQtJGOq2FNLEQVReG0shpFN0qhTiKmMR4Nrk8wu7QnobBhYRxVVg4cJ90+iMKf4/WSzr6qZ5F6UQM0tv9z4W5r9DuaIiV6DGgnQWVVXrpZA2G2S50liUK47bzr2uB2AppNdhYBFR5AuobSnExbwQo+mOXLLImYgk3ZZCvGYsrDInRlEpT2ikE8j7XdsNslxkLIDgAh/52GQppLdhYBFR5HZPu/HjbuaFGDUWqURMOylldX4RZW37diTj9VJI1sXJUEbTiRTKukFjjaJSijdJ+5Ev1u32sXByLMndW0GtT5+xYOawl2FgEVHcZAzc2HqbdW2YBSZuukJEKWSyWNbElRkHvfcycoZDzlIYgyWWQkgnkPe7tvtYuCiFAMEdI1mKN0kNBhYRRZy8RDagGW4GkZl1bZi9342PhZhuOjpZz3g4cQuUSSfi2slRDiaMglQGFqQTyPtduaK25cLqpnU7FlO0tu92lEJKFTUwLQcJPwwsIoqXjIWzUkh9Tkiz97vZflwaQgZU3UATDkooRswCHGMLLdtNSScwBrRBB7jliqpZ9DstKwbdGWI8Fum+2bswsIgobqaLNnOtNKI5Y6abBxZOPTQAIBnTv8atcNO4Djnl2lAKYW2XdACjtqdd1tmA8+NJnCuC0j+YdWyR3oSBRURxM6vDapCYGcIeWz5ZmXVkiO0nXVh6a5/nUrgpMAuQrPw1CGknxkxZPmARsbiIp+IxpBPOyoriXBFUVs9YlqSAs3dhYBFR6uJJ+5OKq4yFiemOMWMh9+w7G0KmDyzcum5arQOwdgQlpJ20uxSS89C2HXQpRJ4yDHDCaS/DwCKiuNE4WA0SM8PMZ8Lo3OnGnAtAg57CbaupcR3MWJCw0e7AwkvbdvCBBTUWpAoDi4iidWU4KYVYDBIzw1xjoR9iJp+Y3Ew31T7PV42F/i6JGgvSCYz7XdD7oZdhfrL7ZiBrmjIGFjwWexUGFhGl6MJHwp3GwiRjUWsNFScON66fQGMpxLPGwixjIWrNbZiFQIgV7c5YuPGwEKQDPkaM5xeWQnoXBhYRxUu7aaFUQb7UXFRmlmI1ahvEHU8ipiAWs/fRSPjUFZJJN5Z0xHpnDaaqa2NgQTpA2wOLWqbOTZAeeCmkIWPBwKJXYWARUfIuxJNykCDP+zBSDTwanTHF3JCsoRTiJKgBGjMWnjUWab3WA6gHOzNrgUWepRDSARpKIe3SWHgRb7ap3bTEY7FnYWARUdwaVA2k9OUMM2SDG9kZU5vT4TGwiMUUyLGFZ41FX2NXSM4QWDBjQTpBY7tpezQWbjqsgmw3lQ27Eto0Y2YsehUGFhHF7cW9Lny0nnAqTlZ9yZiuk8PoY6GZc7lwz9R9Xss+FvXfgaUQEgY6pbEISylELk9O669mOKmx6F0YWEQUr4FFs4xFXWmuHxBmpbFwum1AXw7xrLEwyVjUSyHp6toYWJAO0H6NhZd202oWMoi1ifNKKh5Dfy07WmRXSM/CwCKiCAthp1kDszKCEW1kuuHCX/exqGYK3AY1gCGwcDnZ1Pg+M43FrKFaxoJ1XdIB2q6x8GKQFWC7qdxNJoYO0tK7d2FgEVHcTBcFnA0iG7cw3ZHHpquq6spOXJCUXut2sqnxfbIDqFgzNRakkxj3u6DtrHOeMhbBlULkbjJxE0FL797FW06adBzPGosmpZCsxclK3BVVVGCyWHYd1AD1CaeAvuPEDcZSSL5U0QRiDCxIJ2kohQQt3pwyzy42I1UzqgukFCKdOyq1cenUWPQuzFhEFDezOoB6cGAcbSxjZbrTn4xrXR3jUyVPGgs5Y+HdebPuIKqqqhYkKQowvSYYYymEdIJ2l0Ks9FDNCLLdVD53sBRCGFhEFDdj0wF9OcMKK9MdRVG0trZsvuRNYxGXNRattZuWKirypUr95JpKIJ0MTphGiB3G/S6oCaKClnwsAslY1M8d4lhnKaR3YWARUVyXQvrsSyHNbIIzUleJF42FrhTiMWMxkIxDEZmTfEm33qDnIBDSjM5lLNyUQqrBdxBBjxzoCI0FMxa9CwOLiCJOXEmnpRDDIDEzrDQWgL6UkvdSCqnZeidiiitthkwspmAoVQ9w5JMrZ4WQTtJOjYWqqpYdXM0I1Mei5uhbFW9Wt1NkYNGzMLCIKG51DmYDvIyI58zc/IZMSiFOgxqgnrEY6ktAUezni1ght83KLW5BD1gipBnGDGKQ++FUsaJlA1w5bwapsaiVQob66qUQWnr3LgwsIorbzgxnGgvruyAxL0RXCnEl3qwFFh71Fdo6pO4W7WQmZyx4MiMdQBwT4jgLMrAQ7rmKUi0POqUe9DQfROgF7dwhtZvS0rt3YWARUdzqHOSMgxXN6rZyYOKl3VRYerccWMgZC6l3PiUp0VnbJe1GBLRi/wwysND2+1TC0XRhQToeXDZF52NR2w7bTXsXBhYRxXspxH5WiKnGQg4sXLp+AvVSiFfhZuM6ijpNiPw9sBxC2o3Y58T+GWTmTC4BuiHYUki9jFrPWPA47FUYWEQUz7NCPPhYyI9lWyyFuKkJmyHPPNF1hTCwIB2kIbAIMmPhoSMECLjdVDLsYsaCtBRYXHPNNVAUBV/4whd8Wg5xitdSiJMhZGbOmINSpsDbrBCfSiFSScdY1xWa0HzZ/xoyIc0QnVIiIxekj0WzG4BmpAIshciGXUlmLHoez4HFli1b8L3vfQ/Lli3zcz3EIW4NsrR20ULZUoPg2Meidici+uKdkPCrFNJnnrFQFCXQEychzehIKcRjxiJoH4s4xZs9j6fAYnx8HJdeein+8z//EzNmzPB7TcQBRbcaC+kklCs0Zi0qFRXjBXsfi/EWnTdbzVjIItKsdJckr4eBBWk3otOiLt4MLmvmxcMCqB8fQThiysEOSyHE01l+3bp1uPDCC7F69Wr8/d//fdPX5vN55PN57eexsTEvm+xpbn1iL7a/mtU9NjpZFWE67cxIJ2JIxhUUyyq++Ys/YCidwIfPWIiTR4YBABPFMmqzg8zbTWsX9Gf3jaE/Vc1UuAssRCnE2wAybR21tW3edRgTBf3JPJ2IIQu2nJL2o3WF1PZvEdxOFcu44ZHdODSet3zvUDqBT7ztaMwaSjvaltyB4QY/xJuqquLG3+zBS29MGNZUs/TuS2h6KvpY9C6uA4tbb70VTz31FLZs2eLo9Zs2bcLGjRtdL4xUefFwDn/7k9+bPqcowHC/swu1oiiYm+nDK0cmceuWlwAAz+0fw81XvhVAvQxi5Yw5N1M96e0fndIemznoPEiYVlvn3GFnJ08r5mb6AAA7Do43rI2lENIpNB+LPn0p5H+eexXX3r3d9v3xmIIvrD7B0ba8DCAD/Dk+nn7pCL7x8+fMPz8Rw7T+JEshxF1g8dJLL+Hzn/887rnnHvT19Tl6z4YNG7B+/Xrt57GxMSxatMjdKnsYcaczrT+Jj791se65U0amaRdaJ3znktNx//Ov4pU3JnHntn26uyjZOc/MGfMtR8/E33/wVOwfnQQAzBxMY82pCxxv+6/ecxxOmp/BRW8ecfweMy44dT6+kjsZr+eqa18wrR9nHT0TAEshpHNYdYUcylb30xPmDeG9J89reN+W3W/giT2vN81oGPEq3vTDnVb8PvOH+/CRFUfpnjtzyUz0JeOaIy8Di97F1Z65detWHDx4EGeccYb2WLlcxsMPP4x///d/Rz6fR9wg6Eun00inW7tL7WXGax78R03vx5fOP6mlz1qxZAZWLJmB3750BHdu26f5+wP19OpgynyXiMUUfPytSzxve8G0fly26mjP7xf0JeO44u1LTZ9jYEE6hTGwKNb0BbladmHFkpmmx+9/PvwCntjzuu5YtKOesXAungb8OT6EPuv4eUOW5yMh1OZ0097FVWBx7rnn4ve/16flL7/8cpx00km4+uqrG4IK0jpe706aUfekqJtliRNbq10bnURTvfOERtqMlfOmndDSydRhI3WNhctSiA8ai3EH+g5ONyWuriKZTAannnqq7rHBwUHMmjWr4XHiD6JEkWmxm0JG7qxQVRWKoujmbkSVJDUWpANUKqqWoRDHlmjpbDYxWH5cHH9OkMuWbhAai2JZRaWiurIDF9j9PkBdqF1kV0jPQufNkJMNMGNRUYHJYjmw7bQbijdJJ5AzAMZ2U7s7fLmN2yladtFjVwjgPWvhJIOqiTeZOexZWr6KPPjggz4sg1jh1QynGf3JOGJKNbAYnyphIJUIZDvthhoL0gnki/SgwSDLbq5HxoEjrpFWZ4WI9fW5mIxq3HazoEa0m7IU0rswYxFygtBYKIrSMO1U9vqPKukADYAIsaIoBbLGrhDtuPIxY+HZx0Ky//cafDs5Hwn7/iIDi56FgUXIcXKH4AXj7JCuylgwsCBtROxvyXjdA6aiVksBWWnqpxmiC8uNeNOrHsoP23u73weou+yWOSukZ2FgEXKcHMheMN4pGe2xowg1FqQTyAMBjeUGO6GlPLTMyX5bLFcwVdSbcbmh1XKhs64Qijd7HQYWIcfJgewFrRQypS+FRFq8GeCQJUKskGfnGMsNdqUQ+YYh56AcIr/Gy82G0D94Fm86mFOSoKV3z8PAIuTkPA4csmOor5qZECeKoEou7YTiTdIJ5EnDiXgMoouzUKrYCi2T8Rj6ktX91onOQtwI9CVjWnu1G1o9RnIOMpsJWnr3PAwsQo7XuQB2iAAiZxBv+l1yaSdijDs1FqSdFAyThsX/x6ZKWjmgWcZRHNtOAotWtVCtZvXc+FhwumnvwsAi5ATlL1E35jGIN7ugFMKMBWknssZC/v/ruYL2GiurfKCejWxLYNGiDslJ91g9Y8HjsFdhYBFygurWMFoJd1VXCAML0kbqGot47f8isKgO7BpKJ5q6XBo7tJrRqhZKrNFLVq9UrmiGek4svVkK6V0YWIQYVVUdiaW8YLQSDmo77cSP6Y2EuEUWbwL1rMDhWsbCLlg3eso0w0kpohmtBN/yoLRmJdMkSyE9DwOLEDNVrGjudX5nErT0q7ErJMoZC5HmpcaCtBGxv6Xjeo3F6+O1wMImWB/qc56xcCKebEa6hVJItnYTkk7o22qNxJmx6HkYWIQYcSArCjCQ8ndy7KCksciXyg3TGaMISyGkEzRkLBL6jIWdINrNILJWHXLrJnLOx7Rr23aYLWG7KWFgEWLkLIKiuJ9E2AzZx0K+U2omMgs79LEgncAqsHhjohpY2LVwu9FYdLIU4lTfoZVCmLHoWRhYhJggBZWy86bYzmAqrqUxowhLIaQT5MvNu0JsNRZ9zjUWLYs3WyqFODsfxdkV0vMwsAgxQQYWGakU0g0j0wH5bsx9mpcQr1hlLLTAwk5jYfCUaYbXOSGCVrJ6OYfno2SM4s1eh4FFiAnSZlsWjHVDqylAjQXpDI2BRVUP5TRj4cXHonWNhfdSiN22hcaCs0J6FwYWISbQUoiUseiGjhCApRDSGezaTe0uxMa5Pc0Qr/GqhRJrLJbcX/QdizdjnG7a6zCwCDFBektkau1q+VJFE5l1TymEJzTSPkSHhQgojH4qTn0sXDlvtqqx8NAV4rRkSktvwsAixGQDzCQMpuvtq6+OTQW2nXbCwIJ0AiuNhcBPHwu7aal2tGIi53RukchYFJmx6FkYWISYoAaQAdW7iv5kNbjYPyoCC/+3005anYNAiBesZoUIbDUWXoaQtaqxaKHd1KnGosx2056FgUWICVK8CdSNew7UAoso23kDsjCNJzTSPsT+ZpmxsAksRPbQUcbCryFkXsSbUlt6MxK1rpBiWYWq8ljsRRhYhBhNYxFQiUIEEvWMRZcEFmw3JW3EthTi0MdivFBCpcldvjw7qNWMhZd2U83Hos9ZKQQAmLToTRhYhJig/SXECe+A0FhEPWPBrhDSAQpGgyyXGgtRClFVYKJoHRRPFMoQCYCMx7Jla6UQZx4aohQCAEUeiz0JA4sQI8xw7GYNeEWcIJz224cdTjclnUBkyIztpgK7IKAvGdPcKpuZZIlsRTymoC/p7dTtx3RTu5JpUvr9aevdmzCwCDHagRxUYGE4QXSLxqKicgASaR+tdoUoiuLIyyLrw+wgPzQWTi29AaBMvVNPwsAixLRaT7XDGLBEPWMhn9BZDiHtQhubntD7WAjk1m4rnHhZ+GGY10rGIitKIXZdIVJgwZbT3oSBRYgJ0scCaDxBRD6wkFKwLIeQdtHQbioFFqlEDOmEfWCRceBl0erIdMB7uVAWjtplUBVF0YILmmT1JgwsQkyrA4fsMH5uUFqOdpGIxyBulhhYkHZhZekNOC9j1jMWRcvX+KG58jorZLJY1jo8nGRQOeG0t2FgEVKK5QqmitWDMijtQ7dpLIDW2ukI8UK+icbCaRlTG53uUGPhlaRHEzmRLYkp0Iz1nGyHGYvehIFFSJHV4UF3hVj9HEXYckraTbN2U6fDwgbdaCxauAHw6k4rPCwGHQpHmbHobRhYhBRxd9KXjOnat/ykIbDoioxF9W6KpRDSLoylEPl4dXpMiZJJM41FzgfDPK8ZPbczSpJxEVgwY9GLMLAIKUHOCRHIgUUq7kxkFnboZUHaTbN2U/caiyalED+7Qlxm9NxmS4StN0shvQkDi5AS5Mh0gXyS6IZsBeD9xEmIVxraTT1kLDRb72alEB+ceL0G3m5bXeulEAYWvQgDi5CiDfxx0APvFdkRsBv0FUA9BcuMBWkX9XbT6rGqE2/6mLHwxcci7q1UWA9qnGVQtVIIA/yehIFFSBkP2MMCMGQsuiSwaMUAiBAviH0tmaheTL10hbTLx6LVUojT0k4iXp9wSnoPBhYhpd0ai64phcTZbkraR6Wiaul+s64Q5xqL6nGedaSx8H5OEGsrV1SUXZQp3GZLhEGWm22Q7oGBRUjx4+7EDvmzg5pH0m6osSDtRN7PzAyyHJdCXGQsWmo3lW3vXQTfbictiwmntPTuTRhYhBQ/FOB2pBMx7c4i6q6bArabknYiZ8bMDbKcZRfcaSy866682t67dQFmV0hv4yqwuP7667Fs2TIMDw9jeHgYq1atwi9/+cug1tbT+HF3YoeiKFpA0W2lEAYWpB3I+5lZKcRpEOAusPBeChGiSgDIl8uO3+dW81UvhfA47EVcBRYLFy7ENddcg61bt+LJJ5/Ee97zHlx00UV49tlng1pfzxL0nBCB+PxuKYXU2+mcnzQJ8YrsuikcKdPxejDhNAhoVylEURRPAmfXPhaiFMKMRU/iag99//vfr/v5H/7hH3D99dfjsccewymnnOLrwnqddvhYyJ/fbV0hPKGRdlA0mGMZ/+3Yx6J2/BXKFeRL5QazunyprAUxrR6r6XgMhVLFm8bCZSmE4s3exPMeWi6XcfvttyOXy2HVqlWWr8vn88jn89rPY2NjXjfpC6MTRdz8xF5c9OYRjEzv7+hamhH0yHTBUJeWQn717AHsG53UHk/GY/jTMxfhuLlDTd//yI5D2D86iYvPXBToOkln+NUz+/H47tctn1eg4MJlC7BiyYymn/PMK6O48+lX8HquAKBJYOHSxwIANv7sOS3zJpCDgFbPCalEDMhbB9+/+N1+PPmi/jva9Vquum3XGQt98LLvyCT+n8dexFSxmlE8ffEMfGD5iKv1k/Djeg/9/e9/j1WrVmFqagpDQ0O44447cPLJJ1u+ftOmTdi4cWNLi/ST27e+hH/81fPYPzqJb1x0aqeXY0muDeJNAJg7nK7+P9MX6HbaxYzBFABg64tvYOuLb+ie23Moh+9fdmbT93/xx9vwWjaPtx8/GwumhTfwJO6ZLJTxV7c8bZvN+s3OQ7j7i+c0fc3Gnz2LLXvq+9eMgXrJIx5TMNyXwHi+hFm1/dGOeEzB7KEUDo0XcPPjey1fN3sopblaekUELeLiLjOeL+Fztz5tmWmYm0k72oYm3jR8zvcffgE3PbpH+/mHj+7BO4+fg2kDwbXVk/bj+qp14oknYtu2bRgdHcV///d/Y+3atXjooYcsg4sNGzZg/fr12s9jY2NYtKhzd4OvjVezJ4fG8zav7Cx+TDJ0wt9e8CasOnY2Vp88N9DttIu/eMdSZPoSmCjUa9V7Dk3gF7/f7+hvPjpR1baMTZawYFpgyyQdYHSyiGJZRUwBPvOuYxuefz1XxC1P7HW0nxyp7ScffPMIFs4YwHtPnqd7/rt/vgLZqZIW6DrhPy5dgYf+eLDpa955QuvHqRBs5wqNeo43cgWUKyqScQWfOucY3XNLZg7i5AXDjraRsLD0PjJRzfCcfdwsbNnzBgqlCt6YKDCw6DJcX7VSqRSOO+44AMCKFSuwZcsW/Ou//iu+973vmb4+nU4jnXYW5bYDkQkYz4db3NcO500AWDxrAH8+a0mg22gns4fSWPfu43SP/WbnIfzi9/uRs/mbq6qq1bHZVdJ91HVLSXzp/JMann/5jQnc8sTepp0ZArGf/PmqJVixZGbD8287drbr9Z21dCbOWtr4WX7TTCgqgo1p/SnT78gpCQtLb/G9nXfyfOw8OI5Xx/KOvm8SLVr2sahUKjoNRdgRB9P4VLHDK2lOO3wsegUnrXyA3uyo4KIVj0QDO/dIMTsn70DYaJwPEiWaHQ9+GfOJ0fFGHwt5EqzT45JED1d7z4YNG7BmzRosXrwY2WwWN998Mx588EHcfffdQa3Pd8a1jEV4d2ZVVdtWCukFxHeYtQkm5YsJLcG7D7uLpjzwL5cvIZWwLmMYR6VHiUyTSap+3dBYTTfNl+rtucI8rFmLLYkmrvaegwcP4rLLLsP+/fsxbdo0LFu2DHfffTfe+973BrU+38lqGYvw7swThTLU2vGYCXBWSK+Qke6MVFXV/AaMyIEFSyHdh503TCIeQ38yjsliGeP55vqIKAcW4vfPmpwD/SrBWk03lb+3DDMWXYurveeGG24Iah1tQ+zEzYb9dBqxxnhMQV8yeieusCEyFhUVmCyWMZAy3+11pRAGFl2Hk3kXQ30JTBbLphddmXw5yoFFLVNgVgrxKVMqMhZFQ8aiUG4shYT5XEy8Eb2jokXkUoiqhtO8RfawsLq7Js7pT8YhOvSaZap0GQsOMes6nEzodHIXraqqpLGI3ilU2IybHQtauajFjEXdIMs6Y+HEbZREk+gdFS0idmJVrZYcwojbEcWkOYqiOLo7Yimku3EiTNQudnlrPY7sgxHJjIUDjUWrQwnrpRBz8WY6Los3wy2kJ+6J3lHRIvKFJay1vXaMTO81tJNYk7ujPAOLrkYc74MWpTD5uWalEN2o9EhmLKqlkKYai5ZLIeYGWWalEGYsuo/oHRUtYPTHt6ujdgpmLPyn2V2aQN9uysCi28g60A842k9MRqVHCfE75kw1Fv4MP3Qi3tS6tUJ6g0e8E72jogWMB5LZgRUG2GrqP82U8AKWQrobJx0PGQd30WLfiMeUlu21O0EzHYlfww+FxqJBvFn77pJSKSSs52HinZ4KLIwHUnhLIdW7hlbrnKSO6JlvdhKjj0V34+Si6SZjEcUyCGCjsfCp3VQ4b5abGGQ189Mg0SaaR4ZHjHerYS+FtKrMJnWcqP2Zsehu6iVGa28YJ26QwpU1imUQoHn2zq/hhwmt3VR/HGltunFqLLqZaB4ZHolKxoJ23v7j7IJBjUU340SY6KQFMh9hcywATbsx/CrDJkwsveU23TR9LLqaaB4ZHjEeSGGdF+KXMpvUqdt6O8tYFJmx6Dr88rEQ7aZRLYWIEsRUsYKiIYCu+1i05vgrMhby+HW5Q4Q+Ft1NNI8MjxgvKmHNWLArxH+c9MwzY9Hd+K2xSEc0YyFrt4yaIyedM04QGouihZtt1dLb2gGURJtoHhkeMe7AYU3B0cfCfzIO7o6osehunHSFNPN4EER5TghQ7cgQQZH8e8rDD+WBbF4wy1joAot4PWMxUSjrXkeiTzSPDI8YLyphTcFlHYjMiDscaSwYWHQt+VJZy0I11Vj0gHgTMJ9w6ufwQ7N2U/H9x5SqBkMOXpi16C6ie2R4ICrizZxP6UhSZ9CJj4WUts2zFNJVyDcRzZw33WS2oqqxAGDqIeHn8MOEiUGWMdOTTsS17zCs52LijegeGR4QO+/soeo45LAas1Bj4T9uHRWZseguxN99IBVvamo16CBjEfWuEACmrpd+Dj9MmFh6500CsmYuoCS6RPfI8IC4C5k/rQ9AiH0sqLHwHfpY9DZOjZ/kUkjFou4fdY0FYD47x88bmuYZi3oJxIkjLoke0T0yPCAOnPnDfbqfw4ZfEwZJHSetbQULBTuJPk79GeRgPlcw31cK5W4ohTR2ZPh5Q6PNCjHRWMjdNE40LSR6RPfI8IDYeeeFOLDIl8raRY2lEP9wLd6kxqKrqPszND+m0omY1tFgta90Q8bCTEvi1wAyQJpuWm7sCpG/N3pZdCfRPTI8INJtC2qlkDDuzLl8Wfs3Awv/ECr3vGHCrQzHpncvIvtgl7FQFMW27t8NgYWZ6+V47dzjh2g8GRMZC5NSiJTpyTjwlyHRI7pHhge0Usi0fgDh9LEQwY6dyIy4Q25ts7tgGP9Noo+b4Vp2df+oG2QBkkhVzlhM+ZexMLP0NmvTdeKIS6JHdI8MD4wbMhaFUgX5UrnZW9pO1sd0JKmTiMfQn6wGF5Ypbjpvdi1OBpAJ7Mpm3aCxqPtY1DMFfoo34zETjYVZKYQai64kukeGB4waC0BfeggDnBMSHHZ3RwUpyGTGortwI0y087LoplKIfEH3c/hh0qQrpFm7aRjL0sQ70T0yXFKp1O1qpw8k63evIduhRS2YI9P9x67lVA4m8gwsugo3d+N2UzfFvpGMcMaiHljUg2k/b2qcZiyctIGT6BHdI8MlE0W9KNKJYVInyDJjERiDNkIxfbtpuDJZpDXcHFdDfbVWTKuMRbkLMhZapiCYUogIuszaTeXvbdAmiCPRJLpHhkvESSIRU5BOxEIbKdN1MzicivIAaiy6DTetlLYaiy4ohZid//z0sRAtu2bTTc00FnTe7C6ie2S4RDux9CV0LWVha3OqT2DkADK/sctSsSuke3FXCrER+XbDrBATbYOfww+FpbfZdNO03G5KjUVXEt0jwyXGdrOwWsmKkxntvP0nY9JiJyPrKiqqXnhGoo2TkekCu9Hp3dBuaupj4WMZtm7pbdcV0ugASqJPdI8MlxjvWJwMG+oE4mQm+y4Qf7A1PjIEEsWy+awIEj20u3FHGovm54ZiN2ks8iWotVnpQcwKKUoGWWbfG30supPoHhkuMdYP7e5eO4WbfnviDju1v7H8wXJI9+AmY1E/NzQX+UY6sKj9jqoKTBSqQmU/s6WiFKKq0Ia55U38P+hj0Z1E98hwibFHO6xdIfSxCA67nnljIJEvszOkW8i5uGjWM1vmf/+6H0N0s4r9yTiEsa84B4r/+zH8UGQsgHrWQhxfyYSJxkLKnJDo0zOBRf2CXc0EhF5jwa4Q37HrBCqWmbHoRsoVFbnaXbkfPhbd0BWiKIruHOj38MNkrP7dCAGnmehVbKtcUTFV5PHWLUT3yHBJvcRQvcsIbcaC7aaB4aYrxOxnEk3k8efuNBYWpZAuCCwAIFO7ycrlS74PP5TnHAmtktn3NpCKQ6m9NBuyDj3inWgfGS7IGS7YmZD2T4+7EJkRd9iq/Y0ZC3aFdAUiW5mKx5BO2Jcv7PRX3TArBNDrG/wefpiQPkN0V4nvTe6mkTMnYdO7Ee9E+8hwgbFHO7QZCxciM+KOwSb+BJWKqt1ZiTsqZiy6A7fBulnHhEy3ZCzkjgy/hx/GYoqm4WgohRi+t7pJFjVN3UK0jwwXGEWRdnevnYI+FsGRSVtbNcvZCXHHysCiO3AzMl1+XbGsms6M6QYfC8A8Y+FnplSMTi820VjI62AppHuI9pHhAqMoMoxtTvKgNGYs/KdZlkoOLMTrGFh0B26PqcFU/XXN9pVuyViMTxUDEY0nY/oJp1bfGyecdh/RPjJcYIzIw2gl61ZkRtwhB5OVij7FLQcR4sKSp8aiK3B7Nx6LNa/7d4OlN6DvkgpC22WccJq3KYWE6SaPtEa0jwwXGH0swui8KdaSjCuORGbEHXJ5SZ52C0g99nEF6SQ1Ft2EmwFkgmZ6nK7RWEhttW7LRU7QJpwau0IMAVkmpHo34h1XR8amTZvwlre8BZlMBnPnzsUHP/hBbN++Pai1+Yo4uQyalEKMd6+dgsLNYEknYppa3XgnKp/0xImPgUV34OWiaeVzo6pq15RCBqWsjJ/mWALN1luUQmwyFmHTuxHvuDoyHnroIaxbtw6PPfYY7rnnHhSLRZx33nnI5XJBrc83Giy9pbtXuQTRSdzMMyDuaTbVVr5YsCuku/CS5hdGesa7aFmLE/XAQs4UaOdHPwMLw4RTS40FB5F1Ha72ol/96le6n2+66SbMnTsXW7duxTnnnOPrwvzGKOASd6+lmmBSmMV0Eo5MD56hdAJHJooNd0fy3ZRQ+9PHojvwctGs6w8MAagUbEZdYzFkkrHwtytEaCz0GQtjNw3Fm91HS3vR6OgoAGDmzJmWr8nn88jn89rPY2NjrWzSlnJFxQ2PvID9o1PaY6pad38TO7G4ez0yUcS1d2/HjIEUPrB8BMsXTXe0HVVV8cNH9+DF1ycAANP6k7j87KWY1u8+KHjg+YN4eMdr2Hu4+lm08w4OK6GYLCwzZix2vJrFA9sPYu3bjm5J+zI6UcQNv9mNrMVwq6A4ano/Pnn2UsRaMD46ODaFH27eow2s8ovZQ2lc8fal6EtWv9c3cgXc+JvdlnbaXti86zAAb6WQ2598Gb97eRQrlszA+5aNdFdgUTsXPrd/DC+9UT33+HlTo4k3GzQW+mPIzmq/13j5jQn86LG9yJfcHWuZviQ+efbRmD6QCmhlzvF8BatUKvjCF76As88+G6eeeqrl6zZt2oSNGzd63YxrNu86jG/+f8+bPjeQiutayeZm0jgyUcRPnnoFAPDkntdx11Vvd7SdZ14Zw9d/9pzuMRFcuEFVVVx181PaLAMAmDOcdvUZxDlW3UDNNBbX/PJ53Pf8QSyaMYA1py3wvO3bntyL79y3w/P7W2HZwuk4a6n1DYAdNzyyG997+AUfV1RnyawBvG/ZCADg5if24jv37wxkO3NdHFfitY/uOoxHdx3G/735RZxzwhwti5WIKS0FamFgbqYPALB/dAr7R8Vj/p17tOPIabspAwsAwHUP7MItT+z19N5MOoErzznG5xW5x3NgsW7dOjzzzDN45JFHmr5uw4YNWL9+vfbz2NgYFi1a5HWzthwar2ZHFs3sxweWj+ieW3XMbJ1d7T9d/Gbc/ewBvDo2hdu3voxD4wXX25mbSWPBtD789uVR7TE3TBbLWlDxl+88Bn2JOD5yxkLXn0OcYTVgqn7Si9czFrXHxN/Vy99XRuxfpy+ejrcdO6ulz3LKnU/vwytHJlte+2u197/t2Fk4ffF0H1YG/M+zr2LHwXEcytbXJta5YskMvPUY74GQkRkDKbzfcD5oxrp3H4c5Q2lMlcr4/sMvoFhWMTpRREXVu7NGmTOXzMA/fOhU7DsyCaD6HV24zHvgbEQIQYWjpp14k6WQKuIYePeJc3DyyLCj9/x6xyH87uVRjE6Gw2TMU2Bx1VVX4ec//zkefvhhLFzY/CKYTqeRTrfvDlxEvScvGMaXzj+p6WtPWzgNpy2chp0Hx3H71pddRczitcfMGcSZS2bity+PerKkFZ8TU4C/veAkKEq074LCzqDFSazQpBRSHyvdWhlAfM67TpiLz68+vqXPcspz+8bwypHJlu8GxUydPzltAT7+1iV+LA2HsgXsODiuy9aJ7bznpLlY9+7jfNmOF+YN9+Gvzq3+jW7b8jIOjecxni8hWdMNdENgEYspuHSlP39LM4xlR2vxJjMWMuIY+NAZCxtujq0olJ7D714eDY0uzNXRoaoqrrrqKtxxxx24//77sXSpu7R/O6iLNJ3XCjM2swHstiN77rtFbjFlUBE8Vj3zmrAsHtNqwOIgrQcWrd0NBGGbbIfW3dDi3WAQVvNmx00YLe3lfSbfJeZY7UB29lRV1drSm6UQHV5cUMPWyebq6F23bh1uvvlm3HXXXchkMjhw4AAAYNq0aejv7w9kgW4xtpU6QUTM5YqKqWIF/Sl7gZ68nSELBbkT6idSdoK0A6u7o0K5etdsmrGo/a19uzi3UZzr191gEB4rZsdNEEZNrSKn6oUPSjdkLIJGFmUK8TxAHws7vNyAiJshs9k2ncDV0XH99ddjdHQU73rXu7BgwQLtv9tuuy2o9bnGy6yNgVQcIlngdBCO7OTZinMcTbHai1XPvFUppFxRtVR9q50KnchY+OVqaHSu9QMzIW0YZ+XIupxucd1sB7rvTUrRN7SbtnBj1o14OdYinbFwWiboJFkPJ29Fqc4GyE5VjWLmZuzfI18kWhEf0RSrvVj1zMtp2rR0kMrmaa1mLIK4ONvh191gEEGRWTalE8GXHfI+M2OgGpiyFGKP/L01a9MVAeZUsYJiuaJZgfcqXm42jYLzTtN1f0EvcwEA973U8nasOg0cfQ4zFm3F6u9cKNfV/nKbnNndtFe0faadGgu/SiFaGce/kl1TjUWITOJks6xuGZneDuR9T3xvZm26so14rsd1FqVyBZO1OUZuyvn1jIW/PjNe6bqjw2sq1a37mywys+o0cPM5DCzag23GwlAK0d1N+1QKaavGQhLQeaVcUTVjLDGcyw/MBgGGMWMhH98shTgnY5KxMPvekvEY+mqD/3pdZyF3FrqZ25IO2Xyjrjs6vGYABl1mHcQBMJhqUWPBwKKtWPpYmAQW+VJFfzfdwklPVdVABj3Z4UfGQn6vnxd8Y/aoUlExXhDfUXim+2qZFUkrwMDCHvmcKoujzeC8kCpC45dOxFyVhFgKCRivmgW3OgnZW1+8d6JQ1gbuOMWLJoR4x3oIWe3EFzeUQqQTXSvizXyp0mAr3w780FiI76CqP/Hvgm/MHk0UyxAyrjCVQuRzA9tNnSO+t5yDNl1xc9brpRCv7dZJZiyCJecxA+A26yCnteULhdtJqV41IcQbGYsAUq6dy/VK+UTXyklP3q9kW/mg8cMjIOcxWLfDmD0Sf5N4TNFS42FAu+gVWApxg3xOFd+b1V14Kzq1bsJrxj1sXSFdd3R48bEA3KeM5YxFOhHXInG36XKv6yXekOv6cpeTpcZC+nt6yUgJxrXSWVxnKx80fgx4CspbQmQlCqWKTs8SNrM4OetT30/CU6oJK1p5Q/rerESvovTV67beXjPucvk2DHRVYCF7DrgWb7qs8Rm1EV7vDIVNNDMW7UH8nYplVXcQarXzeExXrzTeQXm9QAcxltoJQ1KK2Wu7eFA6IFlHkcuXQqs30nU3lFkKcYobbQo1FlW8ZtyNA986TVcdHXIZwrXGwkVXiCzEE+/zWsvuRAtiLyOXIeSTmAgykgn9dFPj/tByYNHmi6bYnjGQckNQnRqJeAz9tXHp4/lSaLN3Zn4MLIXYI/a9QqmiXTCtvjerqcO9Rr0U4k5jxFJIgIg/iheRmZuU8UShUWTmVX0f1ru0biUeUzCYaky76sam69pN9SJPrye++sW5vaJEq0DKDeI7CKJNVvayCKveKCPdTdPHwjny3/FwrjrZ1yrTQ41FFa/izTQDi+BoJd3sZpCY2I4sMnPrg6F9Vkjv0roZs7KVTmMhZywaSiHe/CA6MScEqE6wbHUsdZCdS3JAH9YOKV3Ggu2mjonHFAzUgvjXx2uBhVUphBkLAN71TOJ7LbIU4j+tiMzc+NXL2xEiM69+953wNuh1zLJL8gVDp7EwnOi8tm12ws5b0KqXRZD76KB03IQ1e6cJCwsl5Iv1tmRij/hbahkLS42Ft/Nnt+H1WEux3TQ4WjkBuhFfmp0AvWoswjjNsdsxGyVu1m6aN81YtFoK6UBg4SIbZ0aQjqHycRPW7J0ohagq8MZE9cLHjIUzxL73uk0pxK9heVHH6zFAg6wAybWQbrbyNzDD7I/vpSukUKpogrowGQJ1O2Z6Gp3Gopl40+vFuYP6Ab8yFkGsXT5uwpqx6EvGtBZhcYHs9UFZThHH2uuOMxbhmHXRKbweA/VSiIqKx5Z4P+mqo6OVu0KzuQWW26ldJOTMiDiA3Jgoya8Nk4Vxt2MmFJNLIWmpXin2h9lDKQA+ZCw6Gli0pg8JVGMxVdL+HmErC4rpx4D9BZLoGXQbWLQw06Yb8Foylb/XMGQtuuroaKWO7aaUYVa+8HJXKF7bn4wjwTugtmEmFDMzyKqowOhk9UQ3f1pf9T0eAwuvxjd+0Kp4s20ZixCXBcWa3phgYOEG4/dm1U3jh0NsNyACK9cGWXEGFoHRSsZClDXyNRfAptsxuUh4qWOHVQXf7ZjdwZu1mwJ10dn84Vpg0Wq7aScyFn2NGRo3BKl9kANyrZQZwuNBrEnsD2neCDhiyPC92bWb9npXiNfuMV1gEQIBZ1cdHa3028vpV7tyhpmYrZWMRbtbEHsds5NY3qTdFKgfpPOGW8tYeO1P9wP/Mhb+64Dk7FE2wO20imz2BDBj4ZSMw++NPhZVvN4cx2IKkvGqDoiBhc+0krJNxmOaJ4XdxUOMdpa348U5LqjhTqQ5ZnfwssYiEY/BOM5jQa0U4vXE59Wq1w9anRzZLh+LTnbO2GFcEwMLZzj93uRSiFfr+W6glXJ+mFpOu+roaPUE6NSv3uwE6MXrvpPeBr2M2R280VHReAKc22IpJAw+Fp5LIT3eFQI0rok+Fs4wZp9ScXORutzSO1Hozc4QVVVbutkMU8tpVx0drZ6YnPZSm21HdHW40VhoEy9DeCLtZuQx2IKiNlwqXvt//dAYSicw3NfakKQw+Fh4CYp0c3GCMMhKST4WIdZYNAQWzFg4wmnGQm7p7VUB52SxDNEp6iljESJb7646OloVmTmtRZttx4vBS5AzGIg1zTIWKS1jEde9vtUhSXU9Tfv1A634WEwVK9qo+CCCoqh1hQgYWDjDeG6z+t7kll6vRm5RR+z/MQXacD43hGl0elcdHa2KzJymjM1EZnIpxGmNMMw15W7G+HeuVFSUahdPcXDKbXFDfYmWLs7liqqldzvxt27F1TBbC34VBRjwcLKzQwRar+cKWgo3jMcDNRbecBOQidd61QJFHblcKkZFuIEai4CoW3p7OwE6TRmbaixq/y5XnI+npsaiMxj/znJNMmWisRhKJ1wZqBmR39MJIzQt6PVwJ5irOSEOpRKIGRWtPmC0fAb0E1nDAjUW3jAGZM3adHvd1rueCfd2YyyyrNRY+EyrNdq6Qr25+5tZzXkgGYcIMp2m8pix6AwZg9BWDgTFBUO+cGT6ErqTnlvVuthO1dWzE4GFd/Fm0Puo8YI9mIprtfYwYTyncGy6M7xkLHq2FNLijSY1FgGgqqpUo/UW8Q061ViY7ACxmIKhlLuImz4WnUFcJCcKZZQrqu5AFL3gxoyF+FuXKyqmiu4O3E5rB1rxscgGPOPEeMEOq5C5obuBgYUjXAUWPZ6xaLWrMc1SiP/kSxWtTu653dShQ6HVXZxb9X2QMxiINXI5Yjxf0nlYiNqmMbAYSEkZKZczNzo5gAyo71+TxTJKLtOkQWcs0okYElKGIqzHAjUW3mj43pqUQnp9XohvGYty59t1u+boENFeKyIzJ3d2+VK5LjIz7AD1lLOzAyPbYoaFeCOdiGsnuPF8qe5hIZ305BPgYE1M5fXO32y2TDuRA6mcy+mRQXtLKIqiu/iENXtHjYU3vJRCejVj4XVOiIDOmwGgnQBbEJk5EQ/JF5WGwMJrxiKkJ9NuRv5bmdkNy/8W+4XZuHUndDozlU7Etd/HfbYleG8J3TC/kGYsjL8/MxbOSCdi2gUPcKix6NXAosXSeD1j0Xnn0q45OvxI2TqJmMUd34CJyMxtxN1Jm+deRx5EZhdYiNd6NZrKhUBLI7btNmMhsi1BdmqYTQkOG0btBwMLZ8iZPsChxqJHxZutdglqXSHMWPiHHyIzJ4FBs+24DSzYFdI56n+rslaTTFqUQsTfx+sdVRim2NaFcd4yFkGuXR9YhLMsyFKId+R9x5HGokczFuIGxKuAmT4WAaD127eSsXAQMTcLBty0S1UqqukwM9Ie5L913mHGwmnXkJEwlLy8tvKZTfL1G53GIqRBtu6uOx7zZGDUq8jBYrOMRavD8qJOq87RbDcNAD+U90Z/A/PtWJ9oh1wcGBPFMoQdQlhPpt1MxqwUYpGx0DQWJjNGnBCGzJTXu8F2TOCNQikkHlMwkKqmmmXNALEnk3aasaief+lj4bHdlF0h/uOHV4CjjEWTE60bcZ/YRiKm0GynA2itxQ7Em+KE5/muPwwaC4/1azP7er+RA+swlwXF35/6CnfoSiH0sbCk1ZIpMxYB4Ic9tnZXVyihUjFX1jZrHXQjPtIyLH3efOFJa8h38LKPhcBUvOkgo2VGGKzbvWYs2pFtiULGAqh/Bwws3OFYvNnjGouWfSyosfAfP06A4s5JVaulCtPtNLmD01J5Dg6MTnsb9Dpm7aZpm3ZTr6r1+r7ZOWGinKFxQzuyLfKxFOayYIYZC084zVi0OkE46rTa2l1vN2Vg4Rt+nABlF0CrnbuZwMZdxoKBRSeRy1Z2GgvxN2rZx6KjGQtv2Za2dIX0RSxjwY4QVzjXWPS4j0WLhokcmx4AfmQsFEWRpliat+U1m6DqRWMR5ju0bmZQOonZlULEaz3f9YcgOzVU21/D6BoqX3jCOisEkDUW7R8kF2UGHQYW4nWFUgX5UucFiO0m22IQH+lSyMMPP4z3v//9GBkZgaIouPPOOwNYlnv8EpnZCfSa2XC7Ga2d1QKU8J5IuxnZnttMvCnKIqlETHvcLui0otPOm0ALGos2zDmRj4EwZywGWQrxhPibJuNKU1dk+W/v1sgt6uRLZe081JPTTXO5HJYvX47rrrsuiPV4xi+RmZ2ttyy6NOKmayAMd7G9jPx3zpuVQmoHqXw37bUUkp3q7BAyoK7vcJNmLpYr2iTXINceBR8LoP73T7MU4gqnJSS5pbfXdBZyIDWY8pYRC5PGwvVRvGbNGqxZsyaItbSE8BZoVWRmN2iqmZajfrGyv6NtxwwGYo2mOZgqoWhWCqmdBHX1fw/iMlVVQ/G39jI5UvZjCTKzxq6Q7saN6HUoncBEoex6pk3UEeeU/mQcCY+BazpEGYvAj+J8Po98Pq/9PDY2Fsh2/MpYiPff/MRePLHn9Ybn//jqePV1TSy9p4oVfP2nz6JZF+nWF9+w/BwSPOLvvO/IJB54/iAAc42F2UXvYDaPjT971tF2KhUVonO5k39rEdTsOTzheO0TtbuotFQOCnJtQNh9LKrBKAMLd7gJyIb6EjiYzeM/HtiFucPppq9NxmP40zMX4ri5GUfrKJYr+N+/3o2D2SkAwOyhNK54+1L0NZmGXa6ouPE3u7Hq2Fk4ZWSa9vhd217BtpeOAAD6knH8+VuXYGR6v6N1ZKeK+K/H9+LC0xZg0cwBAP6US8OksQj8KN60aRM2btwY9GZ88wqYm6nuzL/ecQi/3nHI+nUmO/1QXwL9yTgmi2Xc9Ogeh9vr87RO0hpzan/nbL6E3748CgCYOZDSnp8xWP232B+A6olIUYCJQhk3/maPq+0NpRNamrcTiN/j9VzB9drtTvCtMmswhZgCZPqSSIdYGCm+w+kD4ZxnElbEOW6GdHxZvzaNF17L4Re/3+/os3cfyuE/LzvT0Wsf2v4a/vFXz+seO2b2INactsDyPY/uOoS//8UfsHLpTNz2l6sAAEcmCvjibdsgWx1NFsr4+gdOcbSOO55+Bdf88nm8eDiHTR9eBsCfrsZIl0LcsmHDBqxfv177eWxsDIsWLfJ9O1e8fSneyBV0FwIvfGH1CThq+kBTW9RFMwZw2lHTGh5PxmP4/mUr8NgLhx1tK9OXxEfP8v+7IPYcNb0f37nkdGw/UM2gDaYT+NhZi7Xn337cbPzDh07FqmNmaY/NyaRx3cfOwLP7Rl1v7+xjZ3fUCO34eRn8Xxcvx+5D467fe+6b5gWwojqzhtL4j0vPwLR++wtPJ7lw2QJMFEp490lzO72USHHi/Az+6eLlOGGefWbhGxedip//dh/KavPR33sOTeAXv9+PQ+P5pq+TEa9dOnsQ8ZiCnQfHbd8vnpdf93qugIpavZC/9ZhZePiPr+E1N+vIVl/7WragPdZMu+eUMIk3Aw8s0uk00ulg73gA4NPvPNaXzxmZ3o/Prz7e8/vfcfwcvOP4Ob6shQTLB5aPAMtHTJ9LxmO4dOWShsf/5LQF+JMmdzhh5v9YsbDTS7DkglPD/532JeP481VHd3oZkeQjDve9E+ZlsP68E21f9+jOQ/jF7/e7GlgmsgLLFk5DKh7DzoPjGLfpPhHPy+JK8e9Zgym8f9kCPPzH11yuQ3xm/T1+tHX3VCmEEEII8RMvQmrZqE7c3dsJ7cXny51gWakF24tbqNim/Jl+mOiFySDL9W8xPj6OnTt3aj/v3r0b27Ztw8yZM7F48eIm7ySEEEJax4tLpyzwFy3DdgGBHARUKipiMUX3OV4cbcVrdYGFD80HIrAoRlFj8eSTT+Ld73639rPQT6xduxY33XSTbwsjhBBCzJAnoaqq6ki/JAskxUXYLjCRA49coYRMX1KXXfDixiteK7/Hr5EUQETFm+9617ug2ghrCCGEkKDI1DIFqlrt0nLis5LVlUKcGXFlDeUKObDI9CU8OdrWMxb1MowIMlrxi0nFq79TGDQWbMgmhBASKfqSMcTFwEiHF3V5yvCQjcOy8T3yv2WhZcaQOXGzjqliRStb+OJjEaKuEAYWhBBCIoWiKK5GKAB6gaQ2lM8usNCJNvXaiMF0PWNRrqia/b3TdQD1zhBtKGULGYtkvBpolSoqKpXOVhUYWBBCCIkcbssQ8nwm2dK/6XtMBJZyEDCQimsOy05tyOVtZg1dJ35kLIDO6ywYWBBCCIkcdnOdjMgXb6ddJeMmAkv5cxRFwVDK+ToqFRXjhSaf2cJ0bjmw6HTLKQMLQgghkWPIxdBHQD9l2Kn/RNYkY1HXWCQN67APLCaKZchSjMbAonWDLKDzOgsGFoQQQiKHG42FccqweO9ksYxSk7KBWcYiZyhbuMmcGF8zbtRYtFAKURSl7r7JUgghhBDiDnFhd2KnPVWs6KYMy22duYK5rXe5omKyWH/OmF0QQks3GQvja4zOnq0O0QxLZwgDC0IIIZEj40K8KYSVigIMpOJIJWKaoZTV+xuCABONBeBORGr2mZWK6ot4E2BgQQghhHjGja233BEiXDrtdBbGIMDomCm2n3GTsTCWQqZKyElizpYzFiEZRMbAghBCSORwM4jMzDK7nmkwF39a6iGkIWTy/51oPYzbyuZL2ucmYoqWRfGKlrEoN5/aGjQMLAghhEQOVyUIkyFfgzYBgTEIGJ8qoliuaEZY9cDC+SAy47bGp0q6tTmZedKMsEw4ZWBBCCEkcrgZWZ41EUfaBSYNQUC+pBOKDhrFmy4yJ/Wfi6Zr8wpLIYQQQohHRKbAlcair25A5UVjIYKNdCKmZQfciEjNyivjUz4GFhRvEkIIId4YTDubUArI7Zxx7TG7jIX43NlDKe11shdGfR1uNBb6z8xOmX+mV1IhGZ3OwIIQQkjkcNWNYVYK6bPTWFQfnz+tT/u52ec4cQDNmn2mjxmLNDMWhBBCiDe8iCblWRx27xfvmT9cDQJy+ZKpCNRLKUT3mfnGMo1XqLEghBBCPOJONFnUvQdwrrGYVwsCimUVh3OF6ueYZSxclELEZ45PmWdBvCJKIUWWQgghhBB3iAtxoVxBvtTctyGXrz5v6mNRMA8IcoYgAAAOjE7W3itnPkTGwt47QgQRC2qlkKyFbsMrbDclhBBCPCLf4edsLupZkxKG3fAwoYcYloaW7R+dAqAPAuyMtmS0Usi0/tp7StrU1cGUj+2mzFgQQggh7ojHFAyknHWGGN0ygXo3h11XyFBfUnvfgVpgMSh1l4ggY6pYsS1BGDMWqgocHMvXtsN2U0IIIaSj1OeFNM8WmA35cqqxGEontPeJjIVcCtFNSrURcNbbTdOIxxTdZ2boY0EIIYR0FqfCSfG8+ayQ5hmLjFQKOTDWWApJxmPoS1YvpXZeFs0+kxkLQgghpMM4bfU0y1jUfSwshpBJGQsRSLxu0hVS/dm+9TVfKmvahyEpsLD6TC+kqbEghBBCvDPk0CTLOOoc0AclqqqavKfeotoYSOh/dmLWJWdVBlOJhi4QZiwIIYSQDiM6KZqVIAqlitZ+aeY/UVGByaK+q0RVVd2o9YbAwhAEOLEXF583mIojHlMagxMfMhZJGmQRQggh3nGSsTCbSAoA/ck4avrJhoBgslhGRa1vwxhIGIOAuojUeh3GllfjZ/qZscizFEIIIYS4J2PjRQHUg46+ZEy7owcARVEsAwLxeTGlGoDYZSw0jYWTjEXtswZtyiteYCmEEEIIaQEnGQuzOSGCTJ95QJCVggA5ANG2a6mxsG57NXamGLMevhpkMbAghBBC3OOkGyNXsLbMFtoIo/9ELq8PAhozFOY/N7P1FuvQSiGyWVcqjpioy7QAMxaEEEJICzjxsWg2ltyuFGIWBMiPu1mHsTPFrPW1VbSx6dRYEEIIIe5x4mORzTcJLGxKIeI9crZD6C50n+NgXkjdFyPZsB4/9BUAMxaEEEJISzjpxjBmH2SsAhN5Tkh1O/pppoqiL1u48bEQr9UNMutr1H94IRWvBjwMLAghhBAP1EsQzTIF1efMfCKsbL3HjRoL2VjLJAjQAhwHXSFaKUQKVvzwsACkjAVLIYQQQoh77OZ9AM0zFnVbb/PAwqwU0kyr4ag7xcTHgqUQQgghJATYTSgFbDQWFtqIhiDAxLFT9zlORKSG0e12n+mFFGeFEEIIId4RF+dcoYxypXHeB2CjsbAICIxBwKCN0DLjoO1VK6+YaSyYsSCEEEI6j3zBFz4RRoxlDRlLjYWhNTSViGkXbbMAxdGskCkxK8RZsOKFdJQDi+uuuw5HH300+vr6sHLlSjzxxBN+r4sQQghpSjoRQzJe7dCwuqg3CywGLUSXZmPWNSGniUOmVgoplFCxyJxk8+7KK16IrHjztttuw/r16/G1r30NTz31FJYvX47zzz8fBw8eDGJ9hBBCiCmy3bZVGcJsZLrAyhLc7D1Ww8OAeilEVYGJorn7pjELEoiPRU1jUa6olqWhduA6sPj2t7+NK6+8EpdffjlOPvlkfPe738XAwAB+8IMfBLE+QgghxBKrzg6BWfZBYOljYfIes4BA0JeMIR5zljkR2op4TMFAKq57rFVExgLobDnE1W9TKBSwdetWbNiwQXssFoth9erV2Lx5s+l78vk88vm89vPY2JjHpRJCCCF6qn4Qk/j+w7swMr2/4fkDo1MA6lkF3XtrF/SDY3ls/Nmz2uMvvT5Re09jYGEWBIjMyehkEdfevR3D/Y2vmSiUdZ8j/j1RKPsu3gSqgUV/Kt7k1cHh6rc5dOgQyuUy5s2bp3t83rx5eP75503fs2nTJmzcuNH7CgkhhBAL5mbS+MN+4O5nX236utmZVONjQ2koCjBZLOPG3+xpeH5OJl3fznBfw2PGdYxOFvH/PvWy5Rr6kjGdwdbc4TQOZvOYm+lrunanJGIKFKVaksmXywD8cfR0vY6gN7BhwwasX79e+3lsbAyLFi0KerOEEEJ6gK++/2TctW0fyhXr1P9J84exYFpjNmP2UBr/8bEz8My+0Ybnjp+bwZJZg9rP/+v8E3HW0pk4/5T5ptv4pz9djv959lWosNY2nLV0li6r8I8fWYZn943h1KOGLd/jBkVR8Nl3HYu4oqAv2ZlsBQAoqqo6VngUCgUMDAzgv//7v/HBD35Qe3zt2rU4cuQI7rrrLtvPGBsbw7Rp0zA6OorhYX++TEIIIYQEi9PrtyvxZiqVwooVK3Dfffdpj1UqFdx3331YtWqV99USQgghpCtwXQpZv3491q5dizPPPBNnnXUW/uVf/gW5XA6XX355EOsjhBBCSIRwHVh89KMfxWuvvYavfvWrOHDgAN785jfjV7/6VYOgkxBCCCG9hyuNhR9QY0EIIYREj0A0FoQQQgghzWBgQQghhBDfYGBBCCGEEN9gYEEIIYQQ32BgQQghhBDfYGBBCCGEEN9gYEEIIYQQ32BgQQghhBDfYGBBCCGEEN8IfGy6EWH0OTY21u5NE0IIIcQj4rptZ9jd9sAim80CABYtWtTuTRNCCCGkRbLZLKZNm2b5fNtnhVQqFezbtw+ZTAaKovj2uWNjY1i0aBFeeumlnp1Bwu+A34GA3wO/A4DfAcDvAPDvO1BVFdlsFiMjI4jFrJUUbc9YxGIxLFy4MLDPHx4e7tmdR8DvgN+BgN8DvwOA3wHA7wDw5ztolqkQULxJCCGEEN9gYEEIIYQQ3+iawCKdTuNrX/sa0ul0p5fSMfgd8DsQ8HvgdwDwOwD4HQDt/w7aLt4khBBCSPfSNRkLQgghhHQeBhaEEEII8Q0GFoQQQgjxDQYWhBBCCPGNrgksrrvuOhx99NHo6+vDypUr8cQTT3R6SYGxadMmvOUtb0Emk8HcuXPxwQ9+ENu3b9e95l3vehcURdH99+lPf7pDK/afr3/96w2/30knnaQ9PzU1hXXr1mHWrFkYGhrCRz7yEbz66qsdXLH/HH300Q3fgaIoWLduHYDu3AcefvhhvP/978fIyAgURcGdd96pe15VVXz1q1/FggUL0N/fj9WrV2PHjh2617z++uu49NJLMTw8jOnTp+OKK67A+Ph4G3+L1mj2HRSLRVx99dU47bTTMDg4iJGREVx22WXYt2+f7jPM9p1rrrmmzb+Jd+z2g0984hMNv98FF1yge03U9wPA/nswOz8oioJrr71We00Q+0JXBBa33XYb1q9fj6997Wt46qmnsHz5cpx//vk4ePBgp5cWCA899BDWrVuHxx57DPfccw+KxSLOO+885HI53euuvPJK7N+/X/vvW9/6VodWHAynnHKK7vd75JFHtOe++MUv4mc/+xluv/12PPTQQ9i3bx8+/OEPd3C1/rNlyxbd73/PPfcAAC6++GLtNd22D+RyOSxfvhzXXXed6fPf+ta38J3vfAff/e538fjjj2NwcBDnn38+pqamtNdceumlePbZZ3HPPffg5z//OR5++GF86lOfatev0DLNvoOJiQk89dRT+MpXvoKnnnoKP/nJT7B9+3Z84AMfaHjtN77xDd2+8Vd/9VftWL4v2O0HAHDBBRfofr9bbrlF93zU9wPA/nuQf//9+/fjBz/4ARRFwUc+8hHd63zfF9Qu4KyzzlLXrVun/Vwul9WRkRF106ZNHVxV+zh48KAKQH3ooYe0x975zneqn//85zu3qID52te+pi5fvtz0uSNHjqjJZFK9/fbbtcf+8Ic/qADUzZs3t2mF7efzn/+8euyxx6qVSkVV1e7fBwCod9xxh/ZzpVJR58+fr1577bXaY0eOHFHT6bR6yy23qKqqqs8995wKQN2yZYv2ml/+8peqoijqK6+80ra1+4XxOzDjiSeeUAGoL774ovbYkiVL1H/+538OdnFtwuw7WLt2rXrRRRdZvqfb9gNVdbYvXHTRRep73vMe3WNB7AuRz1gUCgVs3boVq1ev1h6LxWJYvXo1Nm/e3MGVtY/R0VEAwMyZM3WP/9d//Rdmz56NU089FRs2bMDExEQnlhcYO3bswMjICI455hhceuml2Lt3LwBg69atKBaLun3ipJNOwuLFi7t2nygUCvjRj36ET37yk7rhft2+D8js3r0bBw4c0P3dp02bhpUrV2p/982bN2P69Ok488wztdesXr0asVgMjz/+eNvX3A5GR0ehKAqmT5+ue/yaa67BrFmzcPrpp+Paa69FqVTqzAID4sEHH8TcuXNx4okn4jOf+QwOHz6sPdeL+8Grr76KX/ziF7jiiisanvN7X2j7EDK/OXToEMrlMubNm6d7fN68eXj++ec7tKr2UalU8IUvfAFnn302Tj31VO3xj33sY1iyZAlGRkbwu9/9DldffTW2b9+On/zkJx1crX+sXLkSN910E0488UTs378fGzduxDve8Q4888wzOHDgAFKpVMOJdN68eThw4EBnFhwwd955J44cOYJPfOIT2mPdvg8YEX9bs3OBeO7AgQOYO3eu7vlEIoGZM2d25b4xNTWFq6++Gpdccolu+NTnPvc5nHHGGZg5cyYeffRRbNiwAfv378e3v/3tDq7WPy644AJ8+MMfxtKlS7Fr1y58+ctfxpo1a7B582bE4/Ge2w8A4Ic//CEymUxDSTiIfSHygUWvs27dOjzzzDM6fQEAXa3wtNNOw4IFC3Duuedi165dOPbYY9u9TN9Zs2aN9u9ly5Zh5cqVWLJkCX784x+jv7+/gyvrDDfccAPWrFmDkZER7bFu3wdIc4rFIv70T/8Uqqri+uuv1z23fv167d/Lli1DKpXCX/7lX2LTpk1dYX39Z3/2Z9q/TzvtNCxbtgzHHnssHnzwQZx77rkdXFnn+MEPfoBLL70UfX19useD2BciXwqZPXs24vF4g+L/1Vdfxfz58zu0qvZw1VVX4ec//zkeeOAB21H0K1euBADs3LmzHUtrO9OnT8cJJ5yAnTt3Yv78+SgUCjhy5IjuNd26T7z44ou499578Rd/8RdNX9ft+4D42zY7F8yfP79B1F0qlfD666931b4hgooXX3wR99xzj+2o7JUrV6JUKmHPnj3tWWCbOeaYYzB79mxt3++V/UDw61//Gtu3b7c9RwD+7AuRDyxSqRRWrFiB++67T3usUqngvvvuw6pVqzq4suBQVRVXXXUV7rjjDtx///1YunSp7Xu2bdsGAFiwYEHAq+sM4+Pj2LVrFxYsWIAVK1YgmUzq9ont27dj7969XblP3HjjjZg7dy4uvPDCpq/r9n1g6dKlmD9/vu7vPjY2hscff1z7u69atQpHjhzB1q1btdfcf//9qFQqWuAVdURQsWPHDtx7772YNWuW7Xu2bduGWCzWUB7oFl5++WUcPnxY2/d7YT+QueGGG7BixQosX77c9rW+7Au+SkE7xK233qqm02n1pptuUp977jn1U5/6lDp9+nT1wIEDnV5aIHzmM59Rp02bpj744IPq/v37tf8mJiZUVVXVnTt3qt/4xjfUJ598Ut29e7d61113qcccc4x6zjnndHjl/vHXf/3X6oMPPqju3r1b/c1vfqOuXr1anT17tnrw4EFVVVX105/+tLp48WL1/vvvV5988kl11apV6qpVqzq8av8pl8vq4sWL1auvvlr3eLfuA9lsVn366afVp59+WgWgfvvb31affvpprePhmmuuUadPn67edddd6u9+9zv1oosuUpcuXapOTk5qn3HBBReop59+uvr444+rjzzyiHr88cerl1xySad+Jdc0+w4KhYL6gQ98QF24cKG6bds23fkhn8+rqqqqjz76qPrP//zP6rZt29Rdu3apP/rRj9Q5c+aol112WYd/M+c0+w6y2az6N3/zN+rmzZvV3bt3q/fee696xhlnqMcff7w6NTWlfUbU9wNVtT8eVFVVR0dH1YGBAfX6669veH9Q+0JXBBaqqqr/9m//pi5evFhNpVLqWWedpT722GOdXlJgADD978Ybb1RVVVX37t2rnnPOOerMmTPVdDqtHnfcceqXvvQldXR0tLML95GPfvSj6oIFC9RUKqUeddRR6kc/+lF1586d2vOTk5PqZz/7WXXGjBnqwMCA+qEPfUjdv39/B1ccDHfffbcKQN2+fbvu8W7dBx544AHTfX/t2rWqqlZbTr/yla+o8+bNU9PptHruuec2fDeHDx9WL7nkEnVoaEgdHh5WL7/8cjWbzXbgt/FGs+9g9+7dlueHBx54QFVVVd26dau6cuVKddq0aWpfX5/6pje9Sf3mN7+pu+iGnWbfwcTEhHreeeepc+bMUZPJpLpkyRL1yiuvbLjRjPp+oKr2x4Oqqur3vvc9tb+/Xz1y5EjD+4PaFzg2nRBCCCG+EXmNBSGEEELCAwMLQgghhPgGAwtCCCGE+AYDC0IIIYT4BgMLQgghhPgGAwtCCCGE+AYDC0IIIYT4BgMLQgghhPgGAwtCCCGE+AYDC0IIIYT4BgMLQgghhPgGAwtCCCGE+Mb/D0PHVKkUp9t+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(t_activos_in_top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RQkcy70pxZlQ",
        "outputId": "c9a64ddc-a054-4acd-a4cc-c3b7803e7dce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b1234824a90>]"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(t_evaluacion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ShcpX60oxec7",
        "outputId": "c27925a4-c94c-4113-9298-f0d3d6675dd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b12367fe7a0>]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(t_distancia_media_de_top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-VoXotOE3KRX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uyut6G62SK3S",
        "outputId": "d89d9c9b-1665-4311-88b4-da21b1a00445"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-100-2a86f3659aa0>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    best_population =\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# FRAGMENTO DE MODELO EVOLUTIVO\n",
        "population = np.array([[1,4,3], [1,3,4],[6,3,5],[6,4,3], [1,2,4],[8,3,5]])\n",
        "fitness = np.array([5,6,5.5,1,9,1.3])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "guardar en dos srrays diferentes los 3 mejores individuos ordenados\n",
        "\n",
        "\"\"\"\n",
        "best_population =\n",
        "best_fitness =\n",
        "\n",
        "new_population = np.array([[7,4,3], [1,4,4],[1,3,5],[6,3,3], [1,2,8],[4,3,5]])\n",
        "new_fitness = np.array([4,2,3.5,4,2,2])\n",
        "\n",
        "\"\"\"\n",
        "ver que nuevos indidividuos en base a su fitness son mejores que lo anteriores mejores y guardar los nuevos best\n",
        "\"\"\"\n",
        "\n",
        "best_population =\n",
        "best_fitness =\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tflYIRi4SROd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wZjGZqIhSRSk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL159KFUZtNc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "population = np.array([[1,4,3], [2,3,4],[3,3,5],[4,4,3], [5,2,4],[6,3,5]])\n",
        "fitness = np.array([1,2,3,4,5,6])\n",
        "\n",
        "# Guardar en dos arrays diferentes los 3 mejores individuos ordenados\n",
        "sorted_indices = np.argsort(fitness)[:3]  # Ordenamos de menor a mayor\n",
        "#print(sorted_indices)\n",
        "best_population = population[sorted_indices]\n",
        "print(best_population)\n",
        "best_fitness = fitness[sorted_indices]\n",
        "print(best_fitness)\n",
        "print(\"------\")\n",
        "new_population = np.array([[1,5,3], [2,4,4],[3,3,5],[4,3,3], [5,2,8],[6,3,5]])\n",
        "new_fitness = np.array([1,8,9,5,6,7])\n",
        "\n",
        "# Ver qu칠 nuevos individuos en base a su fitness son mejores que los anteriores mejores\n",
        "combined_population = np.vstack([best_population, new_population])\n",
        "combined_fitness = np.concatenate([best_fitness, new_fitness])\n",
        "\n",
        "sorted_indices = np.argsort(combined_fitness)[:3]  # Ordenamos de menor a mayor\n",
        "best_population = combined_population[sorted_indices]\n",
        "best_fitness = combined_fitness[sorted_indices]\n",
        "\n",
        "print(best_population)\n",
        "print(best_fitness)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNEre4GxS1VOoQUtVKfLDgd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}